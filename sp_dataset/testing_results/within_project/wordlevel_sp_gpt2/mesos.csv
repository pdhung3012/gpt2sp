issuekey,title,description,storypoint,raw predictions
MESOS-4941,Support update existing quota.,"We want to support updating an existing quota without the cycle of delete and recreate. This avoids the possible starvation risk of losing the quota between delete and recreate, and also makes the interface friendly.    Design doc:  https://docs.google.com/document/d/1c8fJY9_N0W04FtUQ_b_kZM6S0eePU7eYVyfUP14dSys",8,3.3705678
MESOS-4942,Docker runtime isolator tests may cause disk issue.,"Currently slave working directory is used as docker store dir and archive dir, which is problematic. Because slave work dir is exactly `environment->mkdtemp()`, it will get cleaned up until the end of the whole test. But the runtime isolator local puller tests cp the host's rootfs, which size is relatively big. Cleanup has to be done by each test tear down. ",2,3.0296607
MESOS-4943,Reduce the size of LinuxRootfs in tests.,"Right now, LinuxRootfs copies files from the host filesystem to construct a chroot-able rootfs. We copy a lot of unnecessary files, making it very large. We can potentially strip a lot files.",13,2.3771758
MESOS-4944,Improve overlay backend so that it's writable,"Currently, the overlay backend will provision a read-only FS. We can use an empty directory from the container sandbox to act as the upper layer so that it's writable.",5,2.4331965
MESOS-4949,Executor shutdown grace period should be configurable.,"Currently, executor shutdown grace period is specified by an agent flag, which is propagated to executors via the {{MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD}} environment variable. There is no way to adjust this timeout for the needs of a particular executor.    To tackle this problem, we propose to introduce an optional {{shutdown_grace_period}} field in {{ExecutorInfo}}.",3,2.6067662
MESOS-4950,Implement reconnect funtionality in the scheduler library.,"Currently, there is no way for the schedulers to force a reconnection attempt with the master using the scheduler library {{src/scheduler/scheduler.cpp}}. It is specifically useful in scenarios where there is a one way network partition with the master. Due to this, the scheduler has not received any {{HEARTBEAT}} events from the master. In this case, the scheduler might want to force a reconnection attempt with the master instead of relying on the {{disconnected}} callback.",3,3.9280386
MESOS-4951,Enable actors to pass an authentication realm to libprocess,"To prepare for MESOS-4902, the Mesos master and agent need a way to pass the desired authentication realm to libprocess. Since some endpoints (like {{/profiler/*}}) get installed in libprocess, the master/agent should be able to specify during initialization what authentication realm the libprocess-level endpoints will be authenticated under.",2,1.9802802
MESOS-4956,Add authentication to /files endpoints,"To protect access (authz) to master/agent logs as well as executor sandboxes, we need authentication on the /files endpoints.    Adding HTTP authentication to these endpoints is a bit complicated since they are defined in code that is shared by the master and agent.    While working on MESOS-4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that HTTP authentication can be independently enabled/disabled for each.    We should establish a mechanism for making an endpoint authenticated that allows us to:  1) Install an endpoint like {{/files}}, whose code is shared by the master and agent, with different authentication realms for the master and agent  2) Avoid hard-coding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application-level concerns from leaking into libprocess    Another option would be to use a single default authentication realm and always enable or disable HTTP authentication for *both* the master and agent in tests. However, this wouldn't allow us to test scenarios where HTTP authentication is enabled on one but disabled on the other.",5,2.0321984
MESOS-4961,ContainerLoggerTest.LOGROTATE_RotateInSandbox is flaky,"The logger subprocesses may exit before we reach the {{waitpid}} in the test.  If this happens, {{waitpid}} will return a {{-1}} as the process no longer exists.    Verbose logs:  {code}  [ RUN      ] ContainerLoggerTest.LOGROTATE_RotateInSandbox  I0316 14:28:51.329337  1242 cluster.cpp:139] Creating default 'local' authorizer  I0316 14:28:51.332823  1242 leveldb.cpp:174] Opened db in 3.079559ms  I0316 14:28:51.333916  1242 leveldb.cpp:181] Compacted db in 1.054247ms  I0316 14:28:51.333979  1242 leveldb.cpp:196] Created db iterator in 21450ns  I0316 14:28:51.334005  1242 leveldb.cpp:202] Seeked to beginning of db in 2205ns  I0316 14:28:51.334025  1242 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns  I0316 14:28:51.334089  1242 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0316 14:28:51.334661  1275 recover.cpp:447] Starting replica recovery  I0316 14:28:51.335044  1275 recover.cpp:473] Replica is in EMPTY status  I0316 14:28:51.336207  1262 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (484)@172.17.0.3:45919  I0316 14:28:51.336730  1270 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0316 14:28:51.337257  1275 recover.cpp:564] Updating replica status to STARTING  I0316 14:28:51.338001  1267 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 537200ns  I0316 14:28:51.338032  1267 replica.cpp:320] Persisted replica status to STARTING  I0316 14:28:51.338183  1261 master.cpp:376] Master c7653f60-33e9-4406-9f62-dc74c906bf83 (2cbb23302fe5) started on 172.17.0.3:45919  I0316 14:28:51.338295  1263 recover.cpp:473] Replica is in STARTING status  I0316 14:28:51.338213  1261 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/XtqwkS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/XtqwkS/master"" --zk_session_timeout=""10secs""  I0316 14:28:51.338562  1261 master.cpp:423] Master only allowing authenticated frameworks to register  I0316 14:28:51.338572  1261 master.cpp:428] Master only allowing authenticated slaves to register  I0316 14:28:51.338580  1261 credentials.hpp:35] Loading credentials for authentication from '/tmp/XtqwkS/credentials'  I0316 14:28:51.338877  1261 master.cpp:468] Using default 'crammd5' authenticator  I0316 14:28:51.339030  1262 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (485)@172.17.0.3:45919  I0316 14:28:51.339246  1261 master.cpp:537] Using default 'basic' HTTP authenticator  I0316 14:28:51.339393  1261 master.cpp:571] Authorization enabled  I0316 14:28:51.339390  1266 recover.cpp:193] Received a recover response from a replica in STARTING status  I0316 14:28:51.339606  1271 whitelist_watcher.cpp:77] No whitelist given  I0316 14:28:51.339607  1275 hierarchical.cpp:144] Initialized hierarchical allocator process  I0316 14:28:51.340077  1268 recover.cpp:564] Updating replica status to VOTING  I0316 14:28:51.340533  1270 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 331558ns  I0316 14:28:51.340558  1270 replica.cpp:320] Persisted replica status to VOTING  I0316 14:28:51.340672  1270 recover.cpp:578] Successfully joined the Paxos group  I0316 14:28:51.340827  1270 recover.cpp:462] Recover process terminated  I0316 14:28:51.341684  1270 master.cpp:1806] The newly elected leader is master@172.17.0.3:45919 with id c7653f60-33e9-4406-9f62-dc74c906bf83  I0316 14:28:51.341717  1270 master.cpp:1819] Elected as the leading master!  I0316 14:28:51.341740  1270 master.cpp:1508] Recovering from registrar  I0316 14:28:51.341954  1263 registrar.cpp:307] Recovering registrar  I0316 14:28:51.342499  1273 log.cpp:659] Attempting to start the writer  I0316 14:28:51.343616  1266 replica.cpp:493] Replica received implicit promise request from (487)@172.17.0.3:45919 with proposal 1  I0316 14:28:51.344183  1266 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 536941ns  I0316 14:28:51.344208  1266 replica.cpp:342] Persisted promised to 1  I0316 14:28:51.344825  1267 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0316 14:28:51.346009  1276 replica.cpp:388] Replica received explicit promise request from (488)@172.17.0.3:45919 for position 0 with proposal 2  I0316 14:28:51.346371  1276 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 327890ns  I0316 14:28:51.346393  1276 replica.cpp:712] Persisted action at 0  I0316 14:28:51.347363  1267 replica.cpp:537] Replica received write request for position 0 from (489)@172.17.0.3:45919  I0316 14:28:51.347414  1267 leveldb.cpp:436] Reading position from leveldb took 24861ns  I0316 14:28:51.347774  1267 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 323654ns  I0316 14:28:51.347796  1267 replica.cpp:712] Persisted action at 0  I0316 14:28:51.348323  1276 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0316 14:28:51.348714  1276 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 361981ns  I0316 14:28:51.348738  1276 replica.cpp:712] Persisted action at 0  I0316 14:28:51.348760  1276 replica.cpp:697] Replica learned NOP action at position 0  I0316 14:28:51.349318  1274 log.cpp:675] Writer started with ending position 0  I0316 14:28:51.350275  1267 leveldb.cpp:436] Reading position from leveldb took 23849ns  I0316 14:28:51.351171  1271 registrar.cpp:340] Successfully fetched the registry (0B) in 9.173248ms  I0316 14:28:51.351300  1271 registrar.cpp:439] Applied 1 operations in 32119ns; attempting to update the 'registry'  I0316 14:28:51.351989  1272 log.cpp:683] Attempting to append 170 bytes to the log  I0316 14:28:51.352108  1266 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0316 14:28:51.352802  1263 replica.cpp:537] Replica received write request for position 1 from (490)@172.17.0.3:45919  I0316 14:28:51.353313  1263 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 474854ns  I0316 14:28:51.353338  1263 replica.cpp:712] Persisted action at 1  I0316 14:28:51.354101  1273 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0316 14:28:51.354483  1273 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 338210ns  I0316 14:28:51.354507  1273 replica.cpp:712] Persisted action at 1  I0316 14:28:51.354529  1273 replica.cpp:697] Replica learned APPEND action at position 1  I0316 14:28:51.355444  1275 registrar.cpp:484] Successfully updated the 'registry' in 4.084224ms  I0316 14:28:51.355569  1275 registrar.cpp:370] Successfully recovered registrar  I0316 14:28:51.355697  1268 log.cpp:702] Attempting to truncate the log to 1  I0316 14:28:51.355870  1269 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0316 14:28:51.356016  1274 master.cpp:1616] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register  I0316 14:28:51.356032  1272 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  I0316 14:28:51.356761  1273 replica.cpp:537] Replica received write request for position 2 from (491)@172.17.0.3:45919  I0316 14:28:51.357203  1273 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 406053ns  I0316 14:28:51.357226  1273 replica.cpp:712] Persisted action at 2  I0316 14:28:51.357718  1270 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0316 14:28:51.358093  1270 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 345370ns  I0316 14:28:51.358175  1270 leveldb.cpp:399] Deleting ~1 keys from leveldb took 57us  I0316 14:28:51.358201  1270 replica.cpp:712] Persisted action at 2  I0316 14:28:51.358220  1270 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0316 14:28:51.368399  1242 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix  W0316 14:28:51.406371  1242 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges  I0316 14:28:51.410480  1266 slave.cpp:193] Slave started on 12)@172.17.0.3:45919  I0316 14:28:51.410518  1266 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --container_logger=""org_apache_mesos_LogrotateContainerLogger"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy""  I0316 14:28:51.411118  1266 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential'  I0316 14:28:51.411381  1266 slave.cpp:324] Slave using credential for: test-principal  I0316 14:28:51.411696  1266 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]  Trying semicolon-delimited string format instead  I0316 14:28:51.412075  1266 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0316 14:28:51.412148  1266 slave.cpp:472] Slave attributes: [  ]  I0316 14:28:51.412160  1266 slave.cpp:477] Slave hostname: 2cbb23302fe5  I0316 14:28:51.413516  1263 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta'  I0316 14:28:51.413774  1266 status_update_manager.cpp:200] Recovering status update manager  I0316 14:28:51.414029  1276 containerizer.cpp:407] Recovering containerizer  I0316 14:28:51.415222  1269 provisioner.cpp:245] Provisioner recovery complete  I0316 14:28:51.415650  1268 slave.cpp:4565] Finished recovery  I0316 14:28:51.416115  1268 slave.cpp:4737] Querying resource estimator for oversubscribable resources  I0316 14:28:51.416365  1268 slave.cpp:796] New master detected at master@172.17.0.3:45919  I0316 14:28:51.416448  1276 status_update_manager.cpp:174] Pausing sending status updates  I0316 14:28:51.416445  1268 slave.cpp:859] Authenticating with master master@172.17.0.3:45919  I0316 14:28:51.416522  1268 slave.cpp:864] Using default CRAM-MD5 authenticatee  I0316 14:28:51.416671  1268 slave.cpp:832] Detecting new master  I0316 14:28:51.416731  1275 authenticatee.cpp:121] Creating new client SASL connection  I0316 14:28:51.416807  1268 slave.cpp:4751] Received oversubscribable resources  from the resource estimator  I0316 14:28:51.417006  1263 master.cpp:5659] Authenticating slave(12)@172.17.0.3:45919  I0316 14:28:51.417103  1262 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(38)@172.17.0.3:45919  I0316 14:28:51.417348  1273 authenticator.cpp:98] Creating new server SASL connection  I0316 14:28:51.417548  1266 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0316 14:28:51.417582  1266 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0316 14:28:51.417696  1264 authenticator.cpp:203] Received SASL authentication start  I0316 14:28:51.417753  1264 authenticator.cpp:325] Authentication requires more steps  I0316 14:28:51.417948  1265 authenticatee.cpp:258] Received SASL authentication step  I0316 14:28:51.418107  1267 authenticator.cpp:231] Received SASL authentication step  I0316 14:28:51.418159  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0316 14:28:51.418180  1267 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0316 14:28:51.418233  1267 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0316 14:28:51.418270  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0316 14:28:51.418289  1267 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0316 14:28:51.418300  1267 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0316 14:28:51.418323  1267 authenticator.cpp:317] Authentication success  I0316 14:28:51.418414  1264 authenticatee.cpp:298] Authentication success  I0316 14:28:51.418473  1269 master.cpp:5689] Successfully authenticated principal 'test-principal' at slave(12)@172.17.0.3:45919  I0316 14:28:51.418514  1275 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(38)@172.17.0.3:45919  I0316 14:28:51.418781  1276 slave.cpp:927] Successfully authenticated with master master@172.17.0.3:45919  I0316 14:28:51.418937  1276 slave.cpp:1321] Will retry registration in 1.983001ms if necessary  I0316 14:28:51.419108  1262 master.cpp:4370] Registering slave at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with id c7653f60-33e9-4406-9f62-dc74c906bf83-S0  I0316 14:28:51.419643  1266 registrar.cpp:439] Applied 1 operations in 75642ns; attempting to update the 'registry'  I0316 14:28:51.420670  1272 log.cpp:683] Attempting to append 339 bytes to the log  I0316 14:28:51.420820  1269 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0316 14:28:51.421495  1270 slave.cpp:1321] Will retry registration in 1.437257ms if necessary  I0316 14:28:51.421716  1275 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress  I0316 14:28:51.422107  1267 replica.cpp:537] Replica received write request for position 3 from (505)@172.17.0.3:45919  I0316 14:28:51.423033  1267 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 762815ns  I0316 14:28:51.423066  1267 replica.cpp:712] Persisted action at 3  I0316 14:28:51.424069  1267 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I0316 14:28:51.424232  1264 slave.cpp:1321] Will retry registration in 66.01292ms if necessary  I0316 14:28:51.424342  1269 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress  I0316 14:28:51.424686  1267 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 574743ns  I0316 14:28:51.424757  1267 replica.cpp:712] Persisted action at 3  I0316 14:28:51.424792  1267 replica.cpp:697] Replica learned APPEND action at position 3  I0316 14:28:51.426441  1272 registrar.cpp:484] Successfully updated the 'registry' in 6.721024ms  I0316 14:28:51.426677  1262 log.cpp:702] Attempting to truncate the log to 3  I0316 14:28:51.426808  1264 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4  I0316 14:28:51.427584  1261 slave.cpp:3482] Received ping from slave-observer(11)@172.17.0.3:45919  I0316 14:28:51.428213  1262 hierarchical.cpp:473] Added slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )  I0316 14:28:51.427865  1266 master.cpp:4438] Registered slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0316 14:28:51.428270  1267 slave.cpp:971] Registered with master master@172.17.0.3:45919; given slave ID c7653f60-33e9-4406-9f62-dc74c906bf83-S0  I0316 14:28:51.428412  1265 replica.cpp:537] Replica received write request for position 4 from (506)@172.17.0.3:45919  I0316 14:28:51.428443  1267 fetcher.cpp:81] Clearing fetcher cache  I0316 14:28:51.428503  1262 hierarchical.cpp:1453] No resources available to allocate!  I0316 14:28:51.428535  1262 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 205421ns  I0316 14:28:51.428750  1273 status_update_manager.cpp:181] Resuming sending status updates  I0316 14:28:51.429157  1265 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 695258ns  I0316 14:28:51.429225  1267 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/slave.info'  I0316 14:28:51.429275  1265 replica.cpp:712] Persisted action at 4  I0316 14:28:51.429759  1267 slave.cpp:1030] Forwarding total oversubscribed resources   I0316 14:28:51.430055  1265 master.cpp:4782] Received update of slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with total oversubscribed resources   I0316 14:28:51.430614  1271 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0  I0316 14:28:51.430891  1242 sched.cpp:222] Version: 0.29.0  I0316 14:28:51.431043  1265 hierarchical.cpp:531] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )  I0316 14:28:51.431236  1271 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 536892ns  I0316 14:28:51.431267  1265 hierarchical.cpp:1453] No resources available to allocate!  I0316 14:28:51.431584  1271 leveldb.cpp:399] Deleting ~2 keys from leveldb took 66904ns  I0316 14:28:51.431538  1273 sched.cpp:326] New master detected at master@172.17.0.3:45919  I0316 14:28:51.431622  1271 replica.cpp:712] Persisted action at 4  I0316 14:28:51.431623  1265 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 518588ns  I0316 14:28:51.431660  1271 replica.cpp:697] Replica learned TRUNCATE action at position 4  I0316 14:28:51.431711  1273 sched.cpp:382]...",1,1.9755133
MESOS-4962,Support for Mesos releases,"As part of Mesos reaching 1.0, we need to formalize the policy of supporting Mesos releases.    Some specific questions we need to answer:    --> What fixes should we backports to older releases.    --> How many old releases are supported.    --> Should we have a LTS version?    --> What is the cadence of major, minor and patch releases?",8,3.2462053
MESOS-4970,Add more examples of JSON resources to docs,"The configuration documentation currently only shows examples of scalar resource types in JSON format. The structures of JSON resources are a bit complicated, so it would be very helpful to include examples of ranges, sets, and text resource types as well.",1,1.8656507
MESOS-4978,Update mesos-execute with Appc changes.,mesos-execute cli application currently does not have support for Appc images. Adding support would make integration tests easier.,3,2.3544278
MESOS-4982,Update example long running to use v1 API.,We need to modify the long running test framework similar to {{src/examples/long_lived_framework.cpp}} to use the v1 API.    This would allow us to vet the v1 API and the scheduler library in test clusters.,5,2.3467388
MESOS-4984,MasterTest.SlavesEndpointTwoSlaves is flaky,"Observed on Arch Linux with GCC 6, running in a virtualbox VM:    [ RUN      ] MasterTest.SlavesEndpointTwoSlaves  /mesos-2/src/tests/master_tests.cpp:1710: Failure  Value of: array.get().values.size()    Actual: 1  Expected: 2u  Which is: 2  [  FAILED  ] MasterTest.SlavesEndpointTwoSlaves (86 ms)    Seems to fail non-deterministically, perhaps more often when there is concurrent CPU load on the machine.",2,1.6628292
MESOS-4985,Destroy a container while it's provisioning can lead to leaked provisioned directories.,"Here is the possible sequence of events:  1) containerizer->launch  2) provisioner->provision is called. it is fetching the image  3) executor registration timed out  4) containerizer->destroy is called  5) container->state is still in PREPARING  6) provisioner->destroy is called    So we can be calling provisioner->destory while provisioner->provision hasn't finished yet. provisioner->destroy might just skip since there's no information about the container yet, and later, provisioner will prepare the root filesystem. This root filesystem will not be destroyed as destroy already finishes.",3,2.2570186
MESOS-4992,sandbox uri does not work outisde mesos http server,"The SandBox uri of a framework does not work if i just copy paste it to the browser.    For example the following sandbox uri:  http://172.17.0.1:5050/#/slaves/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0/frameworks/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-0009/executors/driver-20160321155016-0001/browse    should redirect to:  http://172.17.0.1:5050/#/slaves/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0/browse?path=%2Ftmp%2Fmesos%2Fslaves%2F50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0%2Fframeworks%2F50f87c73-79ef-4f2a-95f0-b2b4062b2de6-0009%2Fexecutors%2Fdriver-20160321155016-0001%2Fruns%2F60533483-31fb-4353-987d-f3393911cc80    yet it fails with the message:  ""Failed to find slaves.  Navigate to the slave's sandbox via the Mesos UI.""  and redirects to:  http://172.17.0.1:5050/#/    It is an issue for me because im working on expanding the mesos spark ui with sandbox uri, The other option is to get the slave info and parse the json file there and get executor paths not so straightforward or elegant though.    Moreover i dont see the runs/container_id in the Mesos Proto Api. I guess this is hidden info, this is the needed piece of info to re-write the uri without redirection.  ",3,2.9711192
MESOS-4998,Problematic fork/clone performance at high load.,"Creating a new subprocess in mesos involves forking/cloning a new process. In most cases (executors, perf, ..) the parent of the new process is the agent/slave process. This can lead to problematic behavior especially when creating several new processes at the same time.    The problem here is that the normal fork() (or clone syscall used by libprocess) provides a copy-on-write (cow) view of the parents address space until the child execs its new binary. Note that during the time between fork and exec Mesos does several setup actions such as placing the new processes in systemd units or assigning them to the freezer cgroup.  This cow property of the address space implies that existing memory is marked as read-only and any write will trigger a page-fault and a newly created page. Note this behavior also extends to the parent process and hence any write will be very costly.    We simulated the number of pagefaults when forking/cloning new processes by this benchmark:  https://github.com/joerg84/forking-benchmark    Results can be seen here:   https://docs.google.com/presentation/d/1SUjKAVHdrutLPpFJy3Q1yhinG5FOMw3HbbEdzuhZ7A8",8,2.5344226
MESOS-5004,Clarify docs on '/reserve' and '/create-volumes' without authentication,"For both reservations and persistent volume creation, the behavior of the HTTP endpoints differs slightly from that of the framework operations. Due to the implementation of HTTP authentication, it is not possible for a framework/operator to provide a principal when HTTP authentication is disabled. This means that when HTTP authentication is disabled, the endpoint handlers will _always_ receive {{None()}} as the principal associated with the request, and thus if authorization is enabled, the request will only succeed if the NONE principal is authorized to do stuff.    The docs should be updated to explain this behavior explicitly.",1,2.094216
MESOS-5005,Enforce that DiskInfo principal is equal to framework/operator principal,"Currently, we require that {{ReservationInfo.principal}} be equal to the principal provided for authentication, which means that when HTTP authentication is disabled this field cannot be set. Based on comments in 'mesos.proto', the original intention was to enforce this same constraint for {{Persistence.principal}}, but it seems that we don't enforce it. This should be changed to make the two fields equivalent, with one exception: when the framework/operator principal is {{None}}, we should allow the principal in {{DiskInfo}} to take any value, along the same lines as MESOS-5212.",3,1.546488
MESOS-5006,Add example for mesos-execute usage of Appc images in container-image.md.,Example usage for Appc flags and images needs to be added to container-image.md.,3,1.729042
MESOS-5010,Installation of mesos python package is incomplete,"The installation of mesos python package is incomplete, i.e., the files {{cli.py}}, {{futures.py}}, and {{http.py}} are not installed.    {code}  % ../configure --enable-python  % make install DESTDIR=$PWD/D  % PYTHONPATH=$PWD/D/usr/local/lib/python2.7/site-packages:$PYTHONPATH python -c 'from mesos import http'    Traceback (most recent call last):    File ""<string>"", line 1, in <module>  ImportError: cannot import name http  {code}    This appears to be first broken with {{d1d70b9}} (MESOS-3969, [Upgraded bundled pip to 7.1.2.|https://reviews.apache.org/r/40630]). Bisecting in {{pip}}-land shows that our install becomes broken for {{pip-6.0.1}} and later (we are using {{pip-7.1.2}}).  ",2,2.0404916
MESOS-5013,Add docker volume driver isolator for Mesos containerizer.,The isolator will interact with Docker Volume Driver Plugins to mount and unmount external volumes to container.  ,8,2.6570573
MESOS-5014,Call and Event Type enums in scheduler.proto should be optional,Having a 'required' Type enum has backwards compatibility issues when adding new enum types. See MESOS-4997 for details.,2,1.6982545
MESOS-5015,Call and Event Type enums in executor.proto should be optional,Having a 'required' Type enum has backwards compatibility issues when adding new enum types. See MESOS-4997 for details.,2,1.8759938
MESOS-5016,Add a reconnect() method to the C++ scheduler library,A reconnect() method on the library would allow the scheduler to force a reconnection (disconnect and reconnect) by the library. This might be used by the scheduler to react to lack of HEARTBEATs.,3,2.790876
MESOS-5023,MesosContainerizerProvisionerTest.DestroyWhileProvisioning is flaky.,Observed on the Apache Jenkins.    {noformat}  [ RUN      ] MesosContainerizerProvisionerTest.ProvisionFailed  I0324 13:38:56.284261  2948 containerizer.cpp:666] Starting container 'test_container' for executor 'executor' of framework ''  I0324 13:38:56.285825  2939 containerizer.cpp:1421] Destroying container 'test_container'  I0324 13:38:56.285854  2939 containerizer.cpp:1424] Waiting for the provisioner to complete for container 'test_container'  [       OK ] MesosContainerizerProvisionerTest.ProvisionFailed (7 ms)  [ RUN      ] MesosContainerizerProvisionerTest.DestroyWhileProvisioning  I0324 13:38:56.291187  2944 containerizer.cpp:666] Starting container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2' for executor 'executor' of framework ''  I0324 13:38:56.292157  2944 containerizer.cpp:1421] Destroying container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2'  I0324 13:38:56.292179  2944 containerizer.cpp:1424] Waiting for the provisioner to complete for container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2'  F0324 13:38:56.292899  2944 containerizer.cpp:752] Check failed: containers_.contains(containerId)  *** Check failure stack trace: ***      @     0x2ac9973d0ae4  google::LogMessage::Fail()      @     0x2ac9973d0a30  google::LogMessage::SendToLog()      @     0x2ac9973d0432  google::LogMessage::Flush()      @     0x2ac9973d3346  google::LogMessageFatal::~LogMessageFatal()      @     0x2ac996af897c  mesos::internal::slave::MesosContainerizerProcess::_launch()      @     0x2ac996b1f18a  _ZZN7process8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS1_11ContainerIDERK6OptionINS1_8TaskInfoEERKNS1_12ExecutorInfoERKSsRKS8_ISsERKNS1_7SlaveIDERKNS_3PIDINS3_5SlaveEEEbRKS8_INS3_13ProvisionInfoEES5_SA_SD_SsSI_SL_SQ_bSU_EENS_6FutureIT_EERKNSO_IT0_EEMS10_FSZ_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_ENKUlPNS_11ProcessBaseEE_clES1P_      @     0x2ac996b479d9  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDERK6OptionINS5_8TaskInfoEERKNS5_12ExecutorInfoERKSsRKSC_ISsERKNS5_7SlaveIDERKNS0_3PIDINS7_5SlaveEEEbRKSC_INS7_13ProvisionInfoEES9_SE_SH_SsSM_SP_SU_bSY_EENS0_6FutureIT_EERKNSS_IT0_EEMS14_FS13_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_      @     0x2ac997334fef  std::function<>::operator()()      @     0x2ac99731b1c7  process::ProcessBase::visit()      @     0x2ac997321154  process::DispatchEvent::visit()      @           0x9a699c  process::ProcessBase::serve()      @     0x2ac9973173c0  process::ProcessManager::resume()      @     0x2ac99731445a  _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_      @     0x2ac997320916  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE      @     0x2ac9973208c6  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_      @     0x2ac997320858  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE      @     0x2ac9973207af  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv      @     0x2ac997320748  _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv      @     0x2ac9989aea60  (unknown)      @     0x2ac999125182  start_thread      @     0x2ac99943547d  (unknown)  make[4]: Leaving directory `/mesos/mesos-0.29.0/_build/src'  make[4]: *** [check-local] Aborted  make[3]: *** [check-am] Error 2  make[3]: Leaving directory `/mesos/mesos-0.29.0/_build/src'  make[2]: *** [check] Error 2  make[2]: Leaving directory `/mesos/mesos-0.29.0/_build/src'  make[1]: *** [check-recursive] Error 1  make[1]: Leaving directory `/mesos/mesos-0.29.0/_build'  make: *** [distcheck] Error 1  Build step 'Execute shell' marked build as failure  {noformat},2,1.2844012
MESOS-5027,Enable authenticated login in the webui,"The webui hits a number of endpoints to get the data that it displays: {{/state}}, {{/metrics/snapshot}}, {{/files/browse}}, {{/files/read}}, and maybe others? Once authentication is enabled on these endpoints, we need to add a login prompt to the webui so that users can provide credentials.",2,2.7115917
MESOS-5028,Copy provisioner cannot replace directory with symlink,"I'm trying to play with the new image provisioner on our custom docker images, but one of layer failed to get copied, possibly due to a dangling symlink.    Error log with Glog_v=1:    {quote}  I0324 05:42:48.926678 15067 copy.cpp:127] Copying layer path '/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs' to rootfs '/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6'  E0324 05:42:49.028506 15062 slave.cpp:3773] Container '5f05be6c-c970-4539-aa64-fd0eef2ec7ae' for executor 'test' of framework 75932a89-1514-4011-bafe-beb6a208bb2d-0004 failed to start: Collect failed: Collect failed: Failed to copy layer: cp: cannot overwrite directory ‘/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6/etc/apt’ with non-directory  {quote}    Content of _/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs/etc/apt_ points to a non-existing absolute path (cannot provide exact path but it's a result of us trying to mount apt keys into docker container at build time).    I believe what happened is that we executed a script at build time, which contains equivalent of:  {quote}  rm -rf /etc/apt/* && ln -sf /build-mount-point/ /etc/apt  {quote}  ",3,3.4200058
MESOS-5031,Authorization Action enum does not support upgrades.,"We need to make the Action enum optional in authorization::Request, and add an `UNKNOWN = 0;` enum value. See MESOS-4997 for details.",2,3.49845
MESOS-5032,Remove plain text Credential format (after deprecation cycle),"Currently two formats of credentials are supported: JSON    {code}    ""credentials"": [      {        ""principal"": ""sherman"",        ""secret"": ""kitesurf""      }  {code}    And a deprecated new line file:  {code}  principal1 secret1  pricipal2 secret2  {code}    We deprecated the new line format in 0.29, and should remove it after the deprecation cycle ends.",3,1.5319042
MESOS-5034,Design doc for ordered message delivery in libprocess,,3,1.8362164
MESOS-5044,Temporary directories created by environment->mkdtemp cleanup can be problematic.,"Currently in mesos test, we have the temporary directories created by `environment->mkdtemp()` cleaned up until the end of the test suite, which can be problematic. For instance, if we have many tests in a test suite, each of those tests is performing large size disk read/write in its temp dir, which may lead to out of disk issue on some resource limited machines.     We should have these temp dir created by `environment->mkdtemp` cleaned up during each test teardown. Currently we only clean up the sandbox for each test.",1,2.458487
MESOS-5049,Refactore subproces setup functions.,"Executing arbitrary setup functions while creating new processes is  dangerous as all functions called have to be async safe. As setup  functions are used for only very few purposes (setsid, chdir, monitoring  and killing a process (see upcoming review) it makes sense to support  them safely via parameters to subprocess.   Another common use of child setup are is to block the child while doing some work in the parent. This pattern can be more cleanly expressed with parentHooks. ",3,2.7914321
MESOS-5050,Design Linux capability support for Mesos containerizer,"We should at least support the following cases:  1) A root user has reduced capability  2) A non-root user has the capability of CAP_NET_ADMIN (to do e.g., tcpdump)",5,3.7372165
MESOS-5051,Create helpers for manipulating Linux capabilities.,"These helpers can either based on some existing library (e.g. libcap), or use system calls directly.",5,3.5513253
MESOS-5054,Namespace the stout flags,A recent name collision occurred when updating the 3rdparty http-parser library: https://github.com/apache/mesos/commit/94df63f72146501872a06c6487e94bdfd0f23025    We should put stout's {{flags}} namespace within another suitable namespace (perhaps {{stout::flags}}) to avoid such collisions.,2,2.411517
MESOS-5055,Slave/Agent Rename Phase I - Update strings in the log message and standard output,"This is a sub ticket of MESOS-3780. In this ticket, we will rename all the slave to agent in the log messages and standard output.",2,3.1855931
MESOS-5056,Replace Master/Slave Terminology Phase I - Update strings in the shell scripts outputs,"This is a sub ticket of MESOS-3780. In this ticket, we will rename slave to agent in the shell script outputs",1,2.805107
MESOS-5057,Slave/Agent Rename Phase I - Update strings in error messages and other strings,"This is a sub ticket of MESOS-3780. In this ticket, we will update all the slave to agent in the error messages and other strings in the code",3,2.2733731
MESOS-5062,Update the long-lived-framework example to run on test clusters,There are a couple of problems with the long-lived framework that prevent it from being deployed (easily) on an actual cluster:    * The framework will greedily accept all offers; it runs one executor per agent in the cluster.  * The framework assumes the {{long-lived-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments.  * The framework does not specify an resources with the executor.  This is required by many isolators.  * The framework has no metrics.,3,1.8812507
MESOS-5064,Remove default value for the agent `work_dir`,"Following a crash report from the user we need to be more explicit about the dangers of using {{/tmp}} as agent {{work_dir}}. In addition, we can remove the default value for the {{\-\-work_dir}} flag, forcing users to explicitly set the work directory for the agent.",2,0.715564
MESOS-5065,Support docker private registry default docker config.,"For docker private registry with authentication, docker containerizer should support using a default .docker/config.json file (or the old .dockercfg file) locally, which is pre-handled by operators. The default docker config file should be exposed by a new agent flag `--docker_config`. ",3,3.428316
MESOS-5069,Upgrade http-parser to v2.6.2,,3,1.3930215
MESOS-5070,Introduce more flexible subprocess interface for child options.,"We introduced a number of parameters to the subprocess interface with MESOS-5049.  Adding all options explicitly to the subprocess interface makes it inflexible.   We should investigate a flexible options, which still prevents arbitrary code to be executed.",2,3.309001
MESOS-5071,Refactor the clone option to subprocess.,The clone option in subprocess is only used (at least in the Mesos codebase) to specify custom namespace flags to clone. It feels having the clone function in the subprocess interface is too explicit for this functionality.  ,2,3.6166582
MESOS-5078,Document TaskStatus reasons,We should document the possible {{reason}} values that can be found in the {{TaskStatus}} message.,1,2.5416706
MESOS-5082,Fix a bug in the Nvidia GPU device isolator that exposes a discrepancy between clang and gcc in 'using' declarations,"There appears to be a discrepancy between clang and gcc, which allows  clang to accept `using` declarations of the form `using ns_name::name;`  that contain nested classes, structs, and enums after the `name` field  in the declaration (e.g. `using ns_name::name::enum;`).    The language for describing this functionality is ambiguous in the  C++11 specification as referenced here:  http://en.cppreference.com/w/cpp/language/namespace#Using-declarations",1,2.51408
MESOS-5101,Add CMake build to docker_build.sh,Add the CMake build system to docker_build.sh to automatically test the build on Jenkins alongside gcc and clang.,2,2.5750241
MESOS-5108,Design a short-term solution for a typed error handling mechanism.,,2,2.368116
MESOS-5109,Capture the error code in `ErrnoError` and `WindowsError`.,"The {{ErrnoError}} and {{WindowsError}} classes simply construct the error string via a mechanism such as {{strerror}}. They should also capture the error code, as it is an essential piece of information for such an error type.",2,1.3477603
MESOS-5110,Introduce an additional template parameter to `Try` for typed error.,"Add an additional template parameter {{E}} to the {{Try}} class template.    {code}  template <typename T, typename E = Error>  class Try {    /* ... */  };  {code}",3,2.385418
MESOS-5111,Update `network::connect` to use the typed error state of `Try`.,"{{network::connect}} function returns a {{Try<int>}} currently and the caller is required to inspect the state of {{errno}} out-of-band. {{network::connect}} should really return something like a {{Try<int, ErrnoError>}}.",2,1.2035041
MESOS-5112,Introduce `WindowsSocketError`.,"{{WindowsError}} invokes {{::GetLastError}} to retrieve the error code. Windows has a {{::WSAGetLastError}} function which at the interface level, is intended for failed socket operations. We should introduce a {{WindowsSocketError}} which invokes {{::WSAGetLastError}} and use them accordingly.",2,2.3750093
MESOS-5113,`network/cni` isolator crashes when launched without the --network_cni_plugins_dir flag,"If we start the agent with the --isolation='network/cni' but do not specify the --network_cni_plugins_dir flag, the agent crashes with the following stack dump:  0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56  56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.  (gdb) bt  #0  0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56  #1  0x00007ffff23280d8 in __GI_abort () at abort.c:89  #2  0x00007ffff231db86 in __assert_fail_base (fmt=0x7ffff246e830 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", assertion=assertion@entry=0x451f5c ""isSome()"",      file=file@entry=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=line@entry=111,      function=function@entry=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:92  #3  0x00007ffff231dc32 in __GI___assert_fail (assertion=0x451f5c ""isSome()"", file=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=111,      function=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:101  #4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111  Python Exception <class 'IndexError'> list index out of range:  #5  0x00007ffff63ef7cc in mesos::internal::slave::NetworkCniIsolatorProcess::recover (this=0x6c1e70, states=empty std::list, orphans=...) at ../../src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331  #6  0x00007ffff60cddd8 in operator() (this=0x7fffc0001e00, process=0x6c1ef8) at ../../3rdparty/libprocess/include/process/dispatch.hpp:239  #7  0x00007ffff60cd972 in std::_Function_handler<void (process::ProcessBase*), process::Future<Nothing> process::dispatch<Nothing, mesos::internal::slave::MesosIsolatorProcess, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > >(process::PID<mesos::internal::slave::MesosIsolatorProcess> const&, process::Future<Nothing> (mesos::internal::slave::MesosIsolatorProcess::*)(std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&), std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> >)::{lambda(process::ProcessBase*)#1}>::_M_invoke(std::_Any_data const&, process::ProcessBase*) (__functor=..., __args=0x6c1ef8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2071  #8  0x00007ffff6a6bf38 in std::function<void (process::ProcessBase*)>::operator()(process::ProcessBase*) const (this=0x7fffc0001d70, __args=0x6c1ef8)      at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2471  #9  0x00007ffff6a561b4 in process::ProcessBase::visit (this=0x6c1ef8, event=...) at ../../../3rdparty/libprocess/src/process.cpp:3130  #10 0x00007ffff6aac5fe in process::DispatchEvent::visit (this=0x7fffc0001570, visitor=0x6c1ef8) at ../../../3rdparty/libprocess/include/process/event.hpp:161  #11 0x00007ffff55e9c91 in process::ProcessBase::serve (this=0x6c1ef8, event=...) at ../../3rdparty/libprocess/include/process/process.hpp:82  #12 0x00007ffff6a53ed4 in process::ProcessManager::resume (this=0x67cca0, process=0x6c1ef8) at ../../../3rdparty/libprocess/src/process.cpp:2570  #13 0x00007ffff6a5bff5 in operator() (this=0x697d70, joining=...) at ../../../3rdparty/libprocess/src/process.cpp:2218  #14 0x00007ffff6a5bf33 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) (this=0x697d70,      __args=<unknown type in /home/vagrant/mesosphere/mesos/build/src/.libs/libmesos-0.29.0.so, CU 0x45bb552, DIE 0x469efe5>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1295  #15 0x00007ffff6a5bee6 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::operator()<, void>() (this=0x697d70)      at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1353  #16 0x00007ffff6a5be95 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x697d70)      at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1731  #17 0x00007ffff6a5be65 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::operator()() (this=0x697d70)      at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1720  #18 0x00007ffff6a5be3c in std::thread::_Impl<std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()> >::_M_run() (this=0x697d58)      at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/thread:115  #19 0x00007ffff2b98a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6  #20 0x00007ffff26bb182 in start_thread (arg=0x7fffeb92d700) at pthread_create.c:312  #21 0x00007ffff23e847d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111  (gdb) frame 4  #4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111",1,1.320704
MESOS-5114,Flags::parse does not handle empty string correctly.,"A missing default for quorum size has generated the following master config   {code}  MESOS_WORK_DIR=""/var/lib/mesos/master""  MESOS_ZK=""zk://zk1:2181,zk2:2181,zk3:2181/mesos""  MESOS_QUORUM=    MESOS_PORT=5050  MESOS_CLUSTER=""mesos""  MESOS_LOG_DIR=""/var/log/mesos""  MESOS_LOGBUFSECS=1  MESOS_LOGGING_LEVEL=""INFO""  {code}    This was causing each elected leader to attempt replica recovery.    E.g. {{group.cpp:700] Trying to get '/mesos/log_replicas/0000000012' in ZooKeeper}}    And eventually:  {{master.cpp:1458] Recovery failed: Failed to recover registrar: Failed to perform fetch within 1mins}}    Full log on one of the masters https://gist.github.com/clehene/09a9ddfe49b92a5deb4c1b421f63479e    All masters and zk nodes were reachable over the network.   Also once the quorum was configured the master recovery protocol finished gracefully.   ",2,2.3817236
MESOS-5115,Grant access to /dev/nvidiactl and /dev/nvidia-uvm in the Nvidia GPU isolator.," Calls to 'nvidia-smi'  fail inside a container even if access to a GPU has been granted. Moreover, access to /dev/nvidiactl is actually required for a container to do anything useful with a GPU even if it has access to it.        We should grant/revoke access to /dev/nvidiactl and /dev/nvidia-uvm as GPUs are added and removed from a container in the Nvidia GPU isolator.",2,2.3285036
MESOS-5121,pivot_root is not available on PowerPC,"When compile on ppc64le, it will through error message: src/linux/fs.cpp:443:2: error: #error ""pivot_root is not available""    The current code logic in src/linux/fs.cpp is:    {code}  #ifdef __NR_pivot_root    int ret = ::syscall(__NR_pivot_root, newRoot.c_str(), putOld.c_str());  #elif __x86_64__    // A workaround for systems that have an old glib but have a new    // kernel. The magic number '155' is the syscall number for    // 'pivot_root' on the x86_64 architecture, see    // arch/x86/syscalls/syscall_64.tbl    int ret = ::syscall(155, newRoot.c_str(), putOld.c_str());  #else  #error ""pivot_root is not available""  #endif  {code}    There is no old glib version and the new kernel version, it will never run code in *#ifdef __NR_pivot_root* condition, and when I build on Ubuntu 16.04(It has the latest linux kernel and glibc), it still can't step into the *#ifdef __NR_pivot_root* condition.    For powerpc case, I added another condition:    {code}  #elif __powerpc__ || __ppc__ || __powerpc64__ || __ppc64__    // A workaround for powerpc. The magic number '203' is the syscall    // number for 'pivot_root' on the powerpc architecture, see    // https://w3challs.com/syscalls/?arch=powerpc_64    int ret = ::syscall(203, newRoot.c_str(), putOld.c_str());  {code}",1,1.6807461
MESOS-5124,TASK_KILLING is not supported by mesos-execute.,Recently {{TASK_KILLING}} state (MESOS-4547) have been introduced to Mesos. We should add support for this feature to {{mesos-execute}}.,3,2.0088336
MESOS-5125,"Commit message hook iterates over words, rather than lines.","{{for LINE in $COMMIT_MESSAGE}} iterates over one word at a time, rather than one line at a time. We should use the following pattern instead:  {code}  while read LINE;  do    ...  done <<< ""$COMMIT_MESSAGE""  {code}",2,2.4753692
MESOS-5126,Commit message hook iterates over the commented lines.,"Currently, the commit message hook iterates over the commented lines.  For example, if there is a modified file for which its path is longer than 72 characters, the commit hook errors out. We should skip over the commented lines.",2,2.3634279
MESOS-5127,Reset `LIBPROCESS_IP` in `network\cni` isolator.,"Currently the `LIBPROCESS_IP` environment variable was being set to      the Agent IP if the environment variable has not be defined by the      `Framework`. For containers having their own IP address (as with      containers on CNI networks) this becomes a problem since the command      executor tries to bind to the `LIBPROCESS_IP` that does not exist in      its network namespace, and fails. Thus, for containers launched on CNI      networks the `LIBPROCESS_IP` should not be set, or rather is set to      ""0.0.0.0"", allowing the container to bind to the IP address provided      by the CNI network.",1,1.7030602
MESOS-5128,PersistentVolumeTest.AccessPersistentVolume is flaky,"Observed on ASF CI:    {code}  [ RUN      ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0  I0405 17:29:19.134435 31837 cluster.cpp:139] Creating default 'local' authorizer  I0405 17:29:19.251143 31837 leveldb.cpp:174] Opened db in 116.386403ms  I0405 17:29:19.310050 31837 leveldb.cpp:181] Compacted db in 58.80688ms  I0405 17:29:19.310180 31837 leveldb.cpp:196] Created db iterator in 37145ns  I0405 17:29:19.310199 31837 leveldb.cpp:202] Seeked to beginning of db in 4212ns  I0405 17:29:19.310210 31837 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns  I0405 17:29:19.310279 31837 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0405 17:29:19.311069 31861 recover.cpp:447] Starting replica recovery  I0405 17:29:19.311362 31861 recover.cpp:473] Replica is in EMPTY status  I0405 17:29:19.312641 31861 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (14359)@172.17.0.4:43972  I0405 17:29:19.313045 31860 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0405 17:29:19.313608 31860 recover.cpp:564] Updating replica status to STARTING  I0405 17:29:19.316416 31867 master.cpp:376] Master 9565ff6f-f1b6-4259-8430-690e635c391f (4090d10eba90) started on 172.17.0.4:43972  I0405 17:29:19.316470 31867 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/0A9ELu/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/0A9ELu/master"" --zk_session_timeout=""10secs""  I0405 17:29:19.316938 31867 master.cpp:427] Master only allowing authenticated frameworks to register  I0405 17:29:19.316951 31867 master.cpp:432] Master only allowing authenticated agents to register  I0405 17:29:19.316961 31867 credentials.hpp:37] Loading credentials for authentication from '/tmp/0A9ELu/credentials'  I0405 17:29:19.317402 31867 master.cpp:474] Using default 'crammd5' authenticator  I0405 17:29:19.317643 31867 master.cpp:545] Using default 'basic' HTTP authenticator  I0405 17:29:19.317854 31867 master.cpp:583] Authorization enabled  I0405 17:29:19.318081 31864 whitelist_watcher.cpp:77] No whitelist given  I0405 17:29:19.318079 31861 hierarchical.cpp:144] Initialized hierarchical allocator process  I0405 17:29:19.320838 31864 master.cpp:1826] The newly elected leader is master@172.17.0.4:43972 with id 9565ff6f-f1b6-4259-8430-690e635c391f  I0405 17:29:19.320888 31864 master.cpp:1839] Elected as the leading master!  I0405 17:29:19.320909 31864 master.cpp:1526] Recovering from registrar  I0405 17:29:19.321218 31871 registrar.cpp:331] Recovering registrar  I0405 17:29:19.347045 31860 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 33.164133ms  I0405 17:29:19.347126 31860 replica.cpp:320] Persisted replica status to STARTING  I0405 17:29:19.347611 31869 recover.cpp:473] Replica is in STARTING status  I0405 17:29:19.349215 31871 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (14361)@172.17.0.4:43972  I0405 17:29:19.349653 31870 recover.cpp:193] Received a recover response from a replica in STARTING status  I0405 17:29:19.350236 31866 recover.cpp:564] Updating replica status to VOTING  I0405 17:29:19.388882 31864 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.38299ms  I0405 17:29:19.388993 31864 replica.cpp:320] Persisted replica status to VOTING  I0405 17:29:19.389369 31856 recover.cpp:578] Successfully joined the Paxos group  I0405 17:29:19.389735 31856 recover.cpp:462] Recover process terminated  I0405 17:29:19.390476 31868 log.cpp:659] Attempting to start the writer  I0405 17:29:19.392125 31862 replica.cpp:493] Replica received implicit promise request from (14362)@172.17.0.4:43972 with proposal 1  I0405 17:29:19.430706 31862 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.505062ms  I0405 17:29:19.430816 31862 replica.cpp:342] Persisted promised to 1  I0405 17:29:19.431918 31856 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0405 17:29:19.433725 31861 replica.cpp:388] Replica received explicit promise request from (14363)@172.17.0.4:43972 for position 0 with proposal 2  I0405 17:29:19.472491 31861 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 38.659492ms  I0405 17:29:19.472595 31861 replica.cpp:712] Persisted action at 0  I0405 17:29:19.474556 31864 replica.cpp:537] Replica received write request for position 0 from (14364)@172.17.0.4:43972  I0405 17:29:19.474652 31864 leveldb.cpp:436] Reading position from leveldb took 49423ns  I0405 17:29:19.528175 31864 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 53.443616ms  I0405 17:29:19.528300 31864 replica.cpp:712] Persisted action at 0  I0405 17:29:19.529389 31865 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0405 17:29:19.571137 31865 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 41.676495ms  I0405 17:29:19.571254 31865 replica.cpp:712] Persisted action at 0  I0405 17:29:19.571302 31865 replica.cpp:697] Replica learned NOP action at position 0  I0405 17:29:19.572322 31856 log.cpp:675] Writer started with ending position 0  I0405 17:29:19.574060 31861 leveldb.cpp:436] Reading position from leveldb took 83200ns  I0405 17:29:19.575417 31864 registrar.cpp:364] Successfully fetched the registry (0B) in 0ns  I0405 17:29:19.575565 31864 registrar.cpp:463] Applied 1 operations in 46419ns; attempting to update the 'registry'  I0405 17:29:19.576517 31857 log.cpp:683] Attempting to append 170 bytes to the log  I0405 17:29:19.576849 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0405 17:29:19.578390 31857 replica.cpp:537] Replica received write request for position 1 from (14365)@172.17.0.4:43972  I0405 17:29:19.780277 31857 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 201.808617ms  I0405 17:29:19.780366 31857 replica.cpp:712] Persisted action at 1  I0405 17:29:19.782024 31857 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0405 17:29:19.823770 31857 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 41.667662ms  I0405 17:29:19.823851 31857 replica.cpp:712] Persisted action at 1  I0405 17:29:19.823889 31857 replica.cpp:697] Replica learned APPEND action at position 1  I0405 17:29:19.825701 31867 registrar.cpp:508] Successfully updated the 'registry' in 0ns  I0405 17:29:19.825929 31867 registrar.cpp:394] Successfully recovered registrar  I0405 17:29:19.826015 31857 log.cpp:702] Attempting to truncate the log to 1  I0405 17:29:19.826262 31867 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0405 17:29:19.827647 31867 replica.cpp:537] Replica received write request for position 2 from (14366)@172.17.0.4:43972  I0405 17:29:19.828018 31857 master.cpp:1634] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register  I0405 17:29:19.828065 31861 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover  I0405 17:29:19.865555 31867 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.822178ms  I0405 17:29:19.865661 31867 replica.cpp:712] Persisted action at 2  I0405 17:29:19.866921 31867 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0405 17:29:19.907341 31867 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 40.356649ms  I0405 17:29:19.907531 31867 leveldb.cpp:399] Deleting ~1 keys from leveldb took 91109ns  I0405 17:29:19.907560 31867 replica.cpp:712] Persisted action at 2  I0405 17:29:19.907599 31867 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0405 17:29:19.923305 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:2048  Trying semicolon-delimited string format instead  I0405 17:29:19.926491 31837 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix  W0405 17:29:19.927836 31837 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges  I0405 17:29:19.932029 31862 slave.cpp:200] Agent started on 441)@172.17.0.4:43972  I0405 17:29:19.932086 31862 slave.cpp:201] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""[{""name"":""cpus"",""role"":""*"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""role"":""*"",""scalar"":{""value"":2048.0},""type"":""SCALAR""},{""name"":""disk"",""role"":""role1"",""scalar"":{""value"":4096.0},""type"":""SCALAR""}]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC""  I0405 17:29:19.932665 31862 credentials.hpp:86] Loading credential for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential'  I0405 17:29:19.932934 31862 slave.cpp:338] Agent using credential for: test-principal  I0405 17:29:19.932968 31862 credentials.hpp:37] Loading credentials for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials'  I0405 17:29:19.933284 31862 slave.cpp:390] Using default 'basic' HTTP authenticator  I0405 17:29:19.934916 31837 sched.cpp:222] Version: 0.29.0  I0405 17:29:19.935566 31862 slave.cpp:589] Agent resources: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000]  I0405 17:29:19.935664 31862 slave.cpp:597] Agent attributes: [  ]  I0405 17:29:19.935679 31862 slave.cpp:602] Agent hostname: 4090d10eba90  I0405 17:29:19.938390 31864 state.cpp:57] Recovering state from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta'  I0405 17:29:19.940608 31869 sched.cpp:326] New master detected at master@172.17.0.4:43972  I0405 17:29:19.940749 31869 sched.cpp:382] Authenticating with master master@172.17.0.4:43972  I0405 17:29:19.940773 31869 sched.cpp:389] Using default CRAM-MD5 authenticatee  I0405 17:29:19.942371 31869 authenticatee.cpp:121] Creating new client SASL connection  I0405 17:29:19.942873 31859 master.cpp:5679] Authenticating scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972  I0405 17:29:19.943156 31859 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(896)@172.17.0.4:43972  I0405 17:29:19.943507 31863 authenticator.cpp:98] Creating new server SASL connection  I0405 17:29:19.943740 31859 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0405 17:29:19.943783 31859 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0405 17:29:19.943892 31859 authenticator.cpp:203] Received SASL authentication start  I0405 17:29:19.943977 31859 authenticator.cpp:325] Authentication requires more steps  I0405 17:29:19.944066 31859 authenticatee.cpp:258] Received SASL authentication step  I0405 17:29:19.944164 31859 authenticator.cpp:231] Received SASL authentication step  I0405 17:29:19.944193 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0405 17:29:19.944206 31859 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0405 17:29:19.944268 31859 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0405 17:29:19.944300 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0405 17:29:19.944313 31859 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0405 17:29:19.944321 31859 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0405 17:29:19.944339 31859 authenticator.cpp:317] Authentication success  I0405 17:29:19.944541 31859 authenticatee.cpp:298] Authentication success  I0405 17:29:19.944655 31859 master.cpp:5709] Successfully authenticated principal 'test-principal' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972  I0405 17:29:19.944737 31859 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(896)@172.17.0.4:43972  I0405 17:29:19.945111 31859 sched.cpp:472] Successfully authenticated with master master@172.17.0.4:43972  I0405 17:29:19.945132 31859 sched.cpp:777] Sending SUBSCRIBE call to master@172.17.0.4:43972  I0405 17:29:19.945591 31859 sched.cpp:810] Will retry registration in 372.80738ms if necessary  I0405 17:29:19.945744 31865 master.cpp:2346] Received SUBSCRIBE call for framework 'default' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972  I0405 17:29:19.945838 31865 master.cpp:1865] Authorizing framework principal 'test-principal' to receive offers for role 'role1'  I0405 17:29:19.946194 31865 master.cpp:2417] Subscribing framework default with checkpointing disabled and capabilities [  ]  I0405 17:29:19.946866 31866 hierarchical.cpp:266] Added framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000  I0405 17:29:19.946974 31866 hierarchical.cpp:1490] No resources available to allocate!  I0405 17:29:19.947010 31866 hierarchical.cpp:1585] No inverse offers to send out!  I0405 17:29:19.947054 31865 sched.cpp:704] Framework registered with 9565ff6f-f1b6-4259-8430-690e635c391f-0000  I0405 17:29:19.947074 31866 hierarchical.cpp:1141] Performed allocation for 0 agents in 178242ns  I0405 17:29:19.947124 31865 sched.cpp:718] Scheduler::registered took 38907ns  I0405 17:29:19.948712 31866 status_update_manager.cpp:200] Recovering status update manager  I0405 17:29:19.948901 31866 containerizer.cpp:416] Recovering containerizer  I0405 17:29:19.951021 31866 provisioner.cpp:245] Provisioner recovery complete  I0405 17:29:19.951802 31866 slave.cpp:4773] Finished recovery  I0405 17:29:19.952518 31866 slave.cpp:4945] Querying resource estimator for oversubscribable resources  I0405 17:29:19.953248 31866 slave.cpp:928] New master detected at master@172.17.0.4:43972  I0405 17:29:19.953305 31865 status_update_manager.cpp:174] Pausing sending status updates  I0405 17:29:19.953626 31866 slave.cpp:991] Authenticating with master master@172.17.0.4:43972  I0405 17:29:19.953716 31866 slave.cpp:996] Using default CRAM-MD5 authenticatee  I0405 17:29:19.954074 31866 slave.cpp:964] Detecting new master  I0405 17:29:19.954167 31861 authenticatee.cpp:121] Creating new client SASL connection  I0405 17:29:19.954372 31866 slave.cpp:4959] Received oversubscribable resources  from the resource estimator  I0405 17:29:19.954756 31866 master.cpp:5679] Authenticating slave(441)@172.17.0.4:43972  I0405 17:29:19.954944 31861 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(897)@172.17.0.4:43972  I0405 17:29:19.955368 31863 authenticator.cpp:98] Creating new server SASL connection  I0405 17:29:19.955687 31861 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0405 17:29:19.955801 31861 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0405 17:29:19.956075 31861 authenticator.cpp:203] Received SASL authentication start  I0405 17:29:19.956279 31861 authenticator.cpp:325] Authentication requires more steps  I0405 17:29:19.956455 31861 authenticatee.cpp:258] Received SASL authentication step  I0405 17:29:19.956676 31861 authenticator.cpp:231] Received SASL authentication step  I0405 17:29:19.956815 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0405 17:29:19.956907 31861 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0405 17:29:19.957044 31861 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0405 17:29:19.957166 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0405 17:29:19.957264 31861 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0405 17:29:19.957353 31861 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0405 17:29:19.957449 31861 authenticator.cpp:317] Authentication success  I0405 17:29:19.957664 31857 authenticatee.cpp:298] Authentication success  I0405 17:29:19.957813 31857 master.cpp:5709] Successfully authenticated principal 'test-principal' at slave(441)@172.17.0.4:43972  I0405 17:29:19.958008 31861 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(897)@172.17.0.4:43972  I0405 17:29:19.958732 31857 slave.cpp:1061] Successfully authenticated with master master@172.17.0.4:43972  I0405 17:29:19.958930 31857 slave.cpp:1457] Will retry registration in 18.568334ms if necessary  I0405 17:29:19.959262 31857 master.cpp:4390] Registering agent at slave(441)@172.17.0.4:43972 (4090d10eba90) with id 9565ff6f-f1b6-4259-8430-690e635c391f-S0  I0405 17:29:19.959934 31857 registrar.cpp:463] Applied 1 operations in 99197ns; attempting to update the 'registry'  I0405 17:29:19.961587 31857 log.cpp:683] Attempting to append 343 bytes to the log  I0405 17:29:19.9...",3,1.8752465
MESOS-5130,Enable `newtork/cni` isolator in `MesosContainerizer` as the default `network` isolator.,"Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator.",1,1.9360985
MESOS-5132,Commit message hook lints the diff in verbose mode.,"In verbose mode (i.e., {{git commit --verbose}}), the commit message includes the diff of the commit at the bottom, delimited by the following lines:    {code}  # ------------------------ >8 ------------------------  # Do not touch the line above.  # Everything below will be removed.  {code}    We should {{break}} once we encounter such a line.",2,2.0822916
MESOS-5133,Expose TaskStatus source & reason in master's '/state' output,It would be helpful if the TaskStatus lists provided by the master's {{/state}} endpoint included the {{source}} and {{reason}} associated with the status message. The JSON modeling function for TaskStatus should be extended to include these fields.,1,2.8622932
MESOS-5135,Update existing documentation to Include references to GPUs as a first class resource.,"Specifically, the documentation in the following files should be udated:    {noformat}  docs/attributes-resources.md  docs/monitoring.md  {noformat}",1,2.4615407
MESOS-5136,Update the default JSON representation of a Resource to include GPUs,"The default JSON representation of a Resource currently lists a value of ""0"" if no value is set on a first class SCALAR resource (i.e. cpus, mem, disk).  We should add GPUs in here as well.  ",1,1.4878933
MESOS-5137,Remove 'dashboard.js' from the webui.,This file is no longer in use anywhere.,1,1.7897899
MESOS-5138,Fix Nvidia GPU test build for namespace change of MasterDetector,An update to master the day after all of the Nvidia GPU stuff landed has a build error in the Nvidia GPU tests. The namespace that MasterDetector lives in has changed and the test needs to be updated to pull in the class from the proper namespace now.,1,1.7435203
MESOS-5139,ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar is flaky,"Found this on ASF CI while testing 0.28.1-rc2    {code}  [ RUN      ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar  E0406 18:29:30.870481   520 shell.hpp:93] Command 'hadoop version 2>&1' failed; this is the output:  sh: 1: hadoop: not found  E0406 18:29:30.870576   520 fetcher.cpp:59] Failed to create URI fetcher plugin 'hadoop': Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127  I0406 18:29:30.871052   520 local_puller.cpp:90] Creating local puller with docker registry '/tmp/3l8ZBv/images'  I0406 18:29:30.873325   539 metadata_manager.cpp:159] Looking for image 'abc'  I0406 18:29:30.874438   539 local_puller.cpp:142] Untarring image 'abc' from '/tmp/3l8ZBv/images/abc.tar' to '/tmp/3l8ZBv/store/staging/5tw8bD'  I0406 18:29:30.901916   547 local_puller.cpp:162] The repositories JSON file for image 'abc' is '{""abc"":{""latest"":""456""}}'  I0406 18:29:30.902304   547 local_puller.cpp:290] Extracting layer tar ball '/tmp/3l8ZBv/store/staging/5tw8bD/123/layer.tar to rootfs '/tmp/3l8ZBv/store/staging/5tw8bD/123/rootfs'  I0406 18:29:30.909144   547 local_puller.cpp:290] Extracting layer tar ball '/tmp/3l8ZBv/store/staging/5tw8bD/456/layer.tar to rootfs '/tmp/3l8ZBv/store/staging/5tw8bD/456/rootfs'  ../../src/tests/containerizer/provisioner_docker_tests.cpp:183: Failure  (imageInfo).failure(): Collect failed: Subprocess 'tar, tar, -x, -f, /tmp/3l8ZBv/store/staging/5tw8bD/456/layer.tar, -C, /tmp/3l8ZBv/store/staging/5tw8bD/456/rootfs' failed: tar: This does not look like a tar archive  tar: Exiting with failure status due to previous errors    [  FAILED  ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar (243 ms)  {code}",2,1.9755133
MESOS-5142,Add agent flags for HTTP authorization.,Flags should be added to the agent to:  1. Enable authorization ({{--authorizers}})  2. Provide ACLs ({{--acls}}),2,2.4007125
MESOS-5144,Cleanup memory leaks in libprocess finalize(),"libprocess's {{finalize}} function currently leaks memory for a few different reasons. Cleaning up the {{SocketManager}} will be somewhat involved (MESOS-3910), but the remaining memory leaks should be fairly easy to address.",2,2.5812426
MESOS-5146,MasterAllocatorTest/1.RebalancedForUpdatedWeights is flaky.,"Observed on the ASF CI:    {code}  [ RUN      ] MasterAllocatorTest/1.RebalancedForUpdatedWeights  I0407 22:34:10.330394 29278 cluster.cpp:149] Creating default 'local' authorizer  I0407 22:34:10.466182 29278 leveldb.cpp:174] Opened db in 135.608207ms  I0407 22:34:10.516398 29278 leveldb.cpp:181] Compacted db in 50.159558ms  I0407 22:34:10.516464 29278 leveldb.cpp:196] Created db iterator in 34959ns  I0407 22:34:10.516484 29278 leveldb.cpp:202] Seeked to beginning of db in 10195ns  I0407 22:34:10.516496 29278 leveldb.cpp:271] Iterated through 0 keys in the db in 7324ns  I0407 22:34:10.516547 29278 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0407 22:34:10.517277 29298 recover.cpp:447] Starting replica recovery  I0407 22:34:10.517693 29300 recover.cpp:473] Replica is in EMPTY status  I0407 22:34:10.520251 29310 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4775)@172.17.0.3:35855  I0407 22:34:10.520611 29311 recover.cpp:193] Received a recover response from a replica in EMPTY status  I0407 22:34:10.521164 29299 recover.cpp:564] Updating replica status to STARTING  I0407 22:34:10.523435 29298 master.cpp:382] Master f59f9057-a5c7-43e1-b129-96862e640a12 (129e11060069) started on 172.17.0.3:35855  I0407 22:34:10.523473 29298 master.cpp:384] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/3rZY8C/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/3rZY8C/master"" --zk_session_timeout=""10secs""  I0407 22:34:10.523885 29298 master.cpp:433] Master only allowing authenticated frameworks to register  I0407 22:34:10.523901 29298 master.cpp:438] Master only allowing authenticated agents to register  I0407 22:34:10.523913 29298 credentials.hpp:37] Loading credentials for authentication from '/tmp/3rZY8C/credentials'  I0407 22:34:10.524298 29298 master.cpp:480] Using default 'crammd5' authenticator  I0407 22:34:10.524441 29298 master.cpp:551] Using default 'basic' HTTP authenticator  I0407 22:34:10.524564 29298 master.cpp:589] Authorization enabled  I0407 22:34:10.525269 29305 hierarchical.cpp:145] Initialized hierarchical allocator process  I0407 22:34:10.525333 29305 whitelist_watcher.cpp:77] No whitelist given  I0407 22:34:10.527331 29298 master.cpp:1832] The newly elected leader is master@172.17.0.3:35855 with id f59f9057-a5c7-43e1-b129-96862e640a12  I0407 22:34:10.527441 29298 master.cpp:1845] Elected as the leading master!  I0407 22:34:10.527545 29298 master.cpp:1532] Recovering from registrar  I0407 22:34:10.527889 29298 registrar.cpp:331] Recovering registrar  I0407 22:34:10.549734 29299 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 28.25177ms  I0407 22:34:10.549782 29299 replica.cpp:320] Persisted replica status to STARTING  I0407 22:34:10.550010 29299 recover.cpp:473] Replica is in STARTING status  I0407 22:34:10.551352 29299 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4777)@172.17.0.3:35855  I0407 22:34:10.551676 29299 recover.cpp:193] Received a recover response from a replica in STARTING status  I0407 22:34:10.552315 29308 recover.cpp:564] Updating replica status to VOTING  I0407 22:34:10.574865 29308 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 22.413614ms  I0407 22:34:10.574928 29308 replica.cpp:320] Persisted replica status to VOTING  I0407 22:34:10.575103 29308 recover.cpp:578] Successfully joined the Paxos group  I0407 22:34:10.575346 29308 recover.cpp:462] Recover process terminated  I0407 22:34:10.575913 29308 log.cpp:659] Attempting to start the writer  I0407 22:34:10.577512 29308 replica.cpp:493] Replica received implicit promise request from (4778)@172.17.0.3:35855 with proposal 1  I0407 22:34:10.599984 29308 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 22.453613ms  I0407 22:34:10.600026 29308 replica.cpp:342] Persisted promised to 1  I0407 22:34:10.601773 29304 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0407 22:34:10.603757 29307 replica.cpp:388] Replica received explicit promise request from (4779)@172.17.0.3:35855 for position 0 with proposal 2  I0407 22:34:10.634392 29307 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.269987ms  I0407 22:34:10.634829 29307 replica.cpp:712] Persisted action at 0  I0407 22:34:10.637017 29297 replica.cpp:537] Replica received write request for position 0 from (4780)@172.17.0.3:35855  I0407 22:34:10.637099 29297 leveldb.cpp:436] Reading position from leveldb took 52948ns  I0407 22:34:10.676170 29297 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 38.917487ms  I0407 22:34:10.676352 29297 replica.cpp:712] Persisted action at 0  I0407 22:34:10.677564 29306 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0407 22:34:10.717959 29306 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 40.306229ms  I0407 22:34:10.718202 29306 replica.cpp:712] Persisted action at 0  I0407 22:34:10.718399 29306 replica.cpp:697] Replica learned NOP action at position 0  I0407 22:34:10.719883 29306 log.cpp:675] Writer started with ending position 0  I0407 22:34:10.721688 29305 leveldb.cpp:436] Reading position from leveldb took 75934ns  I0407 22:34:10.723640 29306 registrar.cpp:364] Successfully fetched the registry (0B) in 195648us  I0407 22:34:10.723999 29306 registrar.cpp:463] Applied 1 operations in 108099ns; attempting to update the 'registry'  I0407 22:34:10.725077 29311 log.cpp:683] Attempting to append 170 bytes to the log  I0407 22:34:10.725328 29308 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0407 22:34:10.726552 29299 replica.cpp:537] Replica received write request for position 1 from (4781)@172.17.0.3:35855  I0407 22:34:10.759747 29299 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.089719ms  I0407 22:34:10.759976 29299 replica.cpp:712] Persisted action at 1  I0407 22:34:10.761739 29299 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0407 22:34:10.801522 29299 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 39.694064ms  I0407 22:34:10.801602 29299 replica.cpp:712] Persisted action at 1  I0407 22:34:10.801638 29299 replica.cpp:697] Replica learned APPEND action at position 1  I0407 22:34:10.803371 29311 registrar.cpp:508] Successfully updated the 'registry' in 79.163904ms  I0407 22:34:10.803829 29311 registrar.cpp:394] Successfully recovered registrar  I0407 22:34:10.804585 29311 master.cpp:1640] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register  I0407 22:34:10.805269 29308 log.cpp:702] Attempting to truncate the log to 1  I0407 22:34:10.805721 29310 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0407 22:34:10.805276 29296 hierarchical.cpp:172] Skipping recovery of hierarchical allocator: nothing to recover  I0407 22:34:10.806529 29307 replica.cpp:537] Replica received write request for position 2 from (4782)@172.17.0.3:35855  I0407 22:34:10.843320 29307 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 36.77593ms  I0407 22:34:10.843531 29307 replica.cpp:712] Persisted action at 2  I0407 22:34:10.845369 29311 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0407 22:34:10.885098 29311 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 39.641102ms  I0407 22:34:10.885401 29311 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88701ns  I0407 22:34:10.885745 29311 replica.cpp:712] Persisted action at 2  I0407 22:34:10.885862 29311 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0407 22:34:10.900660 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix  W0407 22:34:10.901793 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges  I0407 22:34:10.905488 29302 slave.cpp:201] Agent started on 111)@172.17.0.3:35855  I0407 22:34:10.905553 29302 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa""  I0407 22:34:10.906365 29302 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/credential'  I0407 22:34:10.906787 29302 slave.cpp:339] Agent using credential for: test-principal  I0407 22:34:10.907202 29302 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/http_credentials'  I0407 22:34:10.907713 29302 slave.cpp:391] Using default 'basic' HTTP authenticator  I0407 22:34:10.908499 29302 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]  Trying semicolon-delimited string format instead  I0407 22:34:10.910189 29302 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]  I0407 22:34:10.910362 29302 slave.cpp:598] Agent attributes: [  ]  I0407 22:34:10.910465 29302 slave.cpp:603] Agent hostname: 129e11060069  I0407 22:34:10.913280 29303 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/meta'  I0407 22:34:10.914621 29303 status_update_manager.cpp:200] Recovering status update manager  I0407 22:34:10.915226 29303 containerizer.cpp:416] Recovering containerizer  I0407 22:34:10.917246 29301 provisioner.cpp:245] Provisioner recovery complete  I0407 22:34:10.917733 29301 slave.cpp:4784] Finished recovery  I0407 22:34:10.918226 29301 slave.cpp:4956] Querying resource estimator for oversubscribable resources  I0407 22:34:10.918529 29301 slave.cpp:4970] Received oversubscribable resources  from the resource estimator  I0407 22:34:10.918908 29304 slave.cpp:939] New master detected at master@172.17.0.3:35855  I0407 22:34:10.918988 29304 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855  I0407 22:34:10.919098 29301 status_update_manager.cpp:174] Pausing sending status updates  I0407 22:34:10.919309 29304 slave.cpp:1007] Using default CRAM-MD5 authenticatee  I0407 22:34:10.919535 29304 slave.cpp:975] Detecting new master  I0407 22:34:10.919747 29308 authenticatee.cpp:121] Creating new client SASL connection  I0407 22:34:10.920413 29308 master.cpp:5695] Authenticating slave(111)@172.17.0.3:35855  I0407 22:34:10.920650 29308 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(278)@172.17.0.3:35855  I0407 22:34:10.921020 29308 authenticator.cpp:98] Creating new server SASL connection  I0407 22:34:10.921308 29308 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5  I0407 22:34:10.921424 29308 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'  I0407 22:34:10.921596 29308 authenticator.cpp:203] Received SASL authentication start  I0407 22:34:10.921752 29308 authenticator.cpp:325] Authentication requires more steps  I0407 22:34:10.921957 29307 authenticatee.cpp:258] Received SASL authentication step  I0407 22:34:10.922178 29308 authenticator.cpp:231] Received SASL authentication step  I0407 22:34:10.922214 29308 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0407 22:34:10.922229 29308 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  I0407 22:34:10.922281 29308 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0407 22:34:10.922309 29308 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0407 22:34:10.922322 29308 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0407 22:34:10.922332 29308 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0407 22:34:10.922353 29308 authenticator.cpp:317] Authentication success  I0407 22:34:10.922436 29307 authenticatee.cpp:298] Authentication success  I0407 22:34:10.922587 29308 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(111)@172.17.0.3:35855  I0407 22:34:10.922668 29299 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(278)@172.17.0.3:35855  I0407 22:34:10.923256 29307 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855  I0407 22:34:10.923429 29307 slave.cpp:1468] Will retry registration in 3.220345ms if necessary  I0407 22:34:10.923707 29302 master.cpp:4406] Registering agent at slave(111)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S0  I0407 22:34:10.924239 29309 registrar.cpp:463] Applied 1 operations in 105794ns; attempting to update the 'registry'  I0407 22:34:10.925787 29309 log.cpp:683] Attempting to append 339 bytes to the log  I0407 22:34:10.926028 29309 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0407 22:34:10.927139 29309 replica.cpp:537] Replica received write request for position 3 from (4797)@172.17.0.3:35855  I0407 22:34:10.929083 29305 slave.cpp:1468] Will retry registration in 39.293556ms if necessary  I0407 22:34:10.929363 29305 master.cpp:4394] Ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress  I0407 22:34:10.968843 29309 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 41.68025ms  I0407 22:34:10.969005 29309 replica.cpp:712] Persisted action at 3  I0407 22:34:10.969741 29309 slave.cpp:1468] Will retry registration in 54.852242ms if necessary  I0407 22:34:10.970118 29309 master.cpp:4394] Ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress  I0407 22:34:10.970852 29306 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I0407 22:34:11.010634 29306 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 39.680272ms  I0407 22:34:11.010840 29306 replica.cpp:712] Persisted action at 3  I0407 22:34:11.011014 29306 replica.cpp:697] Replica learned APPEND action at position 3  I0407 22:34:11.014020 29306 registrar.cpp:508] Successfully updated the 'registry' in 89.684224ms  I0407 22:34:11.014181 29296 log.cpp:702] Attempting to truncate the log to 3  I0407 22:34:11.014606 29296 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4  I0407 22:34:11.015836 29298 replica.cpp:537] Replica received write request for position 4 from (4798)@172.17.0.3:35855  I0407 22:34:11.016973 29296 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]  I0407 22:34:11.017518 29304 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )  I0407 22:34:11.017763 29311 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S0  I0407 22:34:11.018362 29311 fetcher.cpp:81] Clearing fetcher cache  I0407 22:34:11.018870 29311 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S0/slave.info'  I0407 22:34:11.018890 29307 status_update_manager.cpp:181] Resuming sending status updates  I0407 22:34:11.019182 29304 hierarchical.cpp:1491] No resources available to allocate!  I0407 22:34:11.019304 29304 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 in 1.077349ms  I0407 22:34:11.019493 29311 slave.cpp:1176] Forwarding total oversubscribed resources   I0407 22:34:11.019726 29311 slave.cpp:3675] Received ping from slave-observer(112)@172.17.0.3:35855  I0407 22:34:11.019878 29299 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources   I0407 22:34:11.020845 29305 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )  I0407 22:34:11.021005 29305 hierarchical.cpp:1491] No resources available to allocate!  I0407 22:34:11.021065 29305 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 in 173907ns  I0407 22:34:11.022289 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix  W0407 22:34:11.023422 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges  I0407 22:34:11.026309 29309 slave.cpp:201] Agent started on 112)@172.17.0.3:35855  I0407 22:34:11.026410 29309 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_e...",1,1.5199428
MESOS-5152,Add authentication to agent's /monitor/statistics endpoint,"Operators may want to enforce that only authenticated users (and subsequently only specific authorized users) be able to view per-executor resource usage statistics.  Since this endpoint is handled by the ResourceMonitorProcess, I would expect the work necessary to be similar to what was done for /files or /registry endpoint authn.",2,2.3635166
MESOS-5153,Sandboxes contents should be protected from unauthorized users,"MESOS-4956 introduced authentication support for the sandboxes. However, authentication can only go as far as to tell whether an user is known to mesos or not. An extra additional step is necessary to verify whether the known user is allowed to executed the requested operation on the sandbox (browse, read, download, debug).",8,2.3544104
MESOS-5155,Consolidate authorization actions for quota.,"We should have just a single authz action: {{UPDATE_QUOTA_WITH_ROLE}}. It was a mistake in retrospect to introduce multiple actions.    Actions that are not symmetrical are register/teardown and dynamic reservations. The way they are implemented in this way is because entities that do one action differ from entities that do the other. For example, register framework is issued by a framework, teardown by an operator. What is a good way to identify a framework? A role it runs in, which may be different each launch and makes no sense in multi-role frameworks setup or better a sort of a group id, which is its principal. For dynamic reservations and persistent volumes, they can be both issued by frameworks and operators, hence similar reasoning applies.     Now, quota is associated with a role and set only by operators. Do we need to care about principals that set it? Not that much. ",5,2.9880648
MESOS-5156,Run mesos builds on PowerPC platform in ASF CI,This is the last step to declare official support for PowerPC.    This is currently blocked on ASF INFRA adding PowerPC based Jenkins machines to the ASF CI.  ,1,1.9470866
MESOS-5157,Update webui for GPU metrics,"After adding the GPU metrics and updating the resources JSON to include GPU information, the webui should be updated accordingly.",1,1.1447445
MESOS-5159,Add test to verify error when requesting fractional GPUs,Fractional GPU requests should immediately cause a TASK_FAILED without ever launching the task.,1,1.3698905
MESOS-5160,Make `network/cni` enabled as the default network isolator for `MesosContainerizer`.,"Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator. ",1,1.7005453
MESOS-5162,"Commit message hook behaves incorrectly when a message includes a ""*"".","If there is a ""\*"" in a commit message (there often is when we have bulleted lists), due to the current use of {{echo $LINE}}, the {{$LINE}} gets expanded with a ""*"" in it, which becomes a matcher in bash and therefore subsequently gets expanded into the list of files/directories in the current directory.    In order to avoid this mess, we need to wrap such variables in quotes, like so: {{echo ""$LINE""}}.",2,1.385162
MESOS-5164,Add authorization to agent's /monitor/statistics endpoint.,"Operators may want to enforce that only specific authorized users be able to view per-executor resource usage statistics. For 0.29 MVP, we can make this coarse-grained, and assume that only the operator or a operator-privileged monitoring service will be accessing the endpoint.  For a future release, we can consider fine-grained authz that filters statistics like we plan to do for /tasks.",5,2.5300045
MESOS-5167,Add tests for `network/cni` isolator,We need to add tests to verify the functionality of `network/cni` isolator.,5,2.6760406
MESOS-5168,Benchmark overhead of authorization based filtering.,When adding authorization based filtering as outlined in MESOS-4931 we need to be careful especially for performance critical endpoints such as /state.    We should ensure via a benchmark that performance does not degreade below an acceptable state.,3,2.483862
MESOS-5169,Introduce new Authorizer Actions for Authorized based filtering of endpoints.,For authorization based endpoint filtering we need to introduce the authorizer actions outlined via MESOS-4932.,3,2.4555726
MESOS-5170,Adapt json creation for authorization based endpoint filtering.,For authorization based endpoint filtering we need to adapt the json endpoint creation as discussed in MESOS-4931.,5,3.0947745
MESOS-5171,Expose state/state.hpp to public headers,"We want the Modules to be able to use replicated log along with the APIs to communicate with Zookeeper. This change would require us to expose at least the following headers state/storage.hpp, and any additional files that state.hpp depends on (e.g., zookeeper/authentication.hpp).",3,2.3064783
MESOS-5172,Registry puller cannot fetch blobs correctly from some private repos.,"When the registry puller is pulling a private repository from some private registry (e.g., quay.io), errors may occur when fetching blobs, at which point fetching the manifest of the repo is finished correctly. The error message is `Unexpected HTTP response '400 Bad Request' when trying to download the blob`. This may arise from the logic of fetching blobs, or incorrect format of uri when requesting blobs.",3,2.651451
MESOS-5173,Allow master/agent to take multiple modules manifest files,"When loading multiple modules into master/agent, one has to merge all module metadata (library name, module name, parameters, etc.) into a single json file which is then passed on to the --modules flag. This quickly becomes cumbersome especially if the modules are coming from different vendors/developers.    An alternate would be to allow multiple invocations of --modules flag that can then be passed on to the module manager. That way, each flag corresponds to just one module library and modules from that library.    Another approach is to create a new flag (e.g., --modules-dir) that contains a path to a directory that would contain multiple json files. One can think of it as an analogous to systemd units. The operator that drops a new file into this directory and the file would automatically be picked up by the master/agent module manager. Further, the naming scheme can also be inherited to prefix the filename with an ""NN_"" to signify oad order.",3,1.7926687
MESOS-5174,Update the balloon-framework to run on test clusters,"There are a couple of problems with the balloon framework that prevent it from being deployed (easily) on an actual cluster:    * The framework accepts 100% of memory in an offer.  This means the expected behavior (finish or OOM) is dependent on the offer size.  * The framework assumes the {{balloon-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments.  * The framework does not specify CPUs with the executor.  This is required by many isolators.  * The executor's {{TASK_FINISHED}} logic path was untested and is flaky.  * The framework has no metrics.  * The framework only launches a single task and then exits.  With this behavior, we can't have useful metrics.  ",3,1.9018083
MESOS-5178,Add logic to validate for non-fractional GPU requests in the master,"We should not put this logic directly into the  'Resources::validate()' function.  The primary reason is that the existing 'Resources::validate()' function doesn't consider the semantics of any particular resource when performing its validation (it only makes sure that the fields in the 'Resource' protobuf message are correctly formed). Since a fractional 'gpus' resources is actually well-formed (and only semantically incorrect), we should push this validation logic up into the master.        Moreover, the existing logic to construct a 'Resources' object from a 'RepeatedPtrField<Resource>' silently drops any resources that don't pass 'Resources::validate()'. This means that if we were to push the non-fractional 'gpus' validation into 'Resources::validate()', the 'gpus' resources would just be silently dropped rather than causing a TASK_ERROR in the master. This is obviously *not* the desired behaviour.",2,3.7194371
MESOS-5179,Enhance the error message for Duration flag.,Enhance the error message for  https://github.com/apache/mesos/blob/4dfa91fc21f80204f5125b2e2f35c489f8fb41d8/3rdparty/libprocess/3rdparty/stout/include/stout/duration.hpp#L70 to list all of the supported duration unit.,1,2.436759
MESOS-5180,Scheduler driver does not detect disconnection with master and reregister.,"The existing implementation of the scheduler driver does not re-register with the master under some network partition cases.    When a scheduler registers with the master:  1) master links to the framework  2) framework links to the master    It is possible for either of these links to break *without* the master changing.  (Currently, the scheduler driver will only re-register if the master changes).    If both links break or if just link (1) breaks, the master views the framework as {{inactive}} and {{disconnected}}.  This means the framework will not receive any more events (such as offers) from the master until it re-registers.  There is currently no way for the scheduler to detect a one-way link breakage.    if link (2) breaks, it makes (almost) no difference to the scheduler.  The scheduler usually uses the link to send messages to the master, but libprocess will create another socket if the persistent one is not available.    To fix link breakages for (1+2) and (2), the scheduler driver should implement a `::exited` event handler for the master's {{pid}} and trigger a master (re-)detection upon a disconnection. This in turn should make the driver (re)-register with the master. The scheduler library already does this: https://github.com/apache/mesos/blob/master/src/scheduler/scheduler.cpp#L395    See the related issue MESOS-5181 for link (1) breakage.",3,3.0785027
MESOS-5181,Master should reject calls from the scheduler driver if the scheduler is not connected.,"When a scheduler registers, the master will create a link from master to scheduler.  If this link breaks, the master will consider the scheduler {{inactive}} and mark it as {{disconnected}}.    This causes a couple problems:  1) Master does not send offers to {{inactive}} schedulers.  But these schedulers might consider themselves ""registered"" in a one-way network partition scenario.  2) Any calls from the {{inactive}} scheduler is still accepted, which leaves the scheduler in a starved, but semi-functional state.    See the related issue for more context: MESOS-5180    There should be an additional guard for registered, but {{inactive}} schedulers here:  https://github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/master.cpp#L1977    The HTTP API already does this:  https://github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/http.cpp#L459    Since the scheduler driver cannot return a 403, it may be necessary to return a {{Event::ERROR}} and force the scheduler to abort.",1,2.8683403
MESOS-5199,The mesos-execute prints confusing message when launching tasks.,"{code}  root@mesos002:~/src/mesos/m2/mesos/build# src/mesos-execute --master=192.168.56.12:5050 --name=test --docker_image=ubuntu:14.04 --command=""ls /root""  I0413 07:28:03.833521  2295 scheduler.cpp:175] Version: 0.29.0  Subscribed with ID '3a1af11e-cf66-4ce2-826d-48b332977999-0001'  Submitted task 'test' to agent '3a1af11e-cf66-4ce2-826d-48b332977999-S0'  Received status update TASK_RUNNING for task 'test'    source: SOURCE_EXECUTOR    reason: REASON_COMMAND_EXECUTOR_FAILED <<<   Received status update TASK_FINISHED for task 'test'    message: 'Command exited with status 0'    source: SOURCE_EXECUTOR    reason: REASON_COMMAND_EXECUTOR_FAILED <<<  root@mesos002:~/src/mesos/m2/mesos/build#  {code}",1,2.0181005
MESOS-5212,Allow any principal in ReservationInfo when HTTP authentication is off,"Mesos currently provides no way for operators to pass their principal to HTTP endpoints when HTTP authentication is off. Since we enforce that {{ReservationInfo.principal}} be equal to the operator principal in requests to {{/reserve}}, this means that when HTTP authentication is disabled, the {{ReservationInfo.principal}} field cannot be set.    To address this in the short-term, we should allow {{ReservationInfo.principal}} to hold any value when HTTP authentication is disabled.",1,2.6003857
MESOS-5213,Operator endpoints should accept a principal without HTTP authentication,"Mesos currently provides no way for operators to include their principal with HTTP endpoint requests when HTTP authentication is disabled. To remedy this, we should add optional {{principal}} fields to the relevant protobuf messages. When HTTP authentication is enabled, we can allow the user to leave this field empty and populate it with the principal from their HTTP Auth header.",3,3.0793376
MESOS-5214,Populate FrameworkInfo.principal for authenticated frameworks,"If a framework authenticates and then does not provide a {{principal}} in its {{FrameworkInfo}}, we currently allow this and leave {{FrameworkInfo.principal}} unset. Instead, we should populate {{FrameworkInfo.principal}} for them automatically in that case to ensure that the two principals are equal.",2,1.6248955
MESOS-5215,Update the documentation for '/reserve' and '/create-volumes',There are a couple issues related to the {{principal}} field in {{DiskInfo}} and {{ReservationInfo}} (see linked JIRAs) that should be better documented. We need to help users understand the purpose of these fields and how they interact with the principal provided in the HTTP authentication header. See linked tickets for background.,1,1.3208566
MESOS-5216,Document docker volume driver isolator.,"Should include the followings:    1. What features (driver options) are supported in docker volume driver isolator.  2. How to use docker volume driver isolator.      *related agent flags introduction and usage.      *isolator dependency clarification (e.g., filesystem/linux).      *related driver daemon preprocess.      *volumes pre-specified by users and volume cleanup.",5,2.600832
MESOS-5221,Add Documentation for Nvidia GPU support,https://reviews.apache.org/r/46220/,5,3.232564
MESOS-5222,Create a benchmark for scale testing HTTP frameworks,It would be good to add a benchmark for scale testing the HTTP frameworks wrt driver based frameworks. The benchmark can be as simple as trying to launch N tasks (parameterized) with the old/new API. We can then focus on fixing performance issues that we find as a result of this exercise.,3,2.8117445
MESOS-5227,Implement HTTP Docker Executor that uses the Executor Library,Similar to what we did with the HTTP command executor in MESOS-3558 we should have a HTTP docker executor that can speak the v1 Executor API.,5,4.8000937
MESOS-5228,Add tests for Capability API.,Add basic tests for the capability API.,3,3.0315046
MESOS-5232,Add capability information to ContainerInfo protobuf message.,"To enable support for capability as first class framework entity, we need to add capabilities related information to the ContainerInfo protobuf.",1,2.5075874
MESOS-5237,The windows version of `os::access` has differing behavior than the POSIX version.,"The POSIX version of {{os::access}} looks like this:    {code}  inline Try<bool> access(const std::string& path, int how)  {    if (::access(path.c_str(), how) < 0) {      if (errno == EACCES) {        return false;      } else {        return ErrnoError();      }    }    return true;  }  {code}    Compare this to the Windows version of {{os::access}} which looks like this following:    {code}  inline Try<bool> access(const std::string& fileName, int how)  {    if (::_access(fileName.c_str(), how) != 0) {      return ErrnoError(""access: Could not access path '"" + fileName + ""'"");    }      return true;  }  {code}    As we can see, the case where {{errno}} is set to {{EACCES}} is handled differently between the 2 functions.    We can actually consolidate the 2 functions by simply using the POSIX version. The challenge is that on POSIX, we should use {{::access}} and {{::_access}} on Windows. Note however, that this problem is already solved, as we have an implementation of {{::access}} for Windows in {{3rdparty/libprocess/3rdparty/stout/include/stout/windows.hpp}} which simply defers to {{::_access}}.    Thus, I propose to simply consolidate the 2 implementations.",2,1.595371
MESOS-5238,CHECK failure in AppcProvisionerIntegrationTest.ROOT_SimpleLinuxImageTest,Observed on the Mesosphere internal CI:    {noformat}  [22:56:28]W:     [Step 10/10] F0420 22:56:28.056788   629 containerizer.cpp:1634] Check failed: containers_.contains(containerId)  {noformat}    Complete test log will be attached as a file.,2,1.4893886
MESOS-5239,Persistent volume DockerContainerizer support assumes proper mount propagation setup on the host.,"We recently added persistent volume support in DockerContainerizer (MESOS-3413). To understand the problem, we first need to understand how persistent volumes are supported in DockerContainerizer.    To support persistent volumes in DockerContainerizer, we bind mount persistent volumes under a container's sandbox ('container_path' has to be relative for persistent volumes). When the Docker container is launched, since we always add a volume (-v) for the sandbox, the persistent volumes will be bind mounted into the container as well (since Docker does a 'rbind').    The assumption that the above works is that the Docker daemon should see those persistent volume mounts that Mesos mounts on the host mount table. It's not a problem if Docker daemon itself is using the host mount namespace. However, on systemd enabled systems, Docker daemon is running in a separate mount namespace and all mounts in that mount namespace will be marked as slave mounts due to this [patch|https://github.com/docker/docker/commit/eb76cb2301fc883941bc4ca2d9ebc3a486ab8e0a].    So what that means is that: in order for it to work, the parent mount of agent's work_dir should be a shared mount when docker daemon starts. This is typically true on CentOS7, CoreOS as all mounts are shared mounts by default.    However, this causes an issue with the 'filesystem/linux' isolator. To understand why, first I need to show you a typical problem when dealing with shared mounts. Let me explain that using the following commands on a CentOS7 machine:  {noformat}  [root@core-dev run]# cat /proc/self/mountinfo  24 60 0:19 / /run rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755  [root@core-dev run]# mkdir /run/netns  [root@core-dev run]# mount --bind /run/netns /run/netns  [root@core-dev run]# cat /proc/self/mountinfo  24 60 0:19 / /run rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755  121 24 0:19 /netns /run/netns rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755  [root@core-dev run]# ip netns add test  [root@core-dev run]# cat /proc/self/mountinfo  24 60 0:19 / /run rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755  121 24 0:19 /netns /run/netns rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755  162 121 0:3 / /run/netns/test rw,nosuid,nodev,noexec,relatime shared:5 - proc proc rw  163 24 0:3 / /run/netns/test rw,nosuid,nodev,noexec,relatime shared:5 - proc proc rw  {noformat}    As you can see above, there're two entries (/run/netns/test) in the mount table (unexpected). This will confuse some systems sometimes. The reason is because when we create a self bind mount (/run/netns -> /run/netns), the mount will be put into the same shared mount peer group (shared:22) as its parent (/run). Then, when you create another mount underneath that (/run/netns/test), that mount operation will be propagated to all mounts in the same peer group (shared:22), resulting an unexpected additional mount being created.    The reason we need to do a self bind mount in Mesos is that sometimes, we need to make sure some mounts are shared so that it does not get copied when a new mount namespace is created. However, on some systems, mounts are private by default (e.g., Ubuntu 14.04). In those cases, since we cannot change the system mounts, we have to do a self bind mount so that we can set mount propagation to shared. For instance, in filesytem/linux isolator, we do a self bind mount on agent's work_dir.    To avoid the self bind mount pitfall mentioned above, in filesystem/linux isolator, after we created the mount, we do a make-slave + make-shared so that the mount is its own shared mount peer group. In that way, any mounts underneath it will not be propagated back.    However, that operation will break the assumption that the persistent volume DockerContainerizer support makes. As a result, we're seeing problem with persistent volumes in DockerContainerizer when filesystem/linux isolator is turned on.",3,2.4845793
MESOS-5240,Command executor may escalate after the task is reaped.,"In command executor, {{escalated()}} may be scheduled before the task has been killed, i.e. {{reaped()}}, but called after. In this case {{escalated()}} should be a no-op.",1,2.4233778
MESOS-5243,Remove '/system/stats.json' endpoint,The {{/system/stats.json}} endpoint was deprecated by MESOS-2058. This endpoint can now be removed.,1,2.108889
MESOS-5249,Update CMake files to reflect reorganized 3rdparty,,2,1.91487
MESOS-5250,Move 3rdparty/libprocess/3rdparty/* to 3rdparty/,,5,2.7461681
MESOS-5253,Isolator cleanup should not be invoked if they are not prepared yet.,"If the mesos containerizer destroys a container in PROVISIONING state, isolator cleanup is still called, which is incorrect because there is no isolator prepared yet.     In this case, there no need to clean up any isolator, call provisioner destroy directly.",2,2.5131555
MESOS-5254,Add URI parsing function/library,"The {{uri::Fetcher}} theoretically supports all URIs, per [RFC3986|http://tools.ietf.org/html/rfc3986].  To do this, we need a spec-compliant parser from string to URI.    [uriparser|http://uriparser.sourceforge.net/] appears to fit the bill.",2,2.8660114
MESOS-5255,Add GPUs to container resource consumption metrics.,"Currently the usage callback in the Nvidia GPU isolator is unimplemented:    {noformat}  src/slave/containerizer/mesos/isolators/cgroups/devices/gpus/nvidia.cpp  {noformat}    It should use functionality from NVML to gather the current GPU usage and add it to a ResourceStatistics object. It is still an open question as to exactly what information we want to expose here (power, memory consumption, current load, etc.). Whatever we decide on should be standard across different GPU types, different GPU vendors, etc.",3,1.9176664
MESOS-5256,Add support for per-containerizer resource enumeration,"Currently the top level containerizer includes a static function for enumerating the resources available on a given agent. Ideally, this functionality should be the responsibility of individual containerizers (and specifically the responsibility of each isolator used to control access to those resources).    Adding support for this will involve making the `Containerizer::resources()` function virtual instead of static and then implementing it on a per-containerizer basis.  We should consider providing a default to make this easier in cases where there is only really one good way of enumerating a given set of resources.",3,2.3961577
MESOS-5257,Add autodiscovery for GPU resources,"Right now, the only way to enumerate the available GPUs on an agent is to use the `--nvidia_gpu_devices` flag and explicitly list them out.  Instead, we should leverage NVML to autodiscover the GPUs that are available and only use this flag as a way to explicitly list out the GPUs you want to make available in order to restrict access to some of them.",3,2.83287
MESOS-5258,Turn the Nvidia GPU isolator into a module,"The Nvidia GPU isolator has an external dependence on `libnvidia-ml.so`. As it currently stands, this forces *all* binaries that link with `libmesos.so` to also link with `libnvidia-ml.so` (including master, agents on machines without GPUs, scheduler, exectors, etc.).    By turning the Nvidia GPU isolator into a module, it will be loaded at runtime only when an agent has explicitly including the the Nvidia GPU isolator in its `--isolation` flag.",5,3.64953
MESOS-5259,Refactor the mesos-fetcher binary to use the uri::Fetcher as a backend,This is an intermediate step for combining the {{mesos-fetcher}} binary and {{uri::Fetcher}}.      The {{download}} method should be replaced with {{uri::Fetcher::fetch}}.  https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/launcher/fetcher.cpp#L179    Combining the two will:  * Attach the {{uri::Fetcher}} to the existing Fetcher caching logic.  * Remove some code duplication for downloading URIs.,3,3.3096273
MESOS-5260,"Extend the uri::Fetcher::Plugin interface to include a ""fetchSize""","In order to replace the {{mesos-fetcher}} binary with the {{uri::Fetcher}}, each plugin must be able to determine/estimate the size of a download.  This is used by the Fetcher cache when it creates cache entries and such.    The logic for each of the four {{Fetcher::Plugin}}s can be taken and refactored from the existing fetcher.  https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/slave/containerizer/fetcher.cpp#L267",2,1.6550709
MESOS-5261,Combine the internal::slave::Fetcher class and mesos-fetcher binary,"After [MESOS-5259], the {{mesos-fetcher}} will no longer need to be a separate binary and can be safely folded back into the agent process.  (It was a separate binary because libcurl has synchronous/blocking calls.)      This will likely mean:  * A change to the {{fetch}} continuation chain:    https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/slave/containerizer/fetcher.cpp#L315  * This protobuf can be deprecated (or just removed):    https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/include/mesos/fetcher/fetcher.proto",3,2.390487
MESOS-5263,pivot_root is not available on ARM,"When compile on ARM, it will through error.  The current code logic in src/linux/fs.cpp is:    {code}  #ifdef __NR_pivot_root    int ret = ::syscall(__NR_pivot_root, newRoot.c_str(), putOld.c_str());  #elif __x86_64__    // A workaround for systems that have an old glib but have a new    // kernel. The magic number '155' is the syscall number for    // 'pivot_root' on the x86_64 architecture, see    // arch/x86/syscalls/syscall_64.tbl    int ret = ::syscall(155, newRoot.c_str(), putOld.c_str());  #elif __powerpc__ || __ppc__ || __powerpc64__ || __ppc64__    // A workaround for powerpc. The magic number '203' is the syscall    // number for 'pivot_root' on the powerpc architecture, see    // https://w3challs.com/syscalls/?arch=powerpc_64    int ret = ::syscall(203, newRoot.c_str(), putOld.c_str());  #else  #error ""pivot_root is not available""  #endif  {code}    Possible sollution is to add `unistd.h` header",1,3.2918444
MESOS-5265,Update mesos-execute to support docker volume isolator.,The mesos-execute needs to be updated to support docker volume isolator.,3,2.6676688
MESOS-5266,add test cases for docker volume driver,,5,2.717743
MESOS-5271,Add alias support for Flags,"Currently there is no support for a flag to have an alias. Such support would be useful to rename/deprecate a flag.    For example, for MESOS-4386, we could let the flag have `--authenticate` name and a `--authenticate_frameworks` alias. The alias can be marked as deprecated (need to add support for this as well).    This support will also be useful for slave/agent flag rename. See MESOS-3781 for details.  ",5,3.4780684
MESOS-5272,Support docker image labels.,"Docker image labels should be supported in unified containerizer, which can be used for applying custom metadata. Image labels are necessary for mesos features to support docker in unified containerizer (e.g., for mesos GPU device isolator).",3,3.6009781
MESOS-5273,Need support for Authorization information via HELP.,We should add information about authentication to the help message and thereby endpoint documentation (similarly as MESOS-4934 has done for authentication).,3,3.0222268
MESOS-5275,Add capabilities support for unified containerizer.,Add capabilities support for unified containerizer.     Requirements:  1. Use the mesos capabilities API.  2. Frameworks be able to add capability requests for containers.  3. Agents be able to add maximum allowed capabilities for all containers launched.    Design document: https://docs.google.com/document/d/1YiTift8TQla2vq3upQr7K-riQ_pQ-FKOCOsysQJROGc/edit#heading=h.rgfwelqrskmd  ,5,2.6288328
MESOS-5277,Need to add REMOVE semantics to the copy backend,"Some Dockerfiles run the `rm` command to remove files from the base image using the ""RUN"" directive in the Dockerfile. An example can be found here:  https://github.com/ngineered/nginx-php-fpm.git    In the final rootfs the removed files should not be present. Presence of these files in the final image can make the container misbehave. For example, the nginx-php-fpm docker image that is referenced tries to remove the default nginx config and replaces it with its own config to point to a different HTML root. If the default nginx config is still present after the building the image, nginx will start pointing to a different HTML root than the one set in the Dockerfile.      Currently the copy backend cannot handle removal of files from intermediate layers. This can cause issues with docker images built using a Dockerfile similar to the one listed here. Hence, we need to add REMOVE semantics to the copy backend.  ",5,2.6539426
MESOS-5286,Add authorization to libprocess HTTP endpoints,"Now that the libprocess-level HTTP endpoints have had authentication added to them in MESOS-4902, we can add authorization to them as well. As a first step, we can implement a ""coarse-grained"" approach, in which a principal is granted or denied access to a given endpoint. We will likely need to register an authorizer with libprocess.",5,2.3317575
MESOS-5294,Status updates after a health check are incomplete or invalid,"With command health checks enabled via marathon, mesos-dns will resolve the task correctly until the task is reported as ""healthy"". At that point, mesos-dns stops resolving the task correctly.    -Digging through src/docker/executor.cpp, I found that in the {{taskHealthUpdated()}} function is attempting to copy the taskID to the new status instance with-    {code}status.mutable_task_id()->CopyFrom(taskID);{code}    -but other instances of status updates have a similar line-    {code}status.mutable_task_id()->CopyFrom(taskID.get());{code}    -My assumption is that this difference is causing the status update after a health check to not have a proper taskID, which in turn is causing an incorrect state.json output.-    -I'll try to get a patch together soon.-    UPDATE:  None of the above assumption are correct. Something else is causing the issue.",1,2.1929784
MESOS-5296,Split Resource and Inverse offer protobufs for V1 API,"The protobufs for the V1 api regarding inverse offers initially re-used the existing offer / rescind / accept / decline messages for regular offers.  We should split these out the be more explicit, and provide the ability to augment the messages with particulars to either resource or inverse offers.",5,3.4505153
MESOS-5297,"Add authorization to the master's ""/flags"" endpoint.","Coarse HTTP endpoint authorization using the {{GET_ENDPOINT_WITH_PATH}} ACL rule needs to be added to the ""/flags"" endpoint of the master.",3,2.8031645
MESOS-5301,Add synchronous validation for all types of Calls.,"Currently, we do a best effort validation for all calls sent to the master from the scheduler by invoking {{validation::scheduler::call::validate(call, principal)}}. This is a generic validation helper for all calls. However, for more fine grained validation for a particular call, we invoke the validation as part of the call handle itself.    {code}  Option<Error> validationError = roles::validate(frameworkInfo.role());  {code}    This in turn makes all validations asynchronous i.e. the framework gets them as {{Event::ERROR}} events later. It would be good if such validations can be handled while processing the {{Call}} message itself synchronously.",5,2.2214355
MESOS-5302,Consider adding an Executor Shim/Adapter for the new/old API,"Currently, all the business logic for HTTP based command executor/driver based command executor lives in 2 different files. As more features are added/bugs are discovered in the executor itself, they need to be fixed in two places. It would be nice to have some kind of a shim/adapter that abstracts away the underlying library details from the executor. Hence, the executor can toggle between whether it wants to use the driver or the new API via an environment variable.",5,3.3560898
MESOS-5303,Add capabilities support for mesos execute cli.,Add support for `user` and `capabilities` to execute cli. This will help in testing the `capabilities` feature for unified containerizer.,3,2.8443077
MESOS-5304,/metrics/snapshot endpoint help disappeared on agent.,After https://github.com/apache/mesos/commit/066fc4bd0df6690a5e1a929d3836e307c1e22586  the help for the /metrics/snapshot endpoint on the agent doesn't appear anymore (Master endpoint help is unchanged).,1,2.2090786
MESOS-5307,Sandbox mounts should not be in the host mount namespace.,"Currently, if a container uses container image, we'll do a bind mount of its sandbox (<sandbox> -> <rootfs>/mnt/mesos/sandbox) in the host mount namespace.    However, doing the mounts in the host mount table is not ideal. That complicates both the cleanup path and the recovery path.    Instead, we can do the sandbox bind mount in the container's mount namespace so that cleanup and recovery will be greatly simplified. We can setup mount propagation properly so that persistent volumes mounted at <sandbox>/xxx can be propagated into the container.    Here is a simple proof of concept:    Console 1:  {noformat}  vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ ll .  total 12  drwxrwxr-x 3 vagrant vagrant 4096 Apr 25 16:05 ./  drwxrwxr-x 6 vagrant vagrant 4096 Apr 25 23:17 ../  drwxrwxr-x 5 vagrant vagrant 4096 Apr 25 23:17 slave/  vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ ll slave/  total 20  drwxrwxr-x  5 vagrant vagrant 4096 Apr 25 23:17 ./  drwxrwxr-x  3 vagrant vagrant 4096 Apr 25 16:05 ../  drwxrwxr-x  6 vagrant vagrant 4096 Apr 26 21:06 directory/  drwxr-xr-x 12 vagrant vagrant 4096 Apr 25 23:20 rootfs/  drwxrwxr-x  2 vagrant vagrant 4096 Apr 25 16:09 volume/  vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ sudo mount --bind slave/ slave/                                                                                                                                                                                                                              vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ sudo mount --make-shared slave/  vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ cat /proc/self/mountinfo   50 22 8:1 /home/vagrant/tmp/mesos/slave /home/vagrant/tmp/mesos/slave rw,relatime shared:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered  {noformat}    Console 2:  {noformat}  vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ cd slave/  vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave$ sudo unshare -m /bin/bash  root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# sudo mount --make-rslave .  root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# cat /proc/self/mountinfo  124 63 8:1 /home/vagrant/tmp/mesos/slave /home/vagrant/tmp/mesos/slave rw,relatime master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered  root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# mount --rbind directory/ rootfs/mnt/mesos/sandbox/                                                                                                                                                                                          root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# mount --rbind rootfs/ rootfs/  root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# mount -t proc proc rootfs/proc                                                                                                                                                                                                              root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# pivot_root rootfs rootfs/tmp/.rootfs                                                                                                                                                                                                        root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# cd /  root@vagrant-ubuntu-trusty-64:/# cat /proc/self/mountinfo  126 61 8:1 /home/vagrant/tmp/mesos/slave/rootfs / rw,relatime master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered  127 126 8:1 /home/vagrant/tmp/mesos/slave/directory /mnt/mesos/sandbox rw,relatime master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered  128 126 0:3 / /proc rw,relatime - proc proc rw  {noformat}    Console 1:  {noformat}  agrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ cd slave/  vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave$ sudo mount --bind volume/ directory/v1  vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave$ cat /proc/self/mountinfo  50 22 8:1 /home/vagrant/tmp/mesos/slave /home/vagrant/tmp/mesos/slave rw,relatime shared:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered  129 50 8:1 /home/vagrant/tmp/mesos/slave/volume /home/vagrant/tmp/mesos/slave/directory/v1 rw,relatime shared:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered  {noformat}    Console 2:  {noformat}  root@vagrant-ubuntu-trusty-64:/# cat /proc/self/mountinfo  126 61 8:1 /home/vagrant/tmp/mesos/slave/rootfs / rw,relatime master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered  127 126 8:1 /home/vagrant/tmp/mesos/slave/directory /mnt/mesos/sandbox rw,relatime master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered  128 126 0:3 / /proc rw,relatime - proc proc rw  132 127 8:1 /home/vagrant/tmp/mesos/slave/volume /mnt/mesos/sandbox/v1 rw,relatime shared:4 master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered  {noformat}",5,2.8773952
MESOS-5310,Enable `network/cni` isolator to allow modifications and deletion of CNI config,"Currently the `network/cni` isolator can only load the CNI configs at startup. This makes the CNI networks immutable. From an operational standpoint this can make deployments painful for operators.     To make CNI more flexible the `network/cni` isolator should be able to load configs at run time.     The proposal is to add an endpoint to the `network/cni` isolator, to which when the operator sends a PUT request the `network/cni` isolator will reload  CNI configs. ",5,2.2801187
MESOS-5312,Env `MESOS_SANDBOX` is not set properly for command tasks that changes rootfs.,"This is in the context of Mesos containerizer (a.k.a., unified containerizer).    I did a simple test:  {noformat}  sudo sbin/mesos-master --work_dir=/tmp/mesos/master  sudo GLOG_v=1 sbin/mesos-slave --master=10.0.2.15:5050 --isolation=docker/runtime,filesystem/linux --work_dir=/tmp/mesos/slave/ --image_providers=docker --executor_environment_variables=""{}""  sudo bin/mesos-execute --master=10.0.2.15:5050 --name=test --docker_image=alpine --command=""env""     MESOS_EXECUTOR_ID=test  SHLVL=1  MESOS_CHECKPOINT=0  MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD=5secs  LIBPROCESS_PORT=0  MESOS_AGENT_ENDPOINT=10.0.2.15:5051  MESOS_SANDBOX=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6  MESOS_NATIVE_JAVA_LIBRARY=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so  MESOS_FRAMEWORK_ID=1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000  MESOS_SLAVE_ID=2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0  MESOS_NATIVE_LIBRARY=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so  MESOS_DIRECTORY=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6  PWD=/mnt/mesos/sandbox  MESOS_SLAVE_PID=slave(1)@10.0.2.15:5051  {noformat}    `MESOS_SANDBOX` above should be `/mnt/mesos/sandbox`.",2,1.8473294
MESOS-5313,Failed to set quota and update weight according to document,"{code}  root@mesos002:~/test# curl -d jsonMessageBody -X POST http://192.168.56.12:5050/quota  Failed to parse set quota request JSON 'jsonMessageBody': syntax error at line 1 near: jsonMessageBodyroot@mesos002:~/test# cat jsonMessageBody  {  	""role"": ""role1"",  	""guarantee"": [{  		""name"": ""cpus"",  		""type"": ""SCALAR"",  		""scalar"": {  			""value"": 1  		}  	}, {  		""name"": ""mem"",  		""type"": ""SCALAR"",  		""scalar"": {  			""value"": 128  		}  	}]  }  root@mesos002:~/test# curl -d weight.json -X PUT http://192.168.56.12:5050/weights  Failed to parse update weights request JSON ('weight.json'): syntax error at line 1 near: weight.js  root@mesos002:~/test# cat weight.json      [        {          ""role"": ""role1"",          ""weight"": 2.0        },        {          ""role"": ""role2"",          ""weight"": 3.5        }      ]  {code}    The right command should be adding {{@}} before the quota json file {{jsonMessageBody}}.",1,2.2732944
MESOS-5316,Authenticate the agent's '/containers' endpoint.,The {{/containers}} endpoint was recently added to the agent. Authentication should be enabled on this endpoint.,2,2.7330618
MESOS-5317,Authorize the agent's '/containers' endpoint.,"After the agent's {{/containers}} endpoint is authenticated, we should enabled authorization as well.",2,2.9847865
MESOS-5318,Make `os::close` always catch structured exceptions on Windows,,2,2.500051
MESOS-5335,Add authorization to GET /weights.,"We already authorize which http users can update weights for particular roles, but even knowing of the existence of these roles (let alone their weights) may be sensitive information. We should add authz around GET operations on /weights.    Easy option: GET_ENDPOINT_WITH_PATH /weights  - Pro: No new verb  - Con: All or nothing    Complex option: GET_WEIGHTS_WITH_ROLE  - Pro: Filters contents based on roles the user is authorized to see  - Con: More authorize calls (one per role in each /weights request)",3,2.7715952
MESOS-5336,Add authorization to GET /quota.,"We already authorize which http users can set/remove quota for particular roles, but even knowing of the existence of these roles (let alone their quotas) may be sensitive information. We should add authz around GET operations on /quota.",3,2.9476693
MESOS-5337,Add Master Flag to enable fine-grained filtering of HTTP endpoints.,"As the fine-grained filtering of endpoints can the rather expensive, we should create a master flag to enable/disable this feature.",1,3.049812
MESOS-5338,Add `user` to `Task` protobuf message.,The LocalAuthorizer is supposed to use the OS `user` under which tasks are running for authorization.  As the master keeps track of running and completed processes we need access to this information in Task in order to authorize such tasks.,1,1.6617129
MESOS-5339,Create Tests for testing fine-grained HTTP endpoint filtering.,,3,3.5547242
MESOS-5343,Behavior of custom HTTP authenticators with disabled HTTP authentication is inconsistent between master and agent,"When setting a custom authenticator with {{http_authenticators}} and also specifying {{authenticate_http=false}} currently agents refuse to start with  {code}  A custom HTTP authenticator was specified with the '--http_authenticators' flag, but HTTP authentication was not enabled via '--authenticate_http'  {code}    Masters on the other hand accept this setting.    Having differing behavior between master and agents is confusing, and we should decide on whether we want to accept these settings or not, and make the implementations consistent.  ",3,2.892423
MESOS-5345,Design doc for TASK_LOST_PENDING,"The TASK_LOST task status describes two different situations: (a) the task was not launched because of an error (e.g., insufficient available resources), or (b) the master lost contact with a running task (e.g., due to a network partition); the master will kill the task when it can (e.g., when the network partition heals), but in the meantime the task may still be running.    This has two problems:  1. Using the same task status for two fairly different situations is confusing.  2. In the partitioned-but-still-running case, frameworks have no easy way to determine when a task has truly terminated.    To address these problems, we propose introducing a new task status, TASK_LOST_PENDING. If a framework opts into this behavior using a new capability, TASK_LOST would mean ""the task is definitely not running"", whereas TASK_LOST_PENDING would mean ""the task may or may not be running (we've lost contact with the agent), but the master will try to shut it down when possible.""",5,1.8555332
MESOS-5347,Enhance the log message when launching mesos containerizer.,"Log the launch flag which includes the executor command, pre-launch commands and other information when launching the mesos containerizer. ",2,2.7824879
MESOS-5348,Enhance the log message when launching docker containerizer.,Log the launch flag which includes the executor command and other information when launching the docker containerizer.,2,3.0157037
MESOS-5350,Add asynchronous hook for validating docker containerizer tasks,"It is possible to plug in custom validation logic for the MesosContainerizer via an {{Isolator}} module, but the same is not true of the DockerContainerizer.    Basic logic can be plugged into the DockerContainerizer via {{Hooks}}, but this has some notable differences compared to isolators:  * Hooks are synchronous.  * Modifications to tasks via Hooks have lower priority compared to the task itself.  i.e. If both the {{TaskInfo}} and {{slaveExecutorEnvironmentDecorator}} define the same environment variable, the {{TaskInfo}} wins.  * Hooks have no effect if they fail (short of segfaulting)  i.e. The {{slavePreLaunchDockerHook}} has a return type of {{Try<Nothing>}}:  https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/include/mesos/hook.hpp#L90  But the effect of returning an {{Error}} is a log message:  https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/src/hook/manager.cpp#L227-L230    We should add a hook to the DockerContainerizer to narrow this gap.  This new hook would:  * Be called at roughly the same place as {{slavePreLaunchDockerHook}}  https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/src/slave/containerizer/docker.cpp#L1022  * Return a {{Future}} and require splitting up {{DockerContainerizer::launch}}.  * Prevent a task from launching if it returns a {{Failure}}.",5,2.9856725
MESOS-5353,Use `Connection` abstraction to compare stale connections in scheduler library.,"Previously, we had a bug in the {{Connection}} abstraction in libprocess that hindered the ability to pass it onto {{defer}} callbacks since it could sometimes lead to deadlock (MESOS-4658). Now that it is resolved, we might consider not using {{UUID}} objects for stale connection checks but directly using the {{Connection}} abstraction in the scheduler library.",3,2.4304752
MESOS-5356,Add Windows support for StopWatch,,2,2.0389605
MESOS-5359,The scheduler library should have a delay before initiating a connection with master.,"Currently, the scheduler library {{src/scheduler/scheduler.cpp}} does have an artificially induced delay when trying to initially establish a connection with the master. In the event of a master failover or ZK disconnect, a large number of frameworks can get disconnected and then thereby overwhelm the master with TCP SYN requests.     On a large cluster with many agents, the master is already overwhelmed with handling connection requests from the agents. This compounds the issue further on the master.",3,2.469843
MESOS-5360,Set death signal for dvdcli subprocess in docker volume isolator.,"If the slave crashes, we should kill the dvdcli subprocess. Otherwise, if the dvdcli subprocess gets stuck, it'll not be cleaned up.",2,2.7274795
MESOS-5362,Add authentication to example frameworks,Some example frameworks do not have the ability to authenticate with the master. Adding authentication to the example frameworks that don't already have it implemented would allow us to use these frameworks for testing in authenticated/authorized scenarios.,2,1.9285455
MESOS-5365,Introduce a timeout for docker volume driver mount/unmount operation.,'dvdcli' might hang indefinitely. We should introduce timeout for both mount/unmount operation so that launch/cleanup are not blocked forever.,2,2.6742399
MESOS-5370,Add deprecation support for Flags,MESOS-5271 adds support for a flag name to have an alias. This ticket captures the work need to add deprecation support. The idea is for the caller to explicitly specify deprecation via `FlagsBase::add()`  and get a list of deprecation warnings when doing `FlagsBase::load()`.,5,3.202479
MESOS-5372,Add random() to os:: namespace ,"The function ""random()"" is not available in Windows. After this improvement the calls to ""os::random()"" will result in calls to ""::random()"" on POSIX and ""::rand()"" on Windows.  ",1,2.4807553
MESOS-5373,Remove `Zookeeper's` NTDDI_VERSION define,"Zookeeper client library defines NTDDI_VERSION to 0x0400 in ""winconfig.h"". While this API level is suficient to compile the client library,  Mesos have to use a newer API set. After this improvement the code will compile with the latest NTDDI_VERSION.     ",2,1.126208
MESOS-5374,Add support for Console Ctrl handling in `slave.cpp`,Extract supporting code to handle POSIX signals in a separate header and add support for CTRL handler when running on Windows. ,3,0.77994835
MESOS-5375,Implement stout/os/windows/kill.hpp,Implement equivalent functionality on Windows ,5,2.8985882
MESOS-5378,Terminating a framework during master failover leads to orphaned tasks,"Repro steps:    1) Setup:  {code}  bin/mesos-master.sh --work_dir=/tmp/master  bin/mesos-slave.sh --work_dir=/tmp/slave --master=localhost:5050  src/mesos-execute --checkpoint --command=""sleep 1000"" --master=localhost:5050 --name=""test""  {code}    2) Kill all three from (1), in the order they were started.    3) Restart the master and agent.  Do not restart the framework.    Result)  * The agent will reconnect to an orphaned task.  * The Web UI will report no memory usage  * {{curl localhost:5050/metrics/snapshot}} will say:  {{""master/mem_used"": 128,}}    Cause)   When a framework registers with the master, it provides a {{failover_timeout}}, in case the framework disconnects.  If the framework disconnects and does not reconnect within this {{failover_timeout}}, the master will kill all tasks belonging to the framework.    However, the master does not persist this {{failover_timeout}} across master failover.  The master will ""forget"" about a framework if:  1) The master dies before {{failover_timeout}} passes.  2) The framework dies while the master is dead.    When the master comes back up, the agent will re-register.  The agent will report the orphaned task(s).  Because the master failed over, it does not know these tasks are orphans (i.e. it thinks the frameworks might re-register).    Proposed solution)  The master should save the {{FrameworkID}} and {{failover_timeout}} in the registry.  Upon recovery, the master should resume the {{failover_timeout}} timers.",3,2.9555128
MESOS-5380,Killing a queued task can cause the corresponding command executor to never terminate.,"We observed this in our testing environment. Sequence of events:    1) A command task is queued since the executor has not registered yet.  2) The framework issues a killTask.  3) Since executor is in REGISTERING state, agent calls `statusUpdate(TASK_KILLED, UPID())`  4) `statusUpdate` now will call `containerizer->status()` before calling `executor->terminateTask(status.task_id(), status);` which will remove the queued task. (Introduced in this patch: https://reviews.apache.org/r/43258).  5) Since the above is async, it's possible that the task is still in queued task when we trying to see if we need to kill unregistered executor in `killTask`:  {code}        // TODO(jieyu): Here, we kill the executor if it no longer has        // any task to run and has not yet registered. This is a        // workaround for those single task executors that do not have a        // proper self terminating logic when they haven't received the        // task within a timeout.        if (executor->queuedTasks.empty()) {          CHECK(executor->launchedTasks.empty())              << "" Unregistered executor '"" << executor->id              << ""' has launched tasks"";            LOG(WARNING) << ""Killing the unregistered executor "" << *executor                       << "" because it has no tasks"";            executor->state = Executor::TERMINATING;            containerizer->destroy(executor->containerId);        }      {code}    6) Consequently, the executor will never be terminated by Mesos.    Attaching the relevant agent log:  {noformat}  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.640527  1342 slave.cpp:1361] Got assigned task mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6 for framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.641034  1342 slave.cpp:1480] Launching task mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6 for framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.641440  1342 paths.cpp:528] Trying to chown '/var/lib/mesos/slave/slaves/a3ad8418-cb77-4705-b353-4b514ceca52c-S0/frameworks/a3ad8418-cb77-4705-b353-4b514ceca52c-0000/executors/mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6/runs/24762d43-2134-475e-b724-caa72110497a' to user 'root'  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.644664  1342 slave.cpp:5389] Launching executor mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6 of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/var/lib/mesos/slave/slaves/a3ad8418-cb77-4705-b353-4b514ceca52c-S0/frameworks/a3ad8418-cb77-4705-b353-4b514ceca52c-0000/executors/mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6/runs/24762d43-2134-475e-b724-caa72110497a'  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.645195  1342 slave.cpp:1698] Queuing task 'mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6' for executor 'mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6' of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.645491  1338 containerizer.cpp:671] Starting container '24762d43-2134-475e-b724-caa72110497a' for executor 'mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6' of framework 'a3ad8418-cb77-4705-b353-4b514ceca52c-0000'  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.647897  1345 cpushare.cpp:389] Updated 'cpu.shares' to 1126 (cpus 1.1) for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.648619  1345 cpushare.cpp:411] Updated 'cpu.cfs_period_us' to 100ms and 'cpu.cfs_quota_us' to 110ms (cpus 1.1) for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.650180  1341 mem.cpp:602] Started listening for OOM events for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.650718  1341 mem.cpp:722] Started listening on low memory pressure events for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.651147  1341 mem.cpp:722] Started listening on medium memory pressure events for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.651599  1341 mem.cpp:722] Started listening on critical memory pressure events for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.652015  1341 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 160MB for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:13 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:13.652719  1341 mem.cpp:388] Updated 'memory.limit_in_bytes' to 160MB for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.508930  1342 slave.cpp:1891] Asked to kill task mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6 of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.509063  1342 slave.cpp:3048] Handling status update TASK_KILLED (UUID: f9d15955-6c9a-4a73-98c3-97c0128510ba) for task mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6 of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000 from @0.0.0.0:0  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.509702  1340 disk.cpp:169] Updating the disk resources for container 24762d43-2134-475e-b724-caa72110497a to cpus(*):0.1; mem(*):32  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.510298  1343 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 32MB for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.510349  1341 cpushare.cpp:389] Updated 'cpu.shares' to 102 (cpus 0.1) for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.511102  1343 mem.cpp:388] Updated 'memory.limit_in_bytes' to 32MB for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.511495  1341 cpushare.cpp:411] Updated 'cpu.cfs_period_us' to 100ms and 'cpu.cfs_quota_us' to 10ms (cpus 0.1) for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.511715  1341 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: f9d15955-6c9a-4a73-98c3-97c0128510ba) for task mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6 of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.512032  1341 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_KILLED (UUID: f9d15955-6c9a-4a73-98c3-97c0128510ba) for task mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6 of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.513849  1343 slave.cpp:3446] Forwarding the update TASK_KILLED (UUID: f9d15955-6c9a-4a73-98c3-97c0128510ba) for task mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6 of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000 to master@10.0.5.79:5050  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.528929  1344 status_update_manager.cpp:392] Received status update acknowledgement (UUID: f9d15955-6c9a-4a73-98c3-97c0128510ba) for task mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6 of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 15:36:25 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:25.529002  1344 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_KILLED (UUID: f9d15955-6c9a-4a73-98c3-97c0128510ba) for task mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6 of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 15:36:28 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:28.199105  1345 isolator.cpp:469] Mounting docker volume mount point '//var/lib/rexray/volumes/jdef-test-125/data' to '/var/lib/mesos/slave/slaves/a3ad8418-cb77-4705-b353-4b514ceca52c-S0/frameworks/a3ad8418-cb77-4705-b353-4b514ceca52c-0000/executors/mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6/runs/24762d43-2134-475e-b724-caa72110497a/data' for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:28 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:28.207062  1338 containerizer.cpp:1184] Checkpointing executor's forked pid 5810 to '/var/lib/mesos/slave/meta/slaves/a3ad8418-cb77-4705-b353-4b514ceca52c-S0/frameworks/a3ad8418-cb77-4705-b353-4b514ceca52c-0000/executors/mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6/runs/24762d43-2134-475e-b724-caa72110497a/pids/forked.pid'  May 13 15:36:28 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:28.832330  1338 slave.cpp:2689] Got registration for executor 'mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6' of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000 from executor(1)@10.0.2.74:46154  May 13 15:36:28 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:28.833149  1345 disk.cpp:169] Updating the disk resources for container 24762d43-2134-475e-b724-caa72110497a to cpus(*):0.1; mem(*):32  May 13 15:36:28 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:28.833804  1342 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 32MB for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:28 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:28.833871  1340 cpushare.cpp:389] Updated 'cpu.shares' to 102 (cpus 0.1) for container 24762d43-2134-475e-b724-caa72110497a  May 13 15:36:28 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[1304]: I0513 15:36:28.835160  1340 cpushare.cpp:411] Updated 'cpu.cfs_period_us' to 100ms and 'cpu.cfs_quota_us' to 10ms (cpus 0.1) for container 24762d43-2134-475e-b724-caa72110497a  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: 5804 'mesos-logrotate-logger --help=false --log_filename=/var/lib/mesos/slave/slaves/a3ad8418-cb77-4705-b353-4b514ceca52c-S0/frameworks/a3ad8418-cb77-4705-b353-4b514ceca52c-0000/executors/mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6/runs/24762d43-2134-475e-b724-caa72110497a/stdout --logrotate_options=rotate 9 --logrotate_path=logrotate --max_size=2MB '  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: 5809 'mesos-logrotate-logger --help=false --log_filename=/var/lib/mesos/slave/slaves/a3ad8418-cb77-4705-b353-4b514ceca52c-S0/frameworks/a3ad8418-cb77-4705-b353-4b514ceca52c-0000/executors/mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6/runs/24762d43-2134-475e-b724-caa72110497a/stderr --logrotate_options=rotate 9 --logrotate_path=logrotate --max_size=2MB '  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: 5804 'mesos-logrotate-logger --help=false --log_filename=/var/lib/mesos/slave/slaves/a3ad8418-cb77-4705-b353-4b514ceca52c-S0/frameworks/a3ad8418-cb77-4705-b353-4b514ceca52c-0000/executors/mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6/runs/24762d43-2134-475e-b724-caa72110497a/stdout --logrotate_options=rotate 9 --logrotate_path=logrotate --max_size=2MB '  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: 5809 'mesos-logrotate-logger --help=false --log_filename=/var/lib/mesos/slave/slaves/a3ad8418-cb77-4705-b353-4b514ceca52c-S0/frameworks/a3ad8418-cb77-4705-b353-4b514ceca52c-0000/executors/mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6/runs/24762d43-2134-475e-b724-caa72110497a/stderr --logrotate_options=rotate 9 --logrotate_path=logrotate --max_size=2MB '  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.374567 30993 slave.cpp:5498] Recovering executor 'mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6' of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.420411 30990 status_update_manager.cpp:208] Recovering executor 'mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6' of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.513164 30994 containerizer.cpp:467] Recovering container '24762d43-2134-475e-b724-caa72110497a' for executor 'mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6' of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.533478 30988 mem.cpp:602] Started listening for OOM events for container 24762d43-2134-475e-b724-caa72110497a  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.534553 30988 mem.cpp:722] Started listening on low memory pressure events for container 24762d43-2134-475e-b724-caa72110497a  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.535269 30988 mem.cpp:722] Started listening on medium memory pressure events for container 24762d43-2134-475e-b724-caa72110497a  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.536198 30988 mem.cpp:722] Started listening on critical memory pressure events for container 24762d43-2134-475e-b724-caa72110497a  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.579385 30988 docker.cpp:859] Skipping recovery of executor 'mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6' of framework 'a3ad8418-cb77-4705-b353-4b514ceca52c-0000' because it was not launched from docker containerizer  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.587158 30989 slave.cpp:4527] Sending reconnect request to executor 'mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6' of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000 at executor(1)@10.0.2.74:46154  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.588287 30990 slave.cpp:2838] Re-registering executor 'mesosvol.6ccd993c-1920-11e6-a722-9648cb19afd6' of framework a3ad8418-cb77-4705-b353-4b514ceca52c-0000  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.589736 30988 disk.cpp:169] Updating the disk resources for container 24762d43-2134-475e-b724-caa72110497a to cpus(*):0.1; mem(*):32  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.590117 30990 cpushare.cpp:389] Updated 'cpu.shares' to 102 (cpus 0.1) for container 24762d43-2134-475e-b724-caa72110497a  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.591284 30990 cpushare.cpp:411] Updated 'cpu.cfs_period_us' to 100ms and 'cpu.cfs_quota_us' to 10ms (cpus 0.1) for container 24762d43-2134-475e-b724-caa72110497a  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.595403 30992 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 32MB for container 24762d43-2134-475e-b724-caa72110497a  May 13 16:58:30 ip-10-0-2-74.us-west-2.compute.internal mesos-slave[30985]: I0513 16:58:30.596102 30992 mem.cpp:388] Updated 'memory.limit_in_bytes' to 32MB for container 24762d43-2134-475e-b724-caa72110497a  {noformat}",3,3.012271
MESOS-5382,Implement os::fsync,,1,3.490065
MESOS-5383,Implement os::setHostname,,1,3.490065
MESOS-5386,Add `HANDLE` overloads for functions that take a file descriptor,,3,1.8808968
MESOS-5388,MesosContainerizerLaunch flags execute arbitrary commands via shell,"For example, the docker volume isolator's containerPath is appended (without sanitation) to a command that's executed in this manner. As such, it's possible to inject arbitrary shell commands to be executed by mesos.    https://github.com/apache/mesos/blob/17260204c833c643adf3d8f36ad8a1a606ece809/src/slave/containerizer/mesos/launch.cpp#L206    Perhaps instead of strings these commands could/should be sent as string arrays that could be passed as argv arguments w/o shell interpretation?",3,2.0335896
MESOS-5389,docker containerizer should prefix relative volume.container_path values with the path to the sandbox,docker containerizer currently requires absolute paths for values of volume.container_path. this is inconsistent with the mesos containerizer which requires relative container_path. it makes for a confusing API. both at the Mesos level as well as at the Marathon level.    ideally the docker containerizer would allow a framework to specify a relative path for volume.container_path and in such cases automatically convert it to an absolute path by prepending the sandbox directory to it.    /cc [~jieyu],3,2.5828462
MESOS-5390,v1 Executor Protos not included in maven jar,"According to MESOS-4793 the Executor v1 HTTP API was released in Mesos 0.28.0 however the corresponding protos are not included in the maven jar for version 0.28.0 or 0.28.1.    Script to verify  {code}  wget https://repo.maven.apache.org/maven2/org/apache/mesos/mesos/0.28.1/mesos-0.28.1.jar && unzip -lf mesos-0.28.1.jar | grep ""v1\/executor"" | wc -l  {code}",1,2.661487
MESOS-5391,Add support for controlling resource limits in Mesos containerizer.,"Currently, we dont have ability to control system resource limits. Add support for :  - Frameworks to specify resource limits  - Operators to override default resource limits.",5,2.61673
MESOS-5392,Design doc for adding resource limits support for Mesos containerizer,This will be the design doc for MESOS-5391.,3,2.9775276
MESOS-5397,Slave/Agent Rename Phase 1: Update terms in the website,The following files need to be updated    site/source/index.html.md  ,1,2.55584
MESOS-5398,Rewrite os::read() to be friendlier to reading binary files,"The existing read() implementation is based on calling getline() to  read in chunks of data from a file. This is fine for text-based files,  but is a little strange for binary files.",3,1.895112
MESOS-5399,Add utility for parsing ld.so.cache on linux.,The /etc/ld.so.cache file on linux contains a mapping of dynamic library names to their fully resolved paths for use by ld when linking.    We should write a utility that knows how to parse this file so we can find the paths to these libraries as well.  This is especially important for collecting libraries into a common location for supporting Nvidia GPUs in mesos.,5,2.0396144
MESOS-5400,Add preliminary support for parsing ELF files in stout.,"The upcoming Nvidia GPU support for docker containers in Mesos relies on consolidating all Nvidia shared libraries into a common location for injecting a volume into a container.    As part of this, we need some preliminary parsing capabilities for ELF file to infer things about each shared library we are consolidating.",5,2.4593284
MESOS-5401,Add ability to inject a Volume of Nvidia libraries/binaries into a docker-image container in mesos containerizer.,"In order to support Nvidia GPUs with docker containers in Mesos, we need to be able to consolidate all Nvidia libraries into a common volume and inject that volume into the container.    This tracks the support in the mesos containerizer. The docker containerizer support will be tracked separately.    More info on why this is necessary here: https://github.com/NVIDIA/nvidia-docker/",5,2.75361
MESOS-5403,Introduce ObjectApprover Interface to Authorizer.,As outlined here (https://docs.google.com/document/d/1FuS79P8uj5PIBycrBlkJSBKOtmeO8ezAuiNXxwIA3qA) we plan to add the option of retrieving a FilterObject from the Authorizer with the goal of allowing for efficient authorization of a large number of (potentially large) objects. ,5,2.7512288
MESOS-5404,Allow `Task` to be authorized.,"As we need to be able to authorize `Tasks` (e.g., for deciding whether to include them in the /state endpoint when applying authorization based filtering) we need to expose it to the authorizer. Secondly we also need to include some additional information (`user` and `Env variables`) in order to provide the authorizer  with meaning information.",3,2.5740778
MESOS-5405,Make fields in authorization::Request protobuf optional.,"Currently {{authorization::Request}} protobuf declares {{subject}} and {{object}} as required fields. However, in the codebase we not always set them, which renders the message in the uninitialized state, for example:   * https://github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/common/http.cpp#L603   * https://github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/master/http.cpp#L2057    I believe that the reason why we don't see issues related to this is because we never send authz requests over the wire, i.e., never serialize/deserialize them. However, they are still invalid protobuf messages. Moreover, some external authorizers may serialize these messages.    We can either ensure all required fields are set or make both {{subject}} and {{object}} fields optional. This will also require updating local authorizer, which should properly handle the situation when these fields are absent. We may also want to notify authors of external authorizers to update their code accordingly.    It looks like no deprecation is necessary, mainly because we already—erroneously!—treat these fields as optional.",3,2.146032
MESOS-5406,Validate ACLs on creating an instance of local authorizer.,"Some combinations of ACLs are not allowed, for example, specifying both {{SetQuota}} and {{UpdateQuota}}. We should capture such issues and error out early.     This ticket aims to add as many validations as possible to a dedicated {{validate()}} routine, instead of having them implicitly in the codebase.",3,2.4821076
MESOS-5408,Delete the /observe HTTP endpoint,"The ""/observe"" endpoint was introduced a long time ago for supporting functionality that was never implemented. We should just kill this endpoint and associated code to avoid tech debt.",2,3.6777623
MESOS-5413,`network/cni` isolator should skip the bind mounting of the CNI network information root directory if possible,"Currently in the create() method `network/cni` isolator, for the CNI network information root directory (i.e., {{/var/run/mesos/isolators/network/cni}}), we do a self bind mount and make sure it is a shared mount of its own peer group. However, we should not do a self bind mount if the mount containing the CNI network information root directory is already a shared mount in its own share peer group, just like what we did for `filesystem/linux` isolator in [MESOS-5239 | https://issues.apache.org/jira/browse/MESOS-5239].",3,2.6766052
MESOS-5419,Document all known client libraries for the Scheduler/Executor API,"Previously during various community syncs, we had decided that we would only be supporting the C++ scheduler/executor library in the Mesos code base going forward. We should however, still document the client libraries available in various languages to drive adoption/have a recommended list for users to look up.    This can be similar to the already existing frameworks doc: http://mesos.apache.org/documentation/latest/frameworks/    Other projects also seem to have been following a similar practice:  https://docs.docker.com/engine/reference/api/remote_api_client_libraries/  https://github.com/kubernetes/kubernetes/blob/master/docs/devel/client-libraries.md",2,3.5023375
MESOS-5420,Implement os::exists for processes,"os::exists returns true if the process identified by the parameter is still running or was running and we are able to get information about it, such us the exit code. In Windows after obtaining a handle to the process it is possible perform those operations. ",1,3.7806883
MESOS-5425,Consider using IntervalSet for Port range resource math,Follow-up JIRA for comments raised in MESOS-3051 (see comments there).    We should consider utilizing [{{IntervalSet}}|https://github.com/apache/mesos/blob/a0b798d2fac39445ce0545cfaf05a682cd393abe/3rdparty/stout/include/stout/interval.hpp] in [Port range resource math|https://github.com/apache/mesos/blob/a0b798d2fac39445ce0545cfaf05a682cd393abe/src/common/values.cpp#L143].,3,1.6422825
MESOS-5426,Relax version compatibility requirement for some modules,"Some module interfaces such as authenticatee, have not changed for a while and so we should be able to relax the version compatibility checks. This needs to be done on a case-by-case basis.    I am also hoping, this change will also provide a framework for updating the version requirement for other modules as we go towards a stable module API.    [cc: [~adam-mesos] [~tillt] ]",5,1.5606508
MESOS-5435,Add default implementations to all Isolator virtual functions,"Currently, all of the virtual functions in `mesos::slave::Isolator` are pure virtual (expect status()). For many isolators, however, it doesn't make sense to implement all of these virtual functions. Each isolator has to provide its own default implementation of these functions even if they aren't really relying on them. This adds unnecessary extra code to many isolators that don't need them.    Moreover, the `MesosIsolatorProcess` has the same problem for each of its virtual functions.    We should provide defaults for these instead of making each and every isolator implement even in cases when it doesn't make sense.",1,2.5722208
MESOS-5436,GPU resource broke framework data table in webUI,"In agent_framework.html and master/static/agent.html, we add {{GPUs (Used / Allocated)}} in table header. But we didn't add the corresponding column to the table body as well.    On the other hand, we didn't provide statistics for gpus on monitor endpoints.  To provide those data in webui, it requires we implement gpus statistics in monitor endpoints firstly. ",1,1.6467373
MESOS-5437,AppC  appc_simple_discovery_uri_prefix is lost in configuration.md,AppC  appc_simple_discovery_uri_prefix is lost in configuration.md,1,2.134183
MESOS-5445,Allow libprocess/stout to build without first doing `make` in 3rdparty.,"After the 3rdparty reorg, libprocess/stout are enable to build their dependencies and so one has to do `make` in 3rdpart/ before building libprocess/stout.",2,2.3747385
MESOS-5450,Make the SASL dependency optional.,"Right now there is a hard dependency on SASL, which probably won't work well on Windows (at least) in the near future for our use cases.    In the future, it would be nice to have a pluggable authentication layer.",2,3.5193772
MESOS-5452,Agent modules should be initialized before all components except firewall.,"On Mesos Agents Anonymous modules should not have any dependencies, by design, on any other Mesos components. This implies that Anonymous modules should be initialized before all other Mesos components other than `Firewall`. The dependency on `Firewall` is primarily to enforce any policies to secure endpoints that might be owned by the Anonymous module.",1,2.7306995
MESOS-5453,CNI should not store subnet of address in NetworkInfo,"When the CNI isolator executes the CNI plugin, that CNI plugin will return an IP Address and Subnet (192.168.0.1/32). Mesos should strip the subnet before storing the address in the Task.NetworkInfo.IPAddress.    Reason being - most current mesos components are not expecting a subnet in the Task's NetworkInfo.IPAddress, and instead expect just the IP address. This can cause errors in those components, such as Mesos-DNS failing to return a NetworkInfo address (and instead defaulting to the next configured IPSource), and Marathon generating invalid links to tasks (as it includes /32 in the link)",2,2.2223208
MESOS-5456,Master anonymous modules should initialized before any other components.,"Anonymous modules on the Master are by design supposed to be independent of any Mesos components. However, there might be a dependency in the reverse direction. For e.g., Anonymous modules might want to influence the behavior of Mesos components (say by generating configuration, that might be consumed later by the components).     The Anonymous modules on the Master therefore need to be initialized before other Mesos components. ",1,2.5738754
MESOS-5459,Update RUN_TASK_WITH_USER to use additional metadata,"Currently, the `authorization::Action` `RUN_TASK_WITH_USER` will pass the user as its `Object.value` string, but some authorizers may want to make authorization decisions based on additional task attributes, like role, resources, labels, container type, etc.    We should create a new Action `RUN_TASK` that passes FrameworkInfo and TaskInfo in its Object, and the LocalAuthorizer's RunTaskWithUser ACL can be implemented using the user found in TaskInfo/FrameworkInfo.  We may need to leave the old _WITH_USER action around, but it's arguable whether we should call the authorizer once for RUN_TASK and once for RUN_TASK_WITH_USER, or only use the new action and deprecate the old one?",5,2.2785625
MESOS-5469,Remove hard-coded principals in `PersistentVolumeEndpointsTest.SlavesEndpointFullResources`,"In the test {{PersistentVolumeEndpointsTest.SlavesEndpointFullResources}}, the value {{test-principal}} is hard-coded into the JSON strings expected in HTTP responses. It would be more durable to use {{DEFAULT_CREDENTIAL.principal()}} instead.",1,0.66324824
MESOS-5470,Confirm errors in authorized persistent volume tests,The tests {{PersistentVolumeTest.BadACLDropCreateAndDestroy}} and {{PersistentVolumeTest.BadACLNoPrincipal}} check for a failed Destroy operation by confirming that the persistent volume is still contained in an offer received after the attempted operation. We should also explicitly check that the operation did not succeed due to failed authorization.,1,2.6350775
MESOS-5471,Enable `Option` to handle string literals gracefully,"In {{FlagsBase::add}}, MESOS-5064 begins making use of template function parameters like {{T2*}} for the default flag value rather than {{Option<T2>&}}. This is because in some places in the code base, we pass string literals for this argument. If an {{Option}} type is used, the compiler infers a {{char [x]}} type for {{T2}}, which breaks {{Option::getOrElse}}, which attempts to return that same type, since returning arrays is disallowed.    To fix this, we could employ {{std::decay}}, which would convert a return type of {{char [x]}} into {{const char *}}.",2,1.937346
MESOS-5531,Re-enable style-check for stout.,"After the 3rdparty reorg, the mesos-style checker stopped checking stout.",1,2.2191207
MESOS-5532,Maven build is too verbose for batch builds,"During a non-interactive (without terminal) Mesos build, maven generates several thousands of log lines when downloading artifacts. This often makes several web-based log viewers unresponsive.    Further, these several thousand line long progress indicator logs don't provide any meaningful information either. From a user's point of view, just knowing that the artifact download succeeded/failed is often enough.    We should be using '--batch-mode' flag to disable these additionals log lines.",1,1.9055009
MESOS-5537,http v1 SUBSCRIBED scheduler event always has nil http_interval_seconds,"I'm writing a controller in Go to monitor heartbeats. I'd like to use the interval as communicated by the master, which should be specified in the SUBSCRIBED event. But it's not.    {code}  2016/06/03 18:34:04 {Type:SUBSCRIBED Subscribed:&Event_Subscribed{FrameworkID:&mesos.FrameworkID{Value:ffdb6d6e-0167-4fa2-98f9-2c3f8157fc25-0004,},HeartbeatIntervalSeconds:nil,} Offers:nil Rescind:nil Update:nil Message:nil Failure:nil Error:nil}  {code}    {code}  $ dpkg -l |grep -e mesos  ii  mesos                               0.28.0-2.0.16.ubuntu1404         amd64        Cluster resource manager with efficient resource isolation  {code}    I *am* seeing HEARTBEAT events. Just not seeing the interval specified in the SUBSCRIBED event.",1,1.4611624
MESOS-5549,Document aufs provisioner backend.,We should update container-image.md with the newly supported backend.,2,2.8738945
MESOS-5550,Remove Nvidia GPU Isolator's link-time dependence on `libnvidia-ml`,"The current Nvidia GPU isolator has a dependence on `libnvidia-ml`, and as such, pulls a hard dependence on this library into `libmesos`. The consequence of this is that any process that relies on `libmesos` has to have `libnvidia-ml` available as well, even on machines where no GPUs are available.  Since this library is not easily installable through standard package managers, having such a hard dependence can be burdensome.    This ticket proposes to pull in `libnvidia-ml` as a run-time dependence instead of a link-time dependence. As such, only machines that actually have GPUs installed and would like to rely on this library need to have it installed.",2,0.8107121
MESOS-5551,Move the Nvidia GPU isolator from `cgroups/devices/gpu/nvidia` to `gpu/nvidia`,"Currently, the Nvidia GPU isolator lives in `src/slave/containerizers/mesos/isolators/cgroups/devices/gpu/nvidia`. However, in the future this isolator will do more than simply isolate GPUs using the cgroups devices subsystem (e.g. volume management for injecting machine specific Nvidia libraries into a container). For this reason, we should preemptively move this isolator up to `src/slave/containerizers/mesos/isolators/gpu/nvidia`. As part of this, we should update the string we pass to the `--isolator` agent flag to reflect this.",2,1.7662847
MESOS-5552,Bundle NVML headers for Nvidia GPU support.,"Currently, we rely on a script to install the Nvidia GDK as a build dependence for building Mesos with Nvidia GPU support.    A previous ticket removed the Mesos build dependence on `libnvidia-ml` which comes as part of the GDK. This ticket proposes bundling the NVML headers with Mesos in order to completely remove the build dependence on the GDK.    With this change it will be much simpler to configure and build with Nvidia GPU support.  All that will be required is:  {noformat}  ../configure --enable-nvidia-gpu-support  make -j  {noformat}  ",1,2.7802734
MESOS-5554,Change major/minor device types for Nvidia GPUs to `unsigned int`,"Currently, the GPU struct specifies the type of its `major` and `minor` fields as `dev_t`, which is actually a concatenation of both the major and minor device numbers accessible through the `major()` and `minor()` macros. These macros return an `unsigned int` when handed a `dev_t`, so it makes sense for these fields to be of that type instead.",1,0.9167713
MESOS-5555,Always provide access to NVIDIA control devices within containers (if GPU isolation is enabled).,"Currently, access to `/dev/nvidiactl` and `/dev/nvidia-uvm` is only granted to / revoked from a container as GPUs are added and removed from them. On some level, this makes sense because most jobs don't need access to these devices unless they are also using a GPU. However, there are cases when access to these files is appropriate, even when not making use of a GPU. Running `nvidia-smi` to control the global state of the underlying nvidia driver, for example.        We should add `/dev/nvidiactl` and `/dev/nvidia-uvm` to the default whitelist of devices to include in every container when the `gpu/nvidia` isolator is enabled. This will allow a container to run standard nvidia driver tools (such as `nvidia-smi`) without failing with abnormal errors when no GPUs have been granted to it. As such, these tools will now report that no GPUs are installed instead of failing abnormally.",3,2.770407
MESOS-5556,"Fix method of populating device entries for `/dev/nvidia-uvm`, etc.","Currently, the major/minor numbers of `/dev/nvidiactl` and `/dev/nvidia-uvm` are hard-coded. This causes problems for `/dev/nvidia-uvm` because its major number is part of the ""Experimental"" device range on Linux.    Because this range is experimental, there is no guarantee which device  number will be assigned to it on a given machine.  We should use `os:stat::rdev()` to extract the major/minor numbers programatically.",2,2.0796576
MESOS-5557,Add `NvidiaGpuAllocator` component for cross-containerizer GPU allocation,We need some way of allocating GPUs from a centralized location to allow both the mesos containerizer and the docker containerizer to pull from central pool.  We propose to build a `NvidiaGpuAllocator` for this purpose.        This component should also be overloaded to do resource enumeration of GPUs based on the agent flags. This keeps all code for enumerating GPUs and the resources they represent in a single centralized location.,5,2.3492222
MESOS-5558,Update `Containerizer::resources()` to use the `NvidiaGpuAllocator`,"With the introduction of the shared `NvidiaGpuAllocator` component, `Containerizer::resources()` should be updated to use it.",2,1.1063182
MESOS-5559,Integrate the `NvidiaGpuAllocator` into the `NvidiaGpuIsolator`,,3,1.2053256
MESOS-5561,"Need to remove references to ""messages/messages.hpp"" from `State` API",In order to expose the `State` API for using replicated log in Mesos modules it is necessary that the `State` API does not reference headers that are not exposed as part of the Mesos installation.     Currently include/mesos/state/protobuf.hpp references src/messages/messages.hpp making the `State` API unusable in a module.     We need to move the protobuf `serialize`/`deserialize` functions out of messages.hpp and move them to `stout/protobuf.hpp`. This will help us remove references to messages.hpp from the `State` API.,2,2.3752778
MESOS-5562,Add class to share Nvidia-specific components between containerizers,"Once we have an `NvidiaGPUAllocator` component, we need some way to share it across multiple containerizers.  Moreover, we anticipate needing other Nvidia components to share across multiple containerizers as well (e.g. an `NvidiaVolumeManager` component). As such, we should add a wrapper class around these components to make it easily passable to each containerizer without having to continually add a bunch of parameters to the Containerizer interface.",2,0.9802338
MESOS-5563,Rearrange Nvidia GPU files to cleanup semantics for header inclusion.,"Currently, components outside of `src/slave/containerizers/mesos/isolators/gpu` have to protect their #includes for certain Nvidia header files with the ENABLE_NVIDIA_GPU_SUPPORT flag. Other headers strictly *could not* be wrapped in this flag.        We need to clean up this header madness, by creating a common ""nvidia.hpp"" header that takes care of all the dependencies. All componenents outside of `src/slave/containerizers/mesos/isolators/gpu`  should only need to #include this one header instead of managing everything themselves.",1,2.247474
MESOS-5564,Document common use cases of authorization,"Our authorization documentation covers the existing functionality, but it doesn't provide a practical how-to guide to help users accomplish common authorized use cases. For example, a user recently reported that to gain full use of the web UI after upgrading to Mesos 1.0, six new ACL rules needed to be added: {{get_endpoints, view_frameworks, view_tasks, view_executors,  access_sandboxes, and access_mesos_logs}}. Rather than expecting users to figure this out on their own, we should document the ACLs needed to accomplish a common goal like this.    Similarly, authorizing a stateful framework to accomplish the actions it would usually be expected to perform would involve setting rules for {{register_frameworks, run_tasks, shutdown_frameworks, reserve_resources, unreserve_resources, create_volumes, and destroy_volumes}}.",1,2.5204694
MESOS-5570,Improve CHANGELOG and upgrades.md,Currently we have a lot of data duplication between the CHANGELOG and upgrades.md. We should try to improve this and potentially make the CHANGLOG a markdown file as well. For inspiration see the Hadoop changelog: https://github.com/apache/hadoop/blob/2e1d0ff4e901b8313c8d71869735b94ed8bc40a0/hadoop-common-project/hadoop-common/src/site/markdown/release/1.2.0/CHANGES.1.2.0.md  ,3,2.847685
MESOS-5576,Masters may drop the first message they send between masters after a network partition,"We observed the following situation in a cluster of five masters:  || Time || Master 1 || Master 2 || Master 3 || Master 4 || Master 5 ||  | 0 | Follower | Follower | Follower | Follower | Leader |  | 1 | Follower | Follower | Follower | Follower || Partitioned from cluster by downing this VM's network ||  | 2 || Elected Leader by ZK | Voting | Voting | Voting | Suicides due to lost leadership |  | 3 | Performs consensus | Replies to leader | Replies to leader | Replies to leader | Still down |  | 4 | Performs writing | Acks to leader | Acks to leader | Acks to leader | Still down |  | 5 | Leader | Follower | Follower | Follower | Still down |  | 6 | Leader | Follower | Follower | Follower | Comes back up |  | 7 | Leader | Follower | Follower | Follower | Follower |  | 8 || Partitioned in the same way as Master 5 | Follower | Follower | Follower | Follower |  | 9 | Suicides due to lost leadership || Elected Leader by ZK | Follower | Follower | Follower |  | 10 | Still down | Performs consensus | Replies to leader | Replies to leader || Doesn't get the message! ||  | 11 | Still down | Performs writing | Acks to leader | Acks to leader || Acks to leader ||  | 12 | Still down | Leader | Follower | Follower | Follower |    Master 2 sends a series of messages to the recently-restarted Master 5.  The first message is dropped, but subsequent messages are not dropped.    This appears to be due to a stale link between the masters.  Before leader election, the replicated log actors create a network watcher, which adds links to masters that join the ZK group:  https://github.com/apache/mesos/blob/7a23d0da817be4e8f68d96f524cecf802431033c/src/log/network.hpp#L157-L159    This link does not appear to break (Master 2 -> 5) when Master 5 goes down, perhaps due to how the network partition was induced (in the hypervisor layer, rather than in the VM itself).    When Master 2 tries to send an {{PromiseRequest}} to Master 5, we do not observe the [expected log message|https://github.com/apache/mesos/blob/7a23d0da817be4e8f68d96f524cecf802431033c/src/log/replica.cpp#L493-L494]    Instead, we see a log line in Master 2:  {code}  process.cpp:2040] Failed to shutdown socket with fd 27: Transport endpoint is not connected  {code}    The broken link is removed by the libprocess {{socket_manager}} and the following {{WriteRequest}} from Master 2 to Master 5 succeeds via a new socket.",5,2.6088996
MESOS-5577,Modules using replicated log state API require zookeeper headers,The state API uses zookeeper client headers and hence the bundled zookeeper headers need to be installed during Mesos installation. ,1,2.945825
MESOS-5578,Support static address allocation in CNI,"Currently a framework can't specify a static IP address for the container when using the network/cni isolator.    The `ipaddress` field in the `NetworkInfo` protobuf was designed for this specific purpose but since the CNI spec does not specify a means to allocate an IP address to the container the `network/cni` isolator cannot honor this field even when it is filled in by the framework.    Creating this ticket to act as a place holder to track this limitation. As and when the CNI spec allows us to specify a static IP address for the container, we can resolve this ticket. ",1,2.1795404
MESOS-5579,Support static IP address allocation with `DockerContainerizer`,"Docker run supports the `--ip` option to allocate a specific IPv4 address to the container. Also, the `NetworkInfo` protobuf has an `ipaddress` field that all frameworks to specify an IP address for the container. The docker executor should therefore invoke the `docker run` command with the --ip option whenever the `ipaddress` field of the `NetworkInfo` is set allowing frameworks to try and assign a static IP address for their services.",1,1.2451329
MESOS-5580,Implement authn/authz for the network/cni isolator,Currently any framework can launch containers on any CNI network irrespective of its role and principal. We need perform authn/authz in the network/cni isolator (or Master) to make sure that only roles/principals specified by the operator can launch containers on a given network. ,3,3.9973805
MESOS-5581,Guarantee ordering between Isolators,"Some isolators depend on other isolators. However, we currently do not have a generic method of expressing these dependencies. We special case the `filesystem/*` isolators to make sure that dependencies on them are satisfied, but no other dependencies can be expressed.        Instead, we should use a vector to represent the pairing of isolator name to isolator creator function. This way, the relative dependencies between each isolator will be implicit in the ordering of the vector. Currently, a hashmap is used to hold this pairing, but this is inadequate because hashmaps are inherently unordered. The new implementation using a vector will ensure everything is processed in the order it is listed.",3,2.0770326
MESOS-5582,Create a `cgroups/devices` isolator.,"Currently, all the logic for the `cgroups/devices` isolator is bundled into the Nvidia GPU Isolator. We should abstract it out into it's own component and remove the redundant logic from the Nvidia GPU Isolator. Assuming the guaranteed ordering between isolators from MESOS-5581, we can be sure that the dependency order between the `cgroups/devices` and `gpu/nvidia` isolators is met.",2,2.4666634
MESOS-5583,Improve authorization documentation when setting permissive flag.,"A common problem for a users starting to use acls is that once they set `permisse = false` and not add acls allowing common operations (e.g., register_framework) their Mesos cluster don't  behave as expected. ",1,2.7042854
MESOS-5588,Improve error handling when parsing acls.,"During parsing of the authorizer errors are ignored. This can lead to undetected security issues.    Consider the following acl with an typo (usr instead of user)  {code}     ""view_frameworks"": [                    {                      ""principals"": { ""type"": ""ANY"" },                      ""usr"": { ""type"": ""NONE"" }                    }                  ]  {code}    When the master is started with these flags it will interprete the acl int he following way which gives any principal access to any framework.    {noformat}  view_frameworks {    principals {      type: ANY    }  }  {noformat}",5,2.7141075
MESOS-5592,Pass NetworkInfo to CNI Plugins,"Mesos has adopted the Container Network Interface as a simple means of networking Mesos tasks launched by the Unified Containerizer. The CNI specification covers a minimum feature set, granting the flexibility to add customized networking functionality in the form of agreements made between the orchestrator and CNI plugin.    This proposal is to pass NetworkInfo.Labels to the CNI plugin by injecting it into the CNI network configuration json during plugin invocation.    Design Doc on this change: https://docs.google.com/document/d/1rxruCCcJqpppsQxQrzTbHFVnnW6CgQ2oTieYAmwL284/edit?usp=sharing    reviewboard: https://reviews.apache.org/r/48527/",3,2.2257419
MESOS-5597,"Document Mesos ""health check"" feature",We don't talk about this feature at all.,5,1.8768919
MESOS-5605,Improve documentation for using persistent volumes. ,When using persistent volumes at a arangoDB we ran into a few pitfalls.  We should document them in order for others to avoid those issues.,2,3.2811732
MESOS-5609,Put initial scaffolding in place for implementing SUBSCRIBE call on v1 Master API.,"As discussed on MESOS-5498, this ticket is for tracking work to put the initial scaffolding in place for streaming task status update events to a client that has subscribed to the {{api/v1}} Operator API endpoint. Other events/support for snapshots would be done as part of MESOS-5498.",5,2.896766
MESOS-5618,Added a metric indicating if replicated log for the registrar has recovered or not.,This gives operator insight about the state of the replicated log for registrar. The operator needs to know when it is safe to move on to another master in the upgrade orchestration pipeline.  ,3,2.3832686
MESOS-5629,Agent segfaults after request to '/files/browse',"We observed a number of agent segfaults today on an internal testing cluster. Here is a log excerpt:  {code}  Jun 16 17:12:28 ip-10-10-0-87 mesos-slave[24818]: I0616 17:12:28.522925 24830 status_update_manager.cpp:392] Received status update acknowledgement (UUID: e79ab0f4-2fa2-4df2-9b59-89b97a482167) for task datadog-monitor.804b138b-33e5-11e6-ac16-566ccbdde23e of framework 6d4248cd-2832-4152-b5d0-defbf36f6759-0000  Jun 16 17:12:28 ip-10-10-0-87 mesos-slave[24818]: I0616 17:12:28.523006 24830 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: e79ab0f4-2fa2-4df2-9b59-89b97a482167) for task datadog-monitor.804b138b-33e5-11e6-ac16-566ccbdde23e of framework 6d4248cd-2832-4152-b5d0-defbf36f6759-0000  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: I0616 17:12:29.147181 24824 http.cpp:192] HTTP GET for /slave(1)/state from 10.10.0.87:33356  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: *** Aborted at 1466097149 (unix time) try ""date -d @1466097149"" if you are using GNU date ***  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: PC: @     0x7ff4d68b12a3 (unknown)  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: *** SIGSEGV (@0x0) received by PID 24818 (TID 0x7ff4d31ab700) from PID 0; stack trace: ***  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d6431100 (unknown)  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d68b12a3 (unknown)  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d7eced33 process::dispatch<>()  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d7e7aad7 _ZNSt17_Function_handlerIFN7process6FutureIbEERK6OptionISsEEZN5mesos8internal5slave9Framework15recoverExecutorERKNSA_5state13ExecutorStateEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d7bd1752 mesos::internal::FilesProcess::authorize()  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d7bd1bea mesos::internal::FilesProcess::browse()  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d7bd6e43 std::_Function_handler<>::_M_invoke()  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d85478cb _ZZZN7process11ProcessBase5visitERKNS_9HttpEventEENKUlRKNS_6FutureI6OptionINS_4http14authentication20AuthenticationResultEEEEE0_clESC_ENKUlRKNS4_IbEEE1_clESG_  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d8551341 process::ProcessManager::resume()  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d8551647 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUt_vEEE6_M_runEv  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d6909220 (unknown)  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d6429dc5 start_thread  Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d615728d __clone  Jun 16 17:12:29 ip-10-10-0-87 systemd[1]: dcos-mesos-slave.service: main process exited, code=killed, status=11/SEGV  Jun 16 17:12:29 ip-10-10-0-87 systemd[1]: Unit dcos-mesos-slave.service entered failed state.  Jun 16 17:12:29 ip-10-10-0-87 systemd[1]: dcos-mesos-slave.service failed.  Jun 16 17:12:34 ip-10-10-0-87 systemd[1]: dcos-mesos-slave.service holdoff time over, scheduling restart.  {code}    In every case, the stack trace indicates one of the {{/files/*}} endpoints; I observed this a number of times coming from {{browse()}}, and twice from {{read()}}.    The agent was built from the 1.0.0-rc1 branch, with two cherry-picks applied: [this|https://reviews.apache.org/r/48563/] and [this|https://reviews.apache.org/r/48566/], which were done to repair a different [segfault issue|https://issues.apache.org/jira/browse/MESOS-5587] on the master and agent.    Thanks go to [~bmahler] for digging into this a bit and discovering a possible cause [here|https://github.com/apache/mesos/blob/master/src/slave/slave.cpp#L5737-L5745], where use of {{defer()}} may be necessary to keep execution in the correct context.",3,1.4196448
MESOS-5630,Change build to always enable Nvidia GPU support for Linux,See Summary,2,2.6431007
MESOS-5634,Add Framework Capability for GPU_RESOURCES,"Due to the scarce resource problem described in MESOS-5377, we plan to introduce a GPU_RESOURCES Framework capability. This capability will allow the Mesos allocator to make better decisions about which frameworks should receive resources from GPU capable machines.  In essence, the allocator will ONLY allocate resources from GPU capable machines to frameworks that have this capability. This is necessary to prevent non-GPU workloads from filling up the GPU machines and preventing GPU workloads to run.",3,2.0787952
MESOS-5638,Check all omissions of 'defer' for safety,"When registering callbacks with {{.then}}, {{.onAny}}, etc., we sometimes omit {{defer()}} in cases where it's deemed safe; for example, when the callback uses no process state and thus could be executed in an arbitrary context. Because of recent bugs due to the unsafe omission of {{defer()}}, we should do a sweep of the codebase for all such occurrences and evaluate their safety. We should also consider using {{defer()}} consistently in all such cases, as our [documentation|https://github.com/apache/mesos/tree/master/3rdparty/libprocess#defer] recommends.",5,1.234875
MESOS-5639,Add documentation about metadata for CNI plugins.,We need to document the behavior implemented in MESOS-5592.,2,2.1870706
MESOS-5646,Build `network/cni` isolator with `libnl` support,"Currently, the `network/cni` isolator does not have the ability to collect network statistics for containers launched on a CNI network. We need to give the `network/cni` isolator the ability to query interfaces, route tables and statistics in the containers network namespace. To achieve this the `network/cni` isolator will need to talk `netlink`.    For enabling `netlink` API we need the `network/cni` isolator to be built with libnl support. ",3,2.6181679
MESOS-5647,Expose a statistics endpoint on the `network/cni` isolator.,We need a statistics endpoint in the `network/cni` isolator to expose metrics relating to a containers network traffic.     On receiving a request for a given container the `network/cni` isolator could use NETLINK system calls to query the kernel for interface and routing statistics for a given container's network namespace.,5,2.575447
MESOS-5649,Build an example framework to consume GPUs,This framework should show how to build a GPU capable framework that can accept offers with GPUs and launch tasks that use them.,3,1.6786928
MESOS-5650,UNRESERVE operation causes master to crash.,"{{RESERVE}} operation may cause a master failure:  {noformat}  I0619 05:02:02.298602 11194 http.cpp:312] HTTP GET for /master/slaves from 172.17.0.4:49617 with User-Agent='python-requests/2.9.1'  I0619 05:02:02.305542 11193 http.cpp:312] HTTP POST for /master/destroy-volumes from 172.17.0.4:49618 with User-Agent='python-requests/2.9.1'  I0619 05:02:02.306731 11191 master.cpp:6560] Sending checkpointed resources mem(kafkatest-role, kafkatest-principal, {resource_id: 7408cc53-183c-48c2-a07f-7087806219f3}):256; cpus(kafkatest-role, kafkatest-principal, {resource_id: d7888099-db8f-4018-9109-f70fb1174f53}):1.5; mem(kafkatest-role, kafkatest-principal, {resource_id: b5dd90fc-2c12-4199-9fc4-cf9f918e332b}):2304; ports(kafkatest-role, kafkatest-principal, {resource_id: a0ee4e01-803f-4b71-950d-483caeb01a57}):[9305-9305, 11596-11596]; cpus(kafkatest-role, kafkatest-principal, {resource_id: 8cd72abb-7089-4220-bb90-46b70c9953ab}):0.5; disk(kafkatest-role, kafkatest-principal, {resource_id: ed06ec6e-2d15-4d0e-bbc4-95a942e58596})[]:11204 to slave a80ff9dd-e046-43ab-b763-28365b136f6b-S0 at slave(1)@10.0.0.5:5051 (10.0.0.5)  I0619 05:02:02.311069 11189 http.cpp:312] HTTP POST for /master/destroy-volumes from 172.17.0.4:49619 with User-Agent='python-requests/2.9.1'  I0619 05:02:02.312191 11187 master.cpp:6560] Sending checkpointed resources cpus(kafkatest-role, kafkatest-principal, {resource_id: f1ff4806-0c24-4d60-ad2b-b06462ee4081}):1.5; mem(kafkatest-role, kafkatest-principal, {resource_id: cb8dc92d-64f0-4007-8520-1f63625b98c0}):2304; ports(kafkatest-role, kafkatest-principal, {resource_id: 225b4172-be77-453a-a94f-8845edc3f09a}):[9692-9692, 11824-11824]; cpus(kafkatest-role, kafkatest-principal, {resource_id: 942e102a-ca63-480d-9853-9a39e2695ec9}):0.5; mem(kafkatest-role, kafkatest-principal, {resource_id: cad57f8c-27f5-484c-a3fb-e80da74f0813}):256; disk(kafkatest-role, kafkatest-principal, {resource_id: e6563e09-e284-4aaf-8d53-72056695de41})[]:11204 to slave 489aa72f-ae07-4383-a56f-6fe9346ace37-S7 at slave(1)@10.0.0.7:5051 (10.0.0.7)  I0619 05:02:02.316118 11189 http.cpp:312] HTTP GET for /master/slaves from 172.17.0.4:49620 with User-Agent='python-requests/2.9.1'  I0619 05:02:02.321527 11189 http.cpp:312] HTTP POST for /master/unreserve from 172.17.0.4:49621 with User-Agent='python-requests/2.9.1'  I0619 05:02:02.323523 11193 master.cpp:6560] Sending checkpointed resources  to slave a80ff9dd-e046-43ab-b763-28365b136f6b-S0 at slave(1)@10.0.0.5:5051 (10.0.0.5)  I0619 05:02:02.327658 11191 http.cpp:312] HTTP POST for /master/unreserve from 172.17.0.4:49622 with User-Agent='python-requests/2.9.1'  F0619 05:02:02.329208 11190 sorter.cpp:284] Check failed: total_.scalarQuantities.contains(oldSlaveQuantity)  {noformat}    Possible reasons:  * Recent improvements in allocator (b4d746f)  * Bug in bookkeeping during the previous {{UNRESERVE}}  * Network partition that happened after {{RESERVE}} and before {{UNRESERVE}}",5,2.6500173
MESOS-5657,Executors should not inherit environment variables from the agent.,"Currently executors are inheriting environment variables form the slave in mesos containerizer. This is problematic, because of two reasons:    1. When we use docker images (such as `mongo`) in unified containerizer, duplicated environment variables inherited from the slave lead to initialization failures, because LANG and/or LC_* environment variables are not set correctly.    2. When we are looking at the environment variables from the executor tasks, there are pages of environment variables listed, which is redundant and dangerous.    Depending on the reasons above, we propose that no longer allow executors to inherit environment variables from the slave. Instead, users should specify all environment variables they need by setting the slave flag `--executor_environment_variables` as a JSON format.",3,2.9551325
MESOS-5659,Design doc for TASK_UNREACHABLE,See MESOS-4049.,5,1.8555332
MESOS-5660,ContainerizerTest.ROOT_CGROUPS_BalloonFramework fails because executor environment isn't inherited,A recent change forbits the executor to inherit environment variables from the agent's environment. As a regression this break {{ContainerizerTest.ROOT_CGROUPS_BalloonFramework}}.,2,1.684182
MESOS-5661,Use snake casing for flag names consistently,"Historically, we have always used snake casing for the flag variables e.g., {{docker_config}} etc. However, there are some instances in our .cpp code where we define the flag name in the .cpp file in camel case e.g., {{modulesDir}} but still have the flag name as {{modules_dir}} when taking arguments from the user. It would be good to audit all such occurrences and consistently uses snake casing in our .cpp/.hpp files everywhere.",1,1.9914887
MESOS-5663,Remove hard dependence on libelf for Linux,"    We recently added a hard dependency for `libelf` on Linux. This was in      preparation for some upcoming Nvidia GPU support for injecting volumes      into containers. Since this dependence is not actually necessary for      the upcoming release, we should remove it for now, and rethink the      best way to add it back in later (possibly as a runtime dependence      instead of a linktime one).",1,2.3761427
MESOS-5664,Invalid resources sent to '/reserve' are silently dropped,"If an invalid resource is passed to the master's {{/reserve}} endpoint, it will be silently dropped and not cause an error. This can lead, for example, to a {{/reserve}} request containing a single invalid resource receiving a 200 OK response, despite the fact that no resources were reserved as a result of the request.    This is due to the fact that the {{+=}} operator for {{Resources}} silently drops invalid resources, and this operator is used when parsing the resources in the HTTP request. This could be addressed by validating the resource objects one at a time as they are parsed.",1,0.6713259
MESOS-5666,Deprecate camel case proto field in isolator ContainerConfig.,"Currently there are extra ExecutorInfo and TaskInfo in isolator ContaienrConfig, because a deprecation cycle is needed to deprecate camel cased proto field names. This JIRA is used for tracking this issue, which should address the TODO in isolator.proto.",2,2.433064
MESOS-5667,CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask fails on CentOS 7.,"{noformat}  [22:41:54] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.348641 30896 cluster.cpp:155] Creating default 'local' authorizer  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.353384 30896 leveldb.cpp:174] Opened db in 4.634552ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.354763 30896 leveldb.cpp:181] Compacted db in 1.360201ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.354784 30896 leveldb.cpp:196] Created db iterator in 3421ns  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.354790 30896 leveldb.cpp:202] Seeked to beginning of db in 633ns  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.354797 30896 leveldb.cpp:271] Iterated through 0 keys in the db in 401ns  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.354811 30896 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.354990 30913 recover.cpp:451] Starting replica recovery  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.355123 30915 recover.cpp:477] Replica is in EMPTY status  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.355391 30915 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18695)@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.355479 30912 recover.cpp:197] Received a recover response from a replica in EMPTY status  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.355581 30914 recover.cpp:568] Updating replica status to STARTING  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356091 30910 master.cpp:382] Master 27c796db-6f98-4d61-96c0-f583f22787ff (ip-172-30-2-105.mesosphere.io) started on 172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356104 30910 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/KhgYrQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/KhgYrQ/master"" --zk_session_timeout=""10secs""  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356237 30910 master.cpp:434] Master only allowing authenticated frameworks to register  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356245 30910 master.cpp:448] Master only allowing authenticated agents to register  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356247 30910 master.cpp:461] Master only allowing authenticated HTTP frameworks to register  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356251 30910 credentials.hpp:37] Loading credentials for authentication from '/tmp/KhgYrQ/credentials'  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356351 30910 master.cpp:506] Using default 'crammd5' authenticator  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356389 30910 master.cpp:578] Using default 'basic' HTTP authenticator  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356439 30910 master.cpp:658] Using default 'basic' HTTP framework authenticator  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356467 30910 master.cpp:705] Authorization enabled  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356531 30913 whitelist_watcher.cpp:77] No whitelist given  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356549 30912 hierarchical.cpp:142] Initialized hierarchical allocator process  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356868 30916 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.232816ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356884 30916 replica.cpp:320] Persisted replica status to STARTING  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.356945 30916 recover.cpp:477] Replica is in STARTING status  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.357100 30917 master.cpp:1969] The newly elected leader is master@172.30.2.105:40724 with id 27c796db-6f98-4d61-96c0-f583f22787ff  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.357115 30917 master.cpp:1982] Elected as the leading master!  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.357122 30917 master.cpp:1669] Recovering from registrar  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.357213 30910 registrar.cpp:332] Recovering registrar  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.357429 30913 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18698)@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.357549 30914 recover.cpp:197] Received a recover response from a replica in STARTING status  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.357728 30913 recover.cpp:568] Updating replica status to VOTING  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.358937 30913 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.14792ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.358952 30913 replica.cpp:320] Persisted replica status to VOTING  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.358986 30913 recover.cpp:582] Successfully joined the Paxos group  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.359041 30913 recover.cpp:466] Recover process terminated  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.359180 30916 log.cpp:553] Attempting to start the writer  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.359578 30917 replica.cpp:493] Replica received implicit promise request from (18699)@172.30.2.105:40724 with proposal 1  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.360752 30917 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.157449ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.360767 30917 replica.cpp:342] Persisted promised to 1  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.360982 30914 coordinator.cpp:238] Coordinator attempting to fill missing positions  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.361426 30910 replica.cpp:388] Replica received explicit promise request from (18700)@172.30.2.105:40724 for position 0 with proposal 2  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.362571 30910 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.124969ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.362587 30910 replica.cpp:712] Persisted action at 0  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.362999 30911 replica.cpp:537] Replica received write request for position 0 from (18701)@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.363030 30911 leveldb.cpp:436] Reading position from leveldb took 14967ns  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.364264 30911 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.214497ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.364279 30911 replica.cpp:712] Persisted action at 0  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.364470 30910 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.365622 30910 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.131398ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.365636 30910 replica.cpp:712] Persisted action at 0  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.365643 30910 replica.cpp:697] Replica learned NOP action at position 0  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.365769 30915 log.cpp:569] Writer started with ending position 0  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.366080 30913 leveldb.cpp:436] Reading position from leveldb took 8794ns  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.366284 30915 registrar.cpp:365] Successfully fetched the registry (0B) in 9.053952ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.366315 30915 registrar.cpp:464] Applied 1 operations in 3436ns; attempting to update the 'registry'  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.366487 30911 log.cpp:577] Attempting to append 209 bytes to the log  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.366539 30917 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.366839 30917 replica.cpp:537] Replica received write request for position 1 from (18702)@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.367966 30917 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 1.106053ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.367982 30917 replica.cpp:712] Persisted action at 1  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.368201 30915 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.371786 30915 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 3.566076ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.371803 30915 replica.cpp:712] Persisted action at 1  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.371809 30915 replica.cpp:697] Replica learned APPEND action at position 1  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.372032 30910 registrar.cpp:509] Successfully updated the 'registry' in 5.693952ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.372097 30910 registrar.cpp:395] Successfully recovered registrar  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.372107 30911 log.cpp:596] Attempting to truncate the log to 1  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.372151 30910 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.372218 30911 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.372242 30915 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.372467 30914 replica.cpp:537] Replica received write request for position 2 from (18703)@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.373693 30914 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.207676ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.373708 30914 replica.cpp:712] Persisted action at 2  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.373920 30913 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.375115 30913 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.17978ms  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.375145 30913 leveldb.cpp:399] Deleting ~1 keys from leveldb took 14216ns  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.375154 30913 replica.cpp:712] Persisted action at 2  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.375159 30913 replica.cpp:697] Replica learned TRUNCATE action at position 2  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.383839 30896 containerizer.cpp:201] Using isolation: docker/runtime,filesystem/linux,network/cni  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.388789 30896 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher  [22:41:54]W:	 [Step 10/10] E0619 22:41:54.393234 30896 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:  [22:41:54]W:	 [Step 10/10] sh: hadoop: command not found  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.393265 30896 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.393316 30896 registry_puller.cpp:111] Creating registry puller with docker registry 'https://registry-1.docker.io'  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.395668 30896 cluster.cpp:432] Creating default 'local' authorizer  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.396100 30914 slave.cpp:203] Agent started on 469)@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.396116 30914 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/KhgYrQ/store"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/http_credentials"" --image_providers=""docker"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux,network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/KhgYrQ/configs"" --network_cni_plugins_dir=""/tmp/KhgYrQ/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI""  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.396380 30914 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/credential'  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.396495 30914 slave.cpp:341] Agent using credential for: test-principal  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.396509 30914 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/http_credentials'  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.396586 30914 slave.cpp:393] Using default 'basic' HTTP authenticator  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.396698 30914 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]  [22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.396780 30896 sched.cpp:224] Version: 1.0.0  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.396991 30914 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397020 30914 slave.cpp:600] Agent attributes: [  ]  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397029 30914 slave.cpp:605] Agent hostname: ip-172-30-2-105.mesosphere.io  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397040 30916 sched.cpp:328] New master detected at master@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397068 30916 sched.cpp:394] Authenticating with master master@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397078 30916 sched.cpp:401] Using default CRAM-MD5 authenticatee  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397188 30916 authenticatee.cpp:121] Creating new client SASL connection  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397467 30914 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/meta'  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397476 30912 master.cpp:5943] Authenticating scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397544 30913 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(953)@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397614 30915 status_update_manager.cpp:200] Recovering status update manager  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397668 30912 authenticator.cpp:98] Creating new server SASL connection  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397709 30915 containerizer.cpp:514] Recovering containerizer  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397869 30912 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397886 30912 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397927 30912 authenticator.cpp:204] Received SASL authentication start  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.397964 30912 authenticator.cpp:326] Authentication requires more steps  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398000 30912 authenticatee.cpp:259] Received SASL authentication step  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398052 30912 authenticator.cpp:232] Received SASL authentication step  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398066 30912 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398073 30912 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398087 30912 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398098 30912 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398103 30912 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398108 30912 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398116 30912 authenticator.cpp:318] Authentication success  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398162 30914 authenticatee.cpp:299] Authentication success  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398181 30913 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(953)@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398200 30912 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398270 30914 sched.cpp:484] Successfully authenticated with master master@172.30.2.105:40724  [22:41:54]W:	 [Step 10/10] I0619 22:41:54.398280 30914 sched.cpp:800] Sending SUBSCRIBE call to m...",2,1.8142705
MESOS-5668,Add CGROUP namespace to linux ns helper.,"Since linux kernel 4.6, CGROUP namespace is added. we need to support the handle for the cgroup namespace of the process.    This also relates to two test failures on Ubuntu 16:  {noformat}  [22:41:26] :	 [Step 10/10] [ RUN      ] NsTest.ROOT_setns  [22:41:26] :	 [Step 10/10] ../../src/tests/containerizer/ns_tests.cpp:75: Failure  [22:41:26] :	 [Step 10/10] nstype: Unknown namespace 'cgroup'  [22:41:26] :	 [Step 10/10] [  FAILED  ] NsTest.ROOT_setns (1 ms)  {noformat}    {noformat}  [22:41:26] :	 [Step 10/10] [ RUN      ] NsTest.ROOT_getns  [22:41:26] :	 [Step 10/10] ../../src/tests/containerizer/ns_tests.cpp:160: Failure  [22:41:26] :	 [Step 10/10] nstype: Unknown namespace 'cgroup'  [22:41:26] :	 [Step 10/10] [  FAILED  ] NsTest.ROOT_getns (0 ms)  {noformat}",3,2.613797
MESOS-5669,CNI isolator should not return failure if /etc/hostname does not exist on host.,"/etc/hostname may not necessarily exist on every system (e.g., CentOS 6). Currently CNI isolator just return a failure if it does not exist on host, because the isolator need to mount it into the container. This is fine for /etc/host and /etc/resolv.conf, but we should make an exception for /etc/hostname, because hostname may still be accessible even if /etc/hostname doesn't exist.    This issue relates to 3 failure tests:  {noformat}  [22:45:21] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.647611 24647 cluster.cpp:155] Creating default 'local' authorizer  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.655230 24647 leveldb.cpp:174] Opened db in 7.510408ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.657680 24647 leveldb.cpp:181] Compacted db in 2.427309ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.657702 24647 leveldb.cpp:196] Created db iterator in 6209ns  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.657709 24647 leveldb.cpp:202] Seeked to beginning of db in 692ns  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.657713 24647 leveldb.cpp:271] Iterated through 0 keys in the db in 431ns  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.657727 24647 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.657888 24662 recover.cpp:451] Starting replica recovery  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.658051 24668 recover.cpp:477] Replica is in EMPTY status  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.658495 24664 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18401)@172.30.2.247:42024  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.658583 24662 recover.cpp:197] Received a recover response from a replica in EMPTY status  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.658687 24664 recover.cpp:568] Updating replica status to STARTING  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659111 24664 master.cpp:382] Master 9a4a353b-91c5-43b9-8c37-19245c37758c (ip-172-30-2-247.mesosphere.io) started on 172.30.2.247:42024  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659126 24664 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/l8346Z/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/l8346Z/master"" --zk_session_timeout=""10secs""  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659267 24664 master.cpp:434] Master only allowing authenticated frameworks to register  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659276 24664 master.cpp:448] Master only allowing authenticated agents to register  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659278 24664 master.cpp:461] Master only allowing authenticated HTTP frameworks to register  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659282 24664 credentials.hpp:37] Loading credentials for authentication from '/tmp/l8346Z/credentials'  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659375 24664 master.cpp:506] Using default 'crammd5' authenticator  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659415 24664 master.cpp:578] Using default 'basic' HTTP authenticator  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659495 24664 master.cpp:658] Using default 'basic' HTTP framework authenticator  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659569 24664 master.cpp:705] Authorization enabled  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659684 24666 hierarchical.cpp:142] Initialized hierarchical allocator process  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.659696 24665 whitelist_watcher.cpp:77] No whitelist given  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.660269 24666 master.cpp:1969] The newly elected leader is master@172.30.2.247:42024 with id 9a4a353b-91c5-43b9-8c37-19245c37758c  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.660281 24666 master.cpp:1982] Elected as the leading master!  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.660290 24666 master.cpp:1669] Recovering from registrar  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.660342 24662 registrar.cpp:332] Recovering registrar  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.661232 24669 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.48585ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.661254 24669 replica.cpp:320] Persisted replica status to STARTING  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.661326 24669 recover.cpp:477] Replica is in STARTING status  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.661667 24668 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18404)@172.30.2.247:42024  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.661758 24665 recover.cpp:197] Received a recover response from a replica in STARTING status  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.661893 24664 recover.cpp:568] Updating replica status to VOTING  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.663851 24664 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.915617ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.663866 24664 replica.cpp:320] Persisted replica status to VOTING  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.663899 24664 recover.cpp:582] Successfully joined the Paxos group  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.663944 24664 recover.cpp:466] Recover process terminated  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.664088 24668 log.cpp:553] Attempting to start the writer  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.664556 24668 replica.cpp:493] Replica received implicit promise request from (18405)@172.30.2.247:42024 with proposal 1  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.666551 24668 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.971938ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.666566 24668 replica.cpp:342] Persisted promised to 1  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.666767 24667 coordinator.cpp:238] Coordinator attempting to fill missing positions  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.667230 24668 replica.cpp:388] Replica received explicit promise request from (18406)@172.30.2.247:42024 for position 0 with proposal 2  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.669271 24668 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 2.02399ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.669287 24668 replica.cpp:712] Persisted action at 0  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.669656 24669 replica.cpp:537] Replica received write request for position 0 from (18407)@172.30.2.247:42024  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.669680 24669 leveldb.cpp:436] Reading position from leveldb took 10808ns  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.671674 24669 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.977316ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.671689 24669 replica.cpp:712] Persisted action at 0  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.671907 24665 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.673920 24665 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.991274ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.673935 24665 replica.cpp:712] Persisted action at 0  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.673941 24665 replica.cpp:697] Replica learned NOP action at position 0  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.674190 24665 log.cpp:569] Writer started with ending position 0  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.674489 24663 leveldb.cpp:436] Reading position from leveldb took 9059ns  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.674718 24663 registrar.cpp:365] Successfully fetched the registry (0B) in 14.355968ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.674747 24663 registrar.cpp:464] Applied 1 operations in 3070ns; attempting to update the 'registry'  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.674935 24665 log.cpp:577] Attempting to append 209 bytes to the log  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.674978 24665 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.675242 24666 replica.cpp:537] Replica received write request for position 1 from (18408)@172.30.2.247:42024  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.677088 24666 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 1.823904ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.677103 24666 replica.cpp:712] Persisted action at 1  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.677299 24667 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.679270 24667 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 1.952303ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.679286 24667 replica.cpp:712] Persisted action at 1  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.679291 24667 replica.cpp:697] Replica learned APPEND action at position 1  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.679481 24663 registrar.cpp:509] Successfully updated the 'registry' in 4.715264ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.679503 24666 log.cpp:596] Attempting to truncate the log to 1  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.679560 24667 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.679581 24663 registrar.cpp:395] Successfully recovered registrar  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.679745 24664 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.679774 24662 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.679986 24662 replica.cpp:537] Replica received write request for position 2 from (18409)@172.30.2.247:42024  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.681895 24662 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.891877ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.681910 24662 replica.cpp:712] Persisted action at 2  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.682160 24666 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.684331 24666 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.153217ms  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.684375 24666 leveldb.cpp:399] Deleting ~1 keys from leveldb took 26973ns  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.684383 24666 replica.cpp:712] Persisted action at 2  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.684389 24666 replica.cpp:697] Replica learned TRUNCATE action at position 2  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.691529 24647 containerizer.cpp:201] Using isolation: docker/runtime,filesystem/linux,network/cni  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.694491 24647 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher  [22:45:21]W:	 [Step 10/10] E0619 22:45:21.699741 24647 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:  [22:45:21]W:	 [Step 10/10] sh: hadoop: command not found  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.699769 24647 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.699823 24647 registry_puller.cpp:111] Creating registry puller with docker registry 'https://registry-1.docker.io'  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.700865 24647 linux.cpp:146] Bind mounting '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG' and making it a shared mount  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.707801 24647 cni.cpp:286] Bind mounting '/var/run/mesos/isolators/network/cni' and making it a shared mount  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.714337 24647 cluster.cpp:432] Creating default 'local' authorizer  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.714825 24668 slave.cpp:203] Agent started on 468)@172.30.2.247:42024  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.714839 24668 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/l8346Z/store"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/http_credentials"" --image_providers=""docker"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux,network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/l8346Z/configs"" --network_cni_plugins_dir=""/tmp/l8346Z/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG""  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.715116 24668 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/credential'  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.715195 24668 slave.cpp:341] Agent using credential for: test-principal  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.715214 24668 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/http_credentials'  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.715296 24668 slave.cpp:393] Using default 'basic' HTTP authenticator  [22:45:21]W:	 [Step 10/10] I0619 22:45:21.715400 24668 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]  {noformat}    {noformat}  [22:45:38] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_VerifyCheckpointedInfo  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.459836 24647 cluster.cpp:155] Creating default 'local' authorizer  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.470319 24647 leveldb.cpp:174] Opened db in 10.34226ms  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.472771 24647 leveldb.cpp:181] Compacted db in 2.403554ms  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.472795 24647 leveldb.cpp:196] Created db iterator in 4446ns  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.472801 24647 leveldb.cpp:202] Seeked to beginning of db in 810ns  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.472806 24647 leveldb.cpp:271] Iterated through 0 keys in the db in 393ns  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.472822 24647 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.473093 24665 recover.cpp:451] Starting replica recovery  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.473260 24663 recover.cpp:477] Replica is in EMPTY status  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.473647 24663 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18464)@172.30.2.247:42024  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.473752 24665 recover.cpp:197] Received a recover response from a replica in EMPTY status  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.473896 24667 recover.cpp:568] Updating replica status to STARTING  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.474319 24663 master.cpp:382] Master 64f1f7ac-e810-4fb1-b549-6e29fc62622b (ip-172-30-2-247.mesosphere.io) started on 172.30.2.247:42024  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.474329 24663 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/qJWqSY/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/qJWqSY/master"" --zk_session_timeout=""10secs""  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.474452 24663 master.cpp:434] Master only allowing authenticated frameworks to register  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.474457 24663 master.cpp:448] Master only allowing authenticated agents to register  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.474459 24663 master.cpp:461] Master only allowing authenticated HTTP frameworks to register  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.474463 24663 credentials.hpp:37] Loading credentials for authentication from '/tmp/qJWqSY/credentials'  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.474551 24663 master.cpp:506] Using default 'crammd5' authenticator  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.474598 24663 master.cpp:578] Using default 'basic' HTTP authenticator  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.474643 24663 master.cpp:658] Using default 'basic' HTTP framework authenticator  [22:45:38]W:	 [Step 10/10] I0619 22:45:38.474674 2466...",3,2.4046867
MESOS-5670,MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky.,"{noformat}  [03:36:29] :	 [Step 10/10] [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.461802  2797 cluster.cpp:155] Creating default 'local' authorizer  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.469468  2797 leveldb.cpp:174] Opened db in 7.527163ms  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.470188  2797 leveldb.cpp:181] Compacted db in 699544ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.470206  2797 leveldb.cpp:196] Created db iterator in 4293ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.470211  2797 leveldb.cpp:202] Seeked to beginning of db in 535ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.470216  2797 leveldb.cpp:271] Iterated through 0 keys in the db in 321ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.470230  2797 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.470510  2815 recover.cpp:451] Starting replica recovery  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.470592  2817 recover.cpp:477] Replica is in EMPTY status  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471029  2813 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (19800)@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471139  2816 recover.cpp:197] Received a recover response from a replica in EMPTY status  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471271  2818 recover.cpp:568] Updating replica status to STARTING  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471606  2811 master.cpp:382] Master 6d44b7c1-ac0b-4409-97df-a53fa2e39d09 (ip-172-30-2-29.mesosphere.io) started on 172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471619  2811 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/baXWq5/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/baXWq5/master"" --zk_session_timeout=""10secs""  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471745  2811 master.cpp:434] Master only allowing authenticated frameworks to register  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471753  2811 master.cpp:448] Master only allowing authenticated agents to register  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471757  2811 master.cpp:461] Master only allowing authenticated HTTP frameworks to register  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471761  2811 credentials.hpp:37] Loading credentials for authentication from '/tmp/baXWq5/credentials'  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471829  2811 master.cpp:506] Using default 'crammd5' authenticator  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471868  2811 master.cpp:578] Using default 'basic' HTTP authenticator  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471941  2811 master.cpp:658] Using default 'basic' HTTP framework authenticator  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.471977  2811 master.cpp:705] Authorization enabled  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.472034  2817 hierarchical.cpp:142] Initialized hierarchical allocator process  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.472038  2814 whitelist_watcher.cpp:77] No whitelist given  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.472506  2811 master.cpp:1969] The newly elected leader is master@172.30.2.29:37328 with id 6d44b7c1-ac0b-4409-97df-a53fa2e39d09  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.472522  2811 master.cpp:1982] Elected as the leading master!  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.472527  2811 master.cpp:1669] Recovering from registrar  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.472573  2812 registrar.cpp:332] Recovering registrar  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.473511  2816 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.195002ms  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.473527  2816 replica.cpp:320] Persisted replica status to STARTING  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.473578  2816 recover.cpp:477] Replica is in STARTING status  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.473877  2815 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (19803)@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.473989  2814 recover.cpp:197] Received a recover response from a replica in STARTING status  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.474126  2817 recover.cpp:568] Updating replica status to VOTING  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.474735  2811 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547332ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.474748  2811 replica.cpp:320] Persisted replica status to VOTING  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.474783  2811 recover.cpp:582] Successfully joined the Paxos group  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.474829  2811 recover.cpp:466] Recover process terminated  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.474969  2818 log.cpp:553] Attempting to start the writer  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.475361  2811 replica.cpp:493] Replica received implicit promise request from (19804)@172.30.2.29:37328 with proposal 1  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.475944  2811 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 559444ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.475956  2811 replica.cpp:342] Persisted promised to 1  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.476215  2815 coordinator.cpp:238] Coordinator attempting to fill missing positions  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.476660  2816 replica.cpp:388] Replica received explicit promise request from (19805)@172.30.2.29:37328 for position 0 with proposal 2  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.477262  2816 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 584333ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.477273  2816 replica.cpp:712] Persisted action at 0  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.477699  2815 replica.cpp:537] Replica received write request for position 0 from (19806)@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.477726  2815 leveldb.cpp:436] Reading position from leveldb took 8842ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.478277  2815 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 537361ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.478291  2815 replica.cpp:712] Persisted action at 0  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.478569  2811 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.479132  2811 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 545208ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.479146  2811 replica.cpp:712] Persisted action at 0  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.479152  2811 replica.cpp:697] Replica learned NOP action at position 0  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.479317  2814 log.cpp:569] Writer started with ending position 0  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.479568  2811 leveldb.cpp:436] Reading position from leveldb took 8325ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.479786  2814 registrar.cpp:365] Successfully fetched the registry (0B) in 7.192064ms  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.479822  2814 registrar.cpp:464] Applied 1 operations in 3018ns; attempting to update the 'registry'  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.479995  2818 log.cpp:577] Attempting to append 205 bytes to the log  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.480044  2818 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.480309  2811 replica.cpp:537] Replica received write request for position 1 from (19807)@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.480928  2811 leveldb.cpp:341] Persisting action (224 bytes) to leveldb took 596433ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.480942  2811 replica.cpp:712] Persisted action at 1  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.481148  2815 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.481710  2815 leveldb.cpp:341] Persisting action (226 bytes) to leveldb took 545656ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.481722  2815 replica.cpp:712] Persisted action at 1  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.481727  2815 replica.cpp:697] Replica learned APPEND action at position 1  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.481958  2816 registrar.cpp:509] Successfully updated the 'registry' in 2.119168ms  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.482014  2816 registrar.cpp:395] Successfully recovered registrar  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.482045  2817 log.cpp:596] Attempting to truncate the log to 1  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.482117  2817 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.482166  2816 master.cpp:1777] Recovered 0 agents from the Registry (166B) ; allowing 10mins for agents to re-register  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.482177  2817 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.482404  2817 replica.cpp:537] Replica received write request for position 2 from (19808)@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.482975  2817 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 552763ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.482986  2817 replica.cpp:712] Persisted action at 2  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.483301  2813 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.483870  2813 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 547529ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.483896  2813 leveldb.cpp:399] Deleting ~1 keys from leveldb took 12161ns  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.483904  2813 replica.cpp:712] Persisted action at 2  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.483911  2813 replica.cpp:697] Replica learned TRUNCATE action at position 2  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.492995  2797 containerizer.cpp:201] Using isolation: cgroups/mem,filesystem/posix,network/cni  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.496548  2797 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.503572  2797 cluster.cpp:432] Creating default 'local' authorizer  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.503936  2817 slave.cpp:203] Agent started on 488)@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.503952  2817 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""cgroups/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL""  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504148  2817 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/credential'  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504189  2817 slave.cpp:341] Agent using credential for: test-principal  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504199  2817 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/http_credentials'  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504245  2817 slave.cpp:393] Using default 'basic' HTTP authenticator  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504410  2797 sched.cpp:224] Version: 1.0.0  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504416  2817 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]  [03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504580  2818 sched.cpp:328] New master detected at master@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504613  2818 sched.cpp:394] Authenticating with master master@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504622  2818 sched.cpp:401] Using default CRAM-MD5 authenticatee  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504649  2817 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504673  2817 slave.cpp:600] Agent attributes: [  ]  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504678  2817 slave.cpp:605] Agent hostname: ip-172-30-2-29.mesosphere.io  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504703  2816 authenticatee.cpp:121] Creating new client SASL connection  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504830  2818 master.cpp:5943] Authenticating scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504887  2816 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(991)@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.504982  2811 authenticator.cpp:98] Creating new server SASL connection  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505004  2816 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta'  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505105  2813 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505131  2813 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505138  2818 status_update_manager.cpp:200] Recovering status update manager  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505167  2813 authenticator.cpp:204] Received SASL authentication start  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505200  2813 authenticator.cpp:326] Authentication requires more steps  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505200  2814 containerizer.cpp:514] Recovering containerizer  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505241  2813 authenticatee.cpp:259] Received SASL authentication step  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505300  2812 authenticator.cpp:232] Received SASL authentication step  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505317  2812 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505323  2812 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505331  2812 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505337  2812 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505342  2812 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505347  2812 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505355  2812 authenticator.cpp:318] Authentication success  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505399  2813 authenticatee.cpp:299] Authentication success  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505421  2811 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(991)@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505436  2812 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505534  2816 sched.cpp:484] Successfully authenticated with master master@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505553  2816 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505591  2816 sched.cpp:833] Will retry registration in 11.319315ms if necessary  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505672  2815 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505702  2815 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.505854  2818 master.cpp:2615] Subscribing framework default with checkpointing enabled and capabilities [  ]  [03:36:29]W:	 [Step 10/10] I0618 03:36:29.506031  2818 sched.cpp:723] Framework registered with 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000  [03:36:29]W:	 [...",2,1.0553229
MESOS-5671,MemoryPressureMesosTest.CGROUPS_ROOT_Statistics is flaky.,"{noformat}  [00:48:29] :	 [Step 10/10] [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics  [00:48:29]W:	 [Step 10/10] 1+0 records in  [00:48:29]W:	 [Step 10/10] 1+0 records out  [00:48:29]W:	 [Step 10/10] 1048576 bytes (1.0 MB) copied, 0.000517638 s, 2.0 GB/s  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.000998 25413 cluster.cpp:155] Creating default 'local' authorizer  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.020459 25413 leveldb.cpp:174] Opened db in 19.338463ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.022897 25413 leveldb.cpp:181] Compacted db in 2.416906ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.022919 25413 leveldb.cpp:196] Created db iterator in 4037ns  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.022927 25413 leveldb.cpp:202] Seeked to beginning of db in 769ns  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.022932 25413 leveldb.cpp:271] Iterated through 0 keys in the db in 390ns  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.022944 25413 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.023272 25432 recover.cpp:451] Starting replica recovery  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.023425 25434 recover.cpp:477] Replica is in EMPTY status  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.023748 25434 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (19361)@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.023849 25429 recover.cpp:197] Received a recover response from a replica in EMPTY status  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024019 25435 recover.cpp:568] Updating replica status to STARTING  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024338 25432 master.cpp:382] Master 0e92ffa4-4f26-4cea-84d3-9c67612de1bd (ip-172-30-2-56.mesosphere.io) started on 172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024348 25432 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/jBjY5p/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/jBjY5p/master"" --zk_session_timeout=""10secs""  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024502 25432 master.cpp:434] Master only allowing authenticated frameworks to register  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024508 25432 master.cpp:448] Master only allowing authenticated agents to register  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024513 25432 master.cpp:461] Master only allowing authenticated HTTP frameworks to register  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024516 25432 credentials.hpp:37] Loading credentials for authentication from '/tmp/jBjY5p/credentials'  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024603 25432 master.cpp:506] Using default 'crammd5' authenticator  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024644 25432 master.cpp:578] Using default 'basic' HTTP authenticator  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024701 25432 master.cpp:658] Using default 'basic' HTTP framework authenticator  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024770 25432 master.cpp:705] Authorization enabled  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024883 25435 whitelist_watcher.cpp:77] No whitelist given  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.024885 25434 hierarchical.cpp:142] Initialized hierarchical allocator process  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.025539 25433 master.cpp:1969] The newly elected leader is master@172.30.2.56:53790 with id 0e92ffa4-4f26-4cea-84d3-9c67612de1bd  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.025555 25433 master.cpp:1982] Elected as the leading master!  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.025560 25433 master.cpp:1669] Recovering from registrar  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.025611 25432 registrar.cpp:332] Recovering registrar  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.026397 25431 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.288187ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.026438 25431 replica.cpp:320] Persisted replica status to STARTING  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.026486 25431 recover.cpp:477] Replica is in STARTING status  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.026793 25432 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (19364)@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.026897 25429 recover.cpp:197] Received a recover response from a replica in STARTING status  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.027031 25428 recover.cpp:568] Updating replica status to VOTING  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.028960 25432 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.874668ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.028975 25432 replica.cpp:320] Persisted replica status to VOTING  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.029007 25432 recover.cpp:582] Successfully joined the Paxos group  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.029047 25432 recover.cpp:466] Recover process terminated  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.029209 25430 log.cpp:553] Attempting to start the writer  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.029614 25429 replica.cpp:493] Replica received implicit promise request from (19365)@172.30.2.56:53790 with proposal 1  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.031486 25429 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.850474ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.031502 25429 replica.cpp:342] Persisted promised to 1  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.031726 25431 coordinator.cpp:238] Coordinator attempting to fill missing positions  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.032245 25428 replica.cpp:388] Replica received explicit promise request from (19366)@172.30.2.56:53790 for position 0 with proposal 2  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.034101 25428 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.831441ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.034117 25428 replica.cpp:712] Persisted action at 0  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.034561 25433 replica.cpp:537] Replica received write request for position 0 from (19367)@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.034589 25433 leveldb.cpp:436] Reading position from leveldb took 10586ns  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.036419 25433 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.817267ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.036434 25433 replica.cpp:712] Persisted action at 0  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.036679 25429 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.038661 25429 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.96521ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.038677 25429 replica.cpp:712] Persisted action at 0  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.038682 25429 replica.cpp:697] Replica learned NOP action at position 0  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.038839 25435 log.cpp:569] Writer started with ending position 0  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.039198 25433 leveldb.cpp:436] Reading position from leveldb took 10572ns  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.039412 25433 registrar.cpp:365] Successfully fetched the registry (0B) in 13.778944ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.039448 25433 registrar.cpp:464] Applied 1 operations in 4778ns; attempting to update the 'registry'  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.039643 25428 log.cpp:577] Attempting to append 205 bytes to the log  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.039696 25432 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.039945 25430 replica.cpp:537] Replica received write request for position 1 from (19368)@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.041738 25430 leveldb.cpp:341] Persisting action (224 bytes) to leveldb took 1.771112ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.041754 25430 replica.cpp:712] Persisted action at 1  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.041977 25432 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.043805 25432 leveldb.cpp:341] Persisting action (226 bytes) to leveldb took 1.810425ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.043820 25432 replica.cpp:712] Persisted action at 1  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.043825 25432 replica.cpp:697] Replica learned APPEND action at position 1  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.044040 25430 registrar.cpp:509] Successfully updated the 'registry' in 4.556032ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.044100 25430 registrar.cpp:395] Successfully recovered registrar  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.044124 25428 log.cpp:596] Attempting to truncate the log to 1  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.044215 25431 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.044244 25430 master.cpp:1777] Recovered 0 agents from the Registry (166B) ; allowing 10mins for agents to re-register  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.044317 25433 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.044497 25433 replica.cpp:537] Replica received write request for position 2 from (19369)@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.046368 25433 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.851883ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.046383 25433 replica.cpp:712] Persisted action at 2  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.046583 25430 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.048426 25430 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.821628ms  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.048455 25430 leveldb.cpp:399] Deleting ~1 keys from leveldb took 14283ns  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.048463 25430 replica.cpp:712] Persisted action at 2  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.048468 25430 replica.cpp:697] Replica learned TRUNCATE action at position 2  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.055145 25413 containerizer.cpp:203] Using isolation: cgroups/mem,filesystem/posix,network/cni  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.058349 25413 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.069301 25413 cluster.cpp:432] Creating default 'local' authorizer  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.069707 25431 slave.cpp:203] Agent started on 485)@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.069718 25431 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""cgroups/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p""  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.069916 25431 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/credential'  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.069967 25431 slave.cpp:341] Agent using credential for: test-principal  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.069984 25431 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/http_credentials'  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070050 25431 slave.cpp:393] Using default 'basic' HTTP authenticator  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070127 25431 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]  [00:48:30]W:	 [Step 10/10] Trying semicolon-delimited string format instead  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070282 25431 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070309 25431 slave.cpp:600] Agent attributes: [  ]  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070314 25431 slave.cpp:605] Agent hostname: ip-172-30-2-56.mesosphere.io  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070484 25413 sched.cpp:224] Version: 1.0.0  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070667 25433 sched.cpp:328] New master detected at master@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070711 25429 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/meta'  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070749 25433 sched.cpp:394] Authenticating with master master@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070758 25433 sched.cpp:401] Using default CRAM-MD5 authenticatee  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070793 25430 status_update_manager.cpp:200] Recovering status update manager  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070904 25432 authenticatee.cpp:121] Creating new client SASL connection  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.070914 25430 containerizer.cpp:518] Recovering containerizer  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071049 25432 master.cpp:5943] Authenticating scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071105 25428 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(984)@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071164 25434 authenticator.cpp:98] Creating new server SASL connection  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071241 25434 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071254 25434 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071292 25434 authenticator.cpp:204] Received SASL authentication start  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071336 25434 authenticator.cpp:326] Authentication requires more steps  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071374 25434 authenticatee.cpp:259] Received SASL authentication step  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071553 25434 authenticator.cpp:232] Received SASL authentication step  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071574 25434 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-56' server FQDN: 'ip-172-30-2-56' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071586 25434 auxprop.cpp:179] Looking up auxiliary property '*userPassword'  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071594 25434 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071604 25434 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-56' server FQDN: 'ip-172-30-2-56' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071615 25434 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071619 25434 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071630 25434 authenticator.cpp:318] Authentication success  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071684 25428 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(984)@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071687 25431 authenticatee.cpp:299] Authentication success  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071704 25434 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071826 25431 sched.cpp:484] Successfully authenticated with master master@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071841 25431 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071954 25431 sched.cpp:833] Will retry registration in 731.385085ms if necessary  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.071996 25434 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.072013 25434 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.072180 25430 master.cpp:2615] Subscribing framework default with checkpointing disabled and capabilities [  ]  [00:48:30]W:	 [Step 10/10] I0617 00:48:30.072305 25429 hierarchical.cpp:264] Added framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-000...",2,1.0411898
MESOS-5673,Port mapping isolator may cause segfault if it bind mount root does not exist.,"A check is needed for port mapping isolator for its bind mount root. Otherwise, non-existed port-mapping bind mount root may cause segmentation fault for some cases. Here is the test log:    {noformat}  [00:57:42] :	 [Step 10/10] [----------] 11 tests from PortMappingIsolatorTest  [00:57:42] :	 [Step 10/10] [ RUN      ] PortMappingIsolatorTest.ROOT_NC_ContainerToContainerTCP  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.723029 24841 port_mapping_tests.cpp:229] Using eth0 as the public interface  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.723348 24841 port_mapping_tests.cpp:237] Using lo as the loopback interface  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.735090 24841 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ephemeral_ports:[30001-30999];ports:[31000-32000]  [00:57:42]W:	 [Step 10/10] Trying semicolon-delimited string format instead  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.736006 24841 port_mapping.cpp:1557] Using eth0 as the public interface  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.736331 24841 port_mapping.cpp:1582] Using lo as the loopback interface  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737501 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh3 = '1024'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737545 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh1 = '128'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737578 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_wmem = '4096	16384	4194304'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737608 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_synack_retries = '5'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737637 24841 port_mapping.cpp:1869] /proc/sys/net/core/rmem_max = '212992'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737666 24841 port_mapping.cpp:1869] /proc/sys/net/core/somaxconn = '128'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737694 24841 port_mapping.cpp:1869] /proc/sys/net/core/wmem_max = '212992'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737720 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_rmem = '4096	87380	6291456'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737746 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_time = '7200'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737772 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh2 = '512'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737798 24841 port_mapping.cpp:1869] /proc/sys/net/core/netdev_max_backlog = '1000'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737828 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_intvl = '75'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737854 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_probes = '9'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737879 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_max_syn_backlog = '512'  [00:57:42]W:	 [Step 10/10] I0604 00:57:42.737905 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_retries2 = '15'  [00:57:42]W:	 [Step 10/10] F0604 00:57:42.737968 24841 port_mapping_tests.cpp:448] CHECK_SOME(isolator): Failed to get realpath for bind mount root '/var/run/netns': Not found   [00:57:42]W:	 [Step 10/10] *** Check failure stack trace: ***  [00:57:42]W:	 [Step 10/10]     @     0x7f8bd52583d2  google::LogMessage::Fail()  [00:57:42]W:	 [Step 10/10]     @     0x7f8bd525832b  google::LogMessage::SendToLog()  [00:57:42]W:	 [Step 10/10]     @     0x7f8bd5257d21  google::LogMessage::Flush()  [00:57:42]W:	 [Step 10/10]     @     0x7f8bd525ab92  google::LogMessageFatal::~LogMessageFatal()  [00:57:42]W:	 [Step 10/10]     @           0xa62171  _CheckFatal::~_CheckFatal()  [00:57:42]W:	 [Step 10/10]     @          0x1931b17  mesos::internal::tests::PortMappingIsolatorTest_ROOT_NC_ContainerToContainerTCP_Test::TestBody()  [00:57:42]W:	 [Step 10/10]     @          0x19e17b6  testing::internal::HandleSehExceptionsInMethodIfSupported<>()  [00:57:42]W:	 [Step 10/10]     @          0x19dc864  testing::internal::HandleExceptionsInMethodIfSupported<>()  [00:57:42]W:	 [Step 10/10]     @          0x19bd2ae  testing::Test::Run()  [00:57:42]W:	 [Step 10/10]     @          0x19bda66  testing::TestInfo::Run()  [00:57:42]W:	 [Step 10/10]     @          0x19be0b7  testing::TestCase::Run()  [00:57:42]W:	 [Step 10/10]     @          0x19c4bf5  testing::internal::UnitTestImpl::RunAllTests()  [00:57:42]W:	 [Step 10/10]     @          0x19e247d  testing::internal::HandleSehExceptionsInMethodIfSupported<>()  [00:57:42]W:	 [Step 10/10]     @          0x19dd3a4  testing::internal::HandleExceptionsInMethodIfSupported<>()  [00:57:42]W:	 [Step 10/10]     @          0x19c38d1  testing::UnitTest::Run()  [00:57:42]W:	 [Step 10/10]     @           0xfd28cb  RUN_ALL_TESTS()  [00:57:42]W:	 [Step 10/10]     @           0xfd24b1  main  [00:57:42]W:	 [Step 10/10]     @     0x7f8bceb89580  __libc_start_main  [00:57:42]W:	 [Step 10/10]     @           0xa607c9  _start  [00:57:43]W:	 [Step 10/10] /mnt/teamcity/temp/agentTmp/custom_script659125926639545396: line 3: 24841 Aborted                 (core dumped) GLOG_v=1 ./bin/mesos-tests.sh --verbose --gtest_filter=""$GTEST_FILTER""  [00:57:43]W:	 [Step 10/10] Process exited with code 134  {noformat}",3,2.1376383
MESOS-5674,Port mapping isolator may fail in 'isolate' method.,"Port mapping isolator may return failure in isolate method, if a symlink to the network namespace handle using that ContainerId already existed. We should overwrite the symlink if it exist.    This affects a couple test failures:  {noformat}  PortMappingIsolatorTest.ROOT_TooManyContainers  PortMappingIsolatorTest.ROOT_ContainerARPExternal  PortMappingIsolatorTest.ROOT_ContainerCMPInternal  PortMappingIsolatorTest.ROOT_NC_HostToContainerTCP  {noformat}    Here is an example failure test log:  {noformat}  [00:28:37] :	 [Step 10/10] [ RUN      ] PortMappingIsolatorTest.ROOT_TooManyContainers  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.046444 24846 port_mapping_tests.cpp:229] Using eth0 as the public interface  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.046728 24846 port_mapping_tests.cpp:237] Using lo as the loopback interface  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.058758 24846 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ephemeral_ports:[30001-30999];ports:[31000-32000]  [00:28:37]W:	 [Step 10/10] Trying semicolon-delimited string format instead  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.059711 24846 port_mapping.cpp:1557] Using eth0 as the public interface  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.059998 24846 port_mapping.cpp:1582] Using lo as the loopback interface  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061126 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh3 = '1024'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061172 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh1 = '128'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061206 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_wmem = '4096	16384	4194304'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061256 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_synack_retries = '5'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061297 24846 port_mapping.cpp:1869] /proc/sys/net/core/rmem_max = '212992'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061331 24846 port_mapping.cpp:1869] /proc/sys/net/core/somaxconn = '128'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061360 24846 port_mapping.cpp:1869] /proc/sys/net/core/wmem_max = '212992'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061390 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_rmem = '4096	87380	6291456'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061419 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_time = '7200'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061450 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh2 = '512'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061480 24846 port_mapping.cpp:1869] /proc/sys/net/core/netdev_max_backlog = '1000'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061511 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_intvl = '75'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061540 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_probes = '9'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061569 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_max_syn_backlog = '512'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.061599 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_retries2 = '15'  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.069964 24846 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.070144 24846 resources.cpp:572] Parsing resources as JSON failed: ports:[31000-31499]  [00:28:37]W:	 [Step 10/10] Trying semicolon-delimited string format instead  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.070677 24867 port_mapping.cpp:2512] Using non-ephemeral ports {[31000,31500)} and ephemeral ports [30208,30720) for container container1 of executor ''  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.071688 24846 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWNS | CLONE_NEWNET  [00:28:37]W:	 [Step 10/10] I0606 00:28:37.084079 24863 port_mapping.cpp:2576] Bind mounted '/proc/11997/ns/net' to '/run/netns/11997' for container container1  [00:28:37] :	 [Step 10/10] ../../src/tests/containerizer/port_mapping_tests.cpp:1438: Failure  [00:28:37] :	 [Step 10/10] (isolator.get()->isolate(containerId1, pid.get())).failure(): Failed to symlink the network namespace handle '/var/run/mesos/netns/container1' -> '/run/netns/11997': File exists  [00:28:37] :	 [Step 10/10] [  FAILED  ] PortMappingIsolatorTest.ROOT_TooManyContainers (57 ms)  {noformat}",3,1.8150966
MESOS-5677,Provide doc examples for dynamic reservation/persistent volumes,"Users have found it difficult to make use of the dynamic reservation and persistent volume features. The API governing use of these features is a bit complicated, and this leads to users having trouble forming correct requests for reservations, volume creation, etc. Providing multiple examples of reserve/unreserve/create/destroy requests would make it much easier for users to get started.",3,3.210567
MESOS-5679,Example frameworks should allow setting failover timeout,"The example frameworks do not currently set a framework failover timeout when they register with the master. This means that when these frameworks are used in prolonged testing scenarios, small network outages can lead to flapping frameworks.    We should either set the failover timeout to a reasonable value in the example frameworks, or add command-line flags that allow the timeout to be set.",2,2.567027
MESOS-5684,Master captures `this` when creating authorization callback,"When exposing its log file, the master currently installs an authorization callback for the log file which captures the master's {{this}} pointer. Such captures have previously caused bugs (MESOS-5629), and this one should be fixed as well. The callback should be dispatched to the master process, and it should be dispatched via the {{self()}} PID.",1,1.9914095
MESOS-5685,The /files/download endpoint's authorization can be compromised,"If a forward slash is appended to the path of a file a user wishes to download via {{/files/download}}, the authorization logic for that path will be bypassed and the file will be downloaded regardless of permissions. This is because we store the authorization callbacks for these paths in a map which is keyed by the path name, so a request to {{/master/log/}} fails to find the callback which is installed for {{/master/log}}. When the master fails to find the callback, it assumes authorization is not required for that path and authorizes the action.    Consider the following excerpt:  {code}  gmann@gmac:~/src/mesos/build⚡  http GET http://127.0.0.1:5050/files/download\?path\=/master/log -a foo:bar  HTTP/1.1 403 Forbidden  Content-Length: 0  Date: Wed, 22 Jun 2016 21:28:53 GMT    gmann@gmac:~/src/mesos/build⚡  http GET http://127.0.0.1:5050/files/download\?path\=/master/log/ -a foo:bar  HTTP/1.1 200 OK  Content-Disposition: attachment; filename=mesos-master.gmac.gmann.log.INFO.20160622-142843.65615  Content-Length: 14432  Content-Type: application/octet-stream  Date: Wed, 22 Jun 2016 21:28:56 GMT    Log file created at: 2016/06/22 14:28:43  Running on machine: gmac  Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg  I0622 14:28:43.476925 2080764672 logging.cpp:194] INFO level logging started!  I0622 14:28:43.477522 2080764672 main.cpp:367] Using 'HierarchicalDRF' allocator  I0622 14:28:43.480650 2080764672 leveldb.cpp:174] Opened db in 2961us  I0622 14:28:43.481046 2080764672 leveldb.cpp:181] Compacted db in 372us  I0622 14:28:43.481078 2080764672 leveldb.cpp:196] Created db iterator in 13us  I0622 14:28:43.481096 2080764672 leveldb.cpp:202] Seeked to beginning of db in 9us  I0622 14:28:43.481111 2080764672 leveldb.cpp:271] Iterated through 0 keys in the db in 8us  I0622 14:28:43.481165 2080764672 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0622 14:28:43.481967 219914240 recover.cpp:451] Starting replica recovery  I0622 14:28:43.482193 219914240 recover.cpp:477] Replica is in EMPTY status  I0622 14:28:43.482589 2080764672 main.cpp:488] Creating default 'local' authorizer  I0622 14:28:43.482719 2080764672 main.cpp:545] Starting Mesos master  I0622 14:28:43.483085 218841088 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@127.0.0.1:5050  I0622 14:28:43.487284 218304512 recover.cpp:197] Received a recover response from a replica in EMPTY status  I0622 14:28:43.487694 219914240 recover.cpp:568] Updating replica status to STARTING  {code}    We could consider disallowing paths which end in trailing slashes.",2,1.7458264
MESOS-5691,SSL downgrade support will leak sockets in CLOSE_WAIT status,"Repro steps:  1) Start a master:  {code}  bin/mesos-master.sh --work_dir=/tmp/master  {code}    2) Start an agent with SSL and downgrade enabled:  {code}  # Taken from http://mesos.apache.org/documentation/latest/ssl/  openssl genrsa -des3 -f4 -passout pass:some_password -out key.pem 4096  openssl req -new -x509 -passin pass:some_password -days 365 -key key.pem -out cert.pem    SSL_KEY_FILE=key.pem SSL_CERT_FILE=cert.pem SSL_ENABLED=true SSL_SUPPORT_DOWNGRADE=true sudo -E bin/mesos-agent.sh --master=localhost:5050 --work_dir=/tmp/agent  {code}    3) Start a framework that launches lots of executors, one after another:  {code}  sudo src/balloon-framework --master=localhost:5050 --task_memory=64mb --task_memory_usage_limit=256mb --long_running  {code}    4) Check FDs, repeatedly  {code}  sudo lsof -i | grep mesos | grep CLOSE_WAIT | wc -l  {code}    The number of sockets in {{CLOSE_WAIT}} will increase linearly with the number of launched executors.",5,2.5082848
MESOS-5697,Support file volume in mesos containerizer.,"Currently in mesos containerizer, the host_path volume (to be bind mounted from a host path) specified in ContainerInfo can only be a directory. We should also support the volume type as a file.",3,2.874445
MESOS-5698,Quota sorter not updated for resource changes at agent.,"Consider this sequence of events:    1. Slave connects, with 128MB of disk.  2. Master offers resources at slave to framework  3. Framework creates a dynamic reservation for 1MB and a persistent volume of the same size on the slave's resources.    => This invokes {{Master::apply}}, which invokes {{allocator->updateAllocation}}, which invokes {{Sorter::update()}} on the framework sorter and role sorter. If the framework's role has a configured quota, it also invokes {{update}} on the quota role sorter -- in this case, the framework's role has no quota, so the quota role sorter is *not* updated.    => {{DRFSorter::update}} updates the *total* resources at a given slave, among updating other state. New total resources will be 127MB of unreserved disk and 1MB of reserved disk with a volume. Note that the quota role sorter still thinks the slave has 128MB of unreserved disk.  4. The slave is removed from the cluster. {{HierarchicalAllocatorProcess::removeSlave}} invokes:    {code}    roleSorter->remove(slaveId, slaves[slaveId].total);    quotaRoleSorter->remove(slaveId, slaves[slaveId].total.nonRevocable());  {code}    {{slaves\[slaveId\].total.nonRevocable()}} is 127MB of unreserved disk and 1MB of reserved disk with a volume. When we remove this from the quota role sorter, we're left with total resources on the reserved slave of 1MB of unreserved disk, since that is the result of subtracting <127MB unreserved, 1MB reserved+volume> from <128MB unreserved>.    The implications of this can't be good: at minimum, we're leaking resources for removed slaves in the quota role sorter. We're also introducing an inconsistency between {{total_.resources\[slaveId\]}} and {{total_.scalarQuantities}}, since the latter has already stripped-out volume/reservation information.",5,2.4602346
MESOS-5699,Create new documentation for Mesos networking.,"With introduction of CNI and dockers support docker user-defined networks, there are quite a few options within Mesos for IP-per-container solutions for container networking.     We therefore need to re-write networking documentation for Mesos highlighting all the networking support that Mesos provides for orchestrating containers on IP networks.",1,2.7115545
MESOS-5704,Fine-grained authorization on /frameworks,"Even if ACLs were defined for the actions VIEW_FRAMEWORKS,  VIEW_EXECUTORS and VIEW_TASKS, the data these actions were  supposed to protect, could still leaked through the master's  /frameworks endpoint, since it didn't enable any authorization  mechanism.",3,2.5346632
MESOS-5705,ZK credential is exposed in /flags and /state,"Mesos allows zk credentials to be embedded in the zk url, but exposes these credentials in the /flags and /state endpoint. Even though /state is authorized, it only filters out frameworks/tasks, so the top-level flags are shown to any authenticated user.    ""zk"": ""zk://dcos_mesos_master:my_secret_password@127.0.0.1:2181/mesos"",    We need to find some way to hide this data, or even add a first-class VIEW_FLAGS acl that applies to any endpoint that exposes flags.",5,2.6634803
MESOS-5706,GET_ENDPOINT_WITH_PATH authz doesn't make sense for /flags,"The master or agent flags are exposed in /state as well as /flags, so any user who wants to disable/control access to the flags likely intends to control access to flags no matter what endpoint exposes them. As such, /flags is a poor candidate for GET_ENDPOINT_WITH_PATH authz, since we care more about protecting the flag data than the specific endpoint path.  We should remove the GET_ENDPOINT authz from master and agent /flags until we can come up with a better solution, perhaps a first-class VIEW_FLAGS acl.",2,1.2535777
MESOS-5707,LocalAuthorizer should error if passed a GET_ENDPOINT ACL with an unhandled path,"Since GET_ENDPOINT_WITH_PATH doesn't (yet) work with any arbitrary path, we should  a) validate --acls and error if GET_ENDPOINT_WITH_PATH has a path object that doesn't match an endpoint that uses this authz strategy.  b) document exactly which endpoints support GET_ENDPOINT_WITH_PATH",3,2.8366718
MESOS-5708,Add authz to /files/debug,"The /files/debug endpoint exposes the attached master/agent log paths and every attached sandbox path, which includes the frameworkId and executorId. Even if sandboxes are protected, we still don't want to expose this information to unauthorized users.",3,2.0301886
MESOS-5709,Authorization for /roles,"The /roles endpoint exposes the list of all roles and their weights, as well as the list of all frameworkIds registered with each role. This is a superset of the information exposed on GET /weights, which we already protect. We should protect the data in /roles the same way.  - Should we reuse VIEW_FRAMEWORK with role (from /state)?  - Should we add a new VIEW_ROLE and adapt GET_WEIGHTS to use it?",3,3.393015
MESOS-5710,The /logging/toggle endpoint accepts requests with any http method,"Any of a GET, POST, PUT, or DELETE to `<master>/logging/toggle?level=INFO&duration=5mins` will set the log level and return 200. To be consistent with REST-like syntax, DELETE, GET, and even POST are wrong and should return a MethodNotAllowed.    Once this endpoint no longer accepts GET, it is no longer appropriate to use the GET_ENDPOINT acl here. Instead we could create a new PUT_ENDPOINT_WITH_PATH acl (which hopefully ignores query params), or add a first-class TOGGLE_LOGGING acl.",3,1.7678901
MESOS-5711,Update AUTHORIZATION strings in endpoint help,"The endpoint help macros support AUTHENTICATION and AUTHORIZATION sections. We added AUTHORIZATION help for some of the newer endpoints, but not the previously authenticated endpoints.    Authorization endpoints needing help string updates:  Master::Http::CREATE_VOLUMES_HELP  Master::Http::DESTROY_VOLUMES_HELP  Master::Http::RESERVE_HELP  Master::Http::STATE_HELP  Master::Http::STATESUMMARY_HELP  Master::Http::TEARDOWN_HELP  Master::Http::TASKS_HELP  Master::Http::UNRESERVE_HELP  Slave::Http::STATE_HELP",2,1.9842205
MESOS-5712,Document exactly what is handled by GET_ENDPOINTS_WITH_PATH acl,"Users may expect that the GET_ENDPOINT_WITH_PATH acl can be used with any Mesos endpoint, but that is not (yet) the case. We should clearly document the list of applicable endpoints, in authorization.md and probably even upgrades.md.",1,2.450585
MESOS-5713,Add a __sockets__ diagnostic endpoint to libprocess.,"Libprocess exposes a endpoint {{/__processes__}}, which displays some info on the existing actors and messages queued up on each.    It would be nice to inspect the state of libprocess's {{SocketManager}} too.  This could be an endpoint like {{/__sockets__}} that exposes information like:  * Inbound FDs: type and source  * Outbound FDs: type and source  * Temporary and persistent sockets  * Linkers and linkees.  * Outgoing messages and their associated socket",3,2.6076407
MESOS-5716,Document docker private registry with authentication support in Unified Containerizer.,Add documentation for docker private registry with authentication support in unified containerizer. This is the basic support for docker private registry.,3,2.674336
MESOS-5717,Can't autodiscovery GPU resources without '--enable-nvidia-gpu-support' and '--nvidia_gpu_devices' flags,"Prerequisite: In MESOS\-5257  ""By default, with no '\-\-nvidia_gpu_devices' flag or `gpus` resources flag, the new auto-discovery will simply enumerate all of the GPUs on the system"" and in MESOS\-5630 ""removes this flag(\-\-enable-nvidia-gpu-support) and enables this support for all builds on Linux.""    So, I '../configure' without any flag, and start agent without '\-\-resources' or '\-\-nvidia_gpu_devices' ,  but can not discovery GPU resources, and I also start agent with '\-\-resources' and '\-\-nvidia_gpu_devices' , it also does not work.    I'm sure the NVIDIA GPUs on my machines are OK, because with '\-\-enable-nvidia-gpu-support' when './configure' and with '\-\-resources', '\-\-nvidia_gpu_devices' when starting agents it works well.",2,1.6716322
MESOS-5723,SSL-enabled libprocess will leak incoming links to forks,"Encountered two different buggy behaviors that can be tracked down to the same underlying problem.    Repro #1 (non-crashy):  (1) Start a master.  Doesn't matter if SSL is enabled or not.  (2) Start an agent, with SSL enabled.  Downgrade support has the same problem.  The master/agent {{link}} to one another.  (3) Run a sleep task.  Keep this alive.  If you inspect FDs at this point, you'll notice the task has inherited the {{link}} FD (master -> agent).  (4) Restart the agent.  Due to (3), the master's {{link}} stays open.  (5) Check master's logs for the agent's re-registration message.  (6) Check the agent's logs for re-registration.  The message will not appear.  The master is actually using the old {{link}} which is not connected to the agent.    ----    Repro #2 (crashy):  (1) Start a master.  Doesn't matter if SSL is enabled or not.  (2) Start an agent, with SSL enabled.  Downgrade support has the same problem.  (3) Run ~100 sleep task one after the other, keep them all alive.  Each task links back to the agent.  Due to an FD leak, each task will inherit the incoming links from all other actors...  (4) At some point, the agent will run out of FDs and kernel panic.    ----    It appears that the SSL socket {{accept}} call is missing {{os::nonblock}} and {{os::cloexec}} calls:  https://github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L794-L806    For reference, here's {{poll}} socket's {{accept}}:  https://github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/poll_socket.cpp#L53-L75  ",2,3.1361203
MESOS-5727,Command executor health check does not work when the task specifies container image.,"Since we launch the task after pivot_root, we no longer has the access to the mesos-health-check binary. The solution is to refactor health check to be a library (libprocess) so that it does not depend on the underlying filesystem.    One note here is that we should strive to keep both the command executor and the task in the same mount namespace so that Mesos CLI tooling does not need to find the mount namespace for the task. It just need to find the corresponding pid for the executor.",5,2.24559
MESOS-5729,Consider allowing the libprocess caller an option to not set CLOEXEC on libprocess sockets,"Both implementations of libprocess's {{Socket}} interface will set the {{CLOEXEC}} option on all new sockets (incoming or outgoing).  This assumption is pervasive across Mesos, but since libprocess aims to be a general-purpose library, the caller should be able to *not* {{CLOEXEC}} sockets when desired.    See TODOs added here: https://reviews.apache.org/r/49281/",3,2.1651435
MESOS-5740,Consider adding `relink` functionality to libprocess,"Currently we don't have the {{relink}} functionality in libprocess.  i.e. A way to create a new persistent connection between actors, even if a connection already exists.     This can benefit us in a couple of ways:  - The application may have more information on the state of a connection than libprocess does, as libprocess only checks if the connection is alive or not.  For example, a linkee may accept a connection, then fork, pass the connection to a child, and subsequently exit.  As the connection is still active, libprocess may not detect the exit.  - Sometimes, the {{ExitedEvent}} might be delayed or might be dropped due to the remote instance being unavailable (e.g., partition, network intermediaries not sending RST's etc).   ",3,1.3261118
MESOS-5742,"When start an agent with `--resources`, the GPU resource can be fractional","So far, the GPU resource is not fractional, only integer values are allowed. But when starting agents with {{\-\-resources='gpu:1.2'}}, it can also work without any warning or error. And in the webui the GPU resource is `1.2`.",1,1.9197845
MESOS-5748,Potential segfault in `link` and `send` when linking to a remote process,"There is a race in the SocketManager, between a remote {{link}} and disconnection of the underlying socket.    We potentially segfault here: https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1512    {{\*socket}} dereferences the shared pointer underpinning the {{Socket*}} object.  However, the code above this line actually has ownership of the pointer:  https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1494-L1499    If the socket dies during the link, the {{ignore_recv_data}} may delete the Socket underneath {{link}}:  https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1399-L1411    ----  The same race exists for {{send}}.    This race was discovered while running a new test in repetition:  https://reviews.apache.org/r/49175/    On OSX, I hit the race consistently every 500-800 repetitions:  {code}  3rdparty/libprocess/libprocess-tests --gtest_filter=""ProcessRemoteLinkTest.RemoteLink""  --gtest_break_on_failure --gtest_repeat=1000  {code}",2,2.4358761
MESOS-5753,Command executor should use `mesos-containerizer launch` to launch user task.,"Currently, command executor and `mesos-containerizer launch` share a lot of the logic. Command executor should in fact, just use `mesos-containerizer launch` to launch the user task.    Potentially, `mesos-containerizer launch` can be also used by custom executor to launch user tasks.",8,2.119078
MESOS-5754,CommandInfo.user not honored in docker containerizer,"Repro by creating a framework that starts a task with CommandInfo.user set, and observe that the dockerized executor is still running as the default (e.g. root).    cc [~kaysoky]",3,2.6444519
MESOS-5755,NVML headers are not installed as part of 3rdparty install with --enable-install-module-dependencies,Review: https://reviews.apache.org/r/49480/  ,2,1.8972721
MESOS-5759,ProcessRemoteLinkTest.RemoteUseStaleLink and RemoteStaleLinkRelink are flaky,"{{ProcessRemoteLinkTest.RemoteUseStaleLink}} and {{ProcessRemoteLinkTest.RemoteStaleLinkRelink}} are failing occasionally with the error:  {code}  [ RUN      ] ProcessRemoteLinkTest.RemoteStaleLinkRelink  WARNING: Logging before InitGoogleLogging() is written to STDERR  I0630 07:42:34.661110 18888 process.cpp:1066] libprocess is initialized on 172.17.0.2:56294 with 16 worker threads  E0630 07:42:34.666393 18765 process.cpp:2104] Failed to shutdown socket with fd 7: Transport endpoint is not connected  /mesos/3rdparty/libprocess/src/tests/process_tests.cpp:1059: Failure  Value of: exitedPid.isPending()    Actual: false  Expected: true  [  FAILED  ] ProcessRemoteLinkTest.RemoteStaleLinkRelink (56 ms)  {code}    There appears to be a race between establishing a socket connection and the test calling {{::shutdown}} on the socket.  Under some circumstances, the {{::shutdown}} may actually result in failing the future in {{SocketManager::link_connect}} error and thereby trigger {{SocketManager::close}}.",1,1.9993349
MESOS-5761,Improve the logic of orphan tasks,"Right now, a task is called orphaned if an agent re-registers with it but the corresponding framework information is not known to the master. This happens immediately after a master failover.    It would great if the master knows the information about the framework even after a failover, irrespective of whether a framework re-registers, so that we don't have orphan tasks. Getting rid of orphan tasks will make the task authorization story easy (see MESOS-5757).",5,3.8532858
MESOS-5765,Add 'systemGetDriverVersion' to NVML abstraction.,This command returns a string representing the version of the underlying Nvidia drivers installed on a host. It will be used by the upcoming {{NvidiaVolume}} component.,2,2.3439999
MESOS-5766,Missing License Information for Bundled NVML headers,See Summary,1,2.1553423
MESOS-5767,Add ELFIO as bundled Dependency to Mesos,"ELFIO is a header-only replacement for parsing ELF binaries. Previously we were using libelf, which introduced both a new build-time dependency as well as a runtime dependence even though we only really needed this library when operating on machines that have GPUs.    By using this header-only library and bundling it with Mesos, we can remove this external dependence altogether.",2,2.3062716
MESOS-5768,Reimplement the stout ELF abstraction in terms of ELFIO,"With the introduction of the new bundled ELFIO library, we need to reimplement our stout ELF abstraction in terms of it.  As part of this, we need to update the tests that use it (i.e. ldcache_test.cpp)",2,2.2452354
MESOS-5769,Add get_abi_version() to ELF abstraction in stout,This function allows us to inspect the {{.note.ABI-tag}} section of an ELF binary to determine the ABI version of the executable / library.  This is needed for checking soe of the logic in building up an NvidiaVolume for injection into a container. ,2,2.1355085
MESOS-5779,Allow Docker v1 ImageManifests to be parsed from the output of `docker inspect`,"    The `docker::spec::v1::ImageManifest` protobuf implements the      official v1 image manifest specification found at:            https://github.com/docker/docker/blob/master/image/spec/v1.md            The field names in this spec are all written in snake_case as are the      field names of the JSON representing the image manifest when reading      it from disk (for example after performing a `docker save`). As such,      the protobuf for ImageManifest also provides these fields in      snake_case. Unfortunately, the `docker inspect` command also provides      a method of retrieving the JSON for an image manifest, with one major      caveat -- it represents all of its top level keys in CamelCase.            To allow both representations to be parsed in the same way, we      should intercept the incoming JSON from either source (disk or `docker      inspect`) and convert it to a canonical snake_case representation.",3,1.1423473
MESOS-5782,Renamed 'commands' to 'pre_exec_commands' in ContainerLaunchInfo.,Currently the 'commands' in isolator.proto ContainerLaunchInfo is somehow confusing. It is a pre-executed command (can be any script or shell command) before launch. We should renamed 'commands' to 'pre_exec_commands' in ContainerLaunchInfo and add comments.,2,1.9609218
MESOS-5787,Add ability to set framework capabilities in 'mesos-execute',"For now, we want to add this so that we can run {{mesos-execute}} against agents that offer GPU resources. In the future, as we add more framework capabilities, this functionality will become more generally useful.",2,1.5839914
MESOS-5788,Consider adding a Java Scheduler Shim/Adapter for the new/old API.,"Currently, for existing JAVA based frameworks, moving to try out the new API can be cumbersome. This change intends to introduce a shim/adapter interface that makes this easier by allowing to toggle between the old/new API (driver/new scheduler library) implementation via an environment variable. This would allow framework developers to transition their older frameworks to the new API rather seamlessly.    This would look similar to the work done for the executor shim for C++ (command/docker executor). ",8,2.6632507
MESOS-5792,Add mesos tests to CMake (make check),Provide CMakeLists.txt and configuration files to build mesos tests using CMake.,8,2.473588
MESOS-5793,Add ability to inject Nvidia devices into a container,,3,2.4945102
MESOS-5802,SlaveAuthorizerTest/0.ViewFlags is flaky.,"{noformat}  [15:24:47] :	 [Step 10/10] [ RUN      ] SlaveAuthorizerTest/0.ViewFlags  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.025609 25322 containerizer.cpp:196] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.030421 25322 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032060 25339 slave.cpp:205] Agent started on 335)@172.30.2.7:43076  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032078 25339 slave.cpp:206] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""true"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C"" --xfs_project_range=""[5000-10000]""  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032306 25339 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/credential'  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032424 25339 slave.cpp:343] Agent using credential for: test-principal  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032441 25339 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/http_credentials'  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032528 25339 slave.cpp:395] Using default 'basic' HTTP authenticator  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032754 25339 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]  [15:24:47]W:	 [Step 10/10] Trying semicolon-delimited string format instead  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032838 25339 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]  [15:24:47]W:	 [Step 10/10] Trying semicolon-delimited string format instead  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032968 25339 slave.cpp:594] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032994 25339 slave.cpp:602] Agent attributes: [  ]  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.032999 25339 slave.cpp:607] Agent hostname: ip-172-30-2-7.ec2.internal.mesosphere.io  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.033291 25339 process.cpp:3322] Handling HTTP event for process 'slave(335)' with path: '/slave(335)/flags'  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.033329 25343 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/meta'  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.033576 25342 status_update_manager.cpp:200] Recovering status update manager  [15:24:47] :	 [Step 10/10] ../../src/tests/slave_authorization_tests.cpp:316: Failure  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.033604 25340 http.cpp:269] HTTP GET for /slave(335)/flags from 172.30.2.7:33866  [15:24:47] :	 [Step 10/10] Value of: (response).get().status  [15:24:47] :	 [Step 10/10]   Actual: ""503 Service Unavailable""  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.033687 25340 containerizer.cpp:522] Recovering containerizer  [15:24:47] :	 [Step 10/10] Expected: OK().status  [15:24:47] :	 [Step 10/10] Which is: ""200 OK""  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.034953 25340 process.cpp:3322] Handling HTTP event for process 'slave(335)' with path: '/slave(335)/state'  [15:24:47] :	 [Step 10/10] Agent has not finished recovery  [15:24:47] :	 [Step 10/10] ../../src/tests/slave_authorization_tests.cpp:320: Failure  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.035152 25343 http.cpp:269] HTTP GET for /slave(335)/state from 172.30.2.7:33868  [15:24:47] :	 [Step 10/10] parse: syntax error at line 1 near: Agent has not finished recovery  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.035768 25341 slave.cpp:841] Agent terminating  [15:24:47]W:	 [Step 10/10] I0707 15:24:47.036150 25337 provisioner.cpp:253] Provisioner recovery complete  [15:24:47] :	 [Step 10/10] [  FAILED  ] SlaveAuthorizerTest/0.ViewFlags, where TypeParam = mesos::internal::LocalAuthorizer (14 ms)  {noformat}",2,1.7318616
MESOS-5806,CNI isolator should prepare network related /etc/* files for containers using host mode but specify container images.,"Currently, the CNI isolator will just ignore those containers that want to join the host network (i.e., not specifying NetworkInfo). However, if the container specifies a container image, we need to make sure that it has access to host /etc/* files. We should perform the bind mount for the container. This is also what docker does when a container is running in host mode.",5,2.4624808
MESOS-5812,MasterAPITest.Subscribe is flaky,"This test seems to be flaky, although on Mac OS X and CentOS 7 the error a bit different.    On Mac OS X:  {noformat}[ RUN      ] ContentType/MasterAPITest.Subscribe/0  I0708 11:42:48.474665 1927435008 cluster.cpp:155] Creating default 'local' authorizer  I0708 11:42:48.480677 1927435008 leveldb.cpp:174] Opened db in 5727us  I0708 11:42:48.481494 1927435008 leveldb.cpp:181] Compacted db in 722us  I0708 11:42:48.481541 1927435008 leveldb.cpp:196] Created db iterator in 19us  I0708 11:42:48.481572 1927435008 leveldb.cpp:202] Seeked to beginning of db in 9us  I0708 11:42:48.481587 1927435008 leveldb.cpp:271] Iterated through 0 keys in the db in 7us  I0708 11:42:48.481617 1927435008 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0708 11:42:48.482030 350982144 recover.cpp:451] Starting replica recovery  I0708 11:42:48.482203 350982144 recover.cpp:477] Replica is in EMPTY status  I0708 11:42:48.484107 348299264 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (3780)@127.0.0.1:50325  I0708 11:42:48.484318 350982144 recover.cpp:197] Received a recover response from a replica in EMPTY status  I0708 11:42:48.484750 348835840 master.cpp:382] Master e055d60c-05ff-487e-82da-d0a43e52605c (localhost) started on 127.0.0.1:50325  I0708 11:42:48.484850 349908992 recover.cpp:568] Updating replica status to STARTING  I0708 11:42:48.484788 348835840 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/tmp/Sn2Kf4/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/tmp/Sn2Kf4/master"" --zk_session_timeout=""10secs""  W0708 11:42:48.485263 348835840 master.cpp:387]   **************************************************  Master bound to loopback interface! Cannot communicate with remote schedulers or agents. You might want to set '--ip' flag to a routable IP address.  **************************************************  I0708 11:42:48.485291 348835840 master.cpp:434] Master only allowing authenticated frameworks to register  I0708 11:42:48.485314 348835840 master.cpp:448] Master only allowing authenticated agents to register  I0708 11:42:48.485335 348835840 master.cpp:461] Master only allowing authenticated HTTP frameworks to register  I0708 11:42:48.485347 348835840 credentials.hpp:37] Loading credentials for authentication from '/private/tmp/Sn2Kf4/credentials'  I0708 11:42:48.485373 349372416 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 397us  I0708 11:42:48.485414 349372416 replica.cpp:320] Persisted replica status to STARTING  I0708 11:42:48.485608 350982144 recover.cpp:477] Replica is in STARTING status  I0708 11:42:48.485749 348835840 master.cpp:506] Using default 'crammd5' authenticator  I0708 11:42:48.485852 348835840 master.cpp:578] Using default 'basic' HTTP authenticator  I0708 11:42:48.486018 348835840 master.cpp:658] Using default 'basic' HTTP framework authenticator  I0708 11:42:48.486140 348835840 master.cpp:705] Authorization enabled  I0708 11:42:48.486486 350982144 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (3783)@127.0.0.1:50325  I0708 11:42:48.486758 352055296 recover.cpp:197] Received a recover response from a replica in STARTING status  I0708 11:42:48.487176 350982144 recover.cpp:568] Updating replica status to VOTING  I0708 11:42:48.487576 352055296 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 300us  I0708 11:42:48.487658 352055296 replica.cpp:320] Persisted replica status to VOTING  I0708 11:42:48.487736 350982144 recover.cpp:582] Successfully joined the Paxos group  I0708 11:42:48.487951 350982144 recover.cpp:466] Recover process terminated  I0708 11:42:48.489441 348835840 master.cpp:1973] The newly elected leader is master@127.0.0.1:50325 with id e055d60c-05ff-487e-82da-d0a43e52605c  I0708 11:42:48.489518 348835840 master.cpp:1986] Elected as the leading master!  I0708 11:42:48.489545 348835840 master.cpp:1673] Recovering from registrar  I0708 11:42:48.489637 350982144 registrar.cpp:332] Recovering registrar  I0708 11:42:48.490120 351518720 log.cpp:553] Attempting to start the writer  I0708 11:42:48.491161 350445568 replica.cpp:493] Replica received implicit promise request from (3784)@127.0.0.1:50325 with proposal 1  I0708 11:42:48.491461 350445568 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 252us  I0708 11:42:48.491528 350445568 replica.cpp:342] Persisted promised to 1  I0708 11:42:48.492337 348299264 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0708 11:42:48.493482 349372416 replica.cpp:388] Replica received explicit promise request from (3785)@127.0.0.1:50325 for position 0 with proposal 2  I0708 11:42:48.493854 349372416 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 283us  I0708 11:42:48.493904 349372416 replica.cpp:712] Persisted action at 0  I0708 11:42:48.495302 348299264 replica.cpp:537] Replica received write request for position 0 from (3786)@127.0.0.1:50325  I0708 11:42:48.495455 348299264 leveldb.cpp:436] Reading position from leveldb took 45us  I0708 11:42:48.495761 348299264 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 261us  I0708 11:42:48.495803 348299264 replica.cpp:712] Persisted action at 0  I0708 11:42:48.496484 350445568 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0708 11:42:48.496795 350445568 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 255us  I0708 11:42:48.496857 350445568 replica.cpp:712] Persisted action at 0  I0708 11:42:48.496896 350445568 replica.cpp:697] Replica learned NOP action at position 0  I0708 11:42:48.497445 350982144 log.cpp:569] Writer started with ending position 0  I0708 11:42:48.498523 350982144 leveldb.cpp:436] Reading position from leveldb took 80us  I0708 11:42:48.499307 349908992 registrar.cpp:365] Successfully fetched the registry (0B) in 9.63712ms  I0708 11:42:48.499464 349908992 registrar.cpp:464] Applied 1 operations in 36us; attempting to update the 'registry'  I0708 11:42:48.499953 351518720 log.cpp:577] Attempting to append 159 bytes to the log  I0708 11:42:48.500088 350982144 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0708 11:42:48.500880 348299264 replica.cpp:537] Replica received write request for position 1 from (3787)@127.0.0.1:50325  I0708 11:42:48.501186 348299264 leveldb.cpp:341] Persisting action (178 bytes) to leveldb took 259us  I0708 11:42:48.501231 348299264 replica.cpp:712] Persisted action at 1  I0708 11:42:48.501786 351518720 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0708 11:42:48.502118 351518720 leveldb.cpp:341] Persisting action (180 bytes) to leveldb took 311us  I0708 11:42:48.502260 351518720 replica.cpp:712] Persisted action at 1  I0708 11:42:48.502305 351518720 replica.cpp:697] Replica learned APPEND action at position 1  I0708 11:42:48.503475 349908992 registrar.cpp:509] Successfully updated the 'registry' in 3.944192ms  I0708 11:42:48.503909 349908992 registrar.cpp:395] Successfully recovered registrar  I0708 11:42:48.504003 350982144 log.cpp:596] Attempting to truncate the log to 1  I0708 11:42:48.504250 349372416 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0708 11:42:48.504546 350445568 master.cpp:1781] Recovered 0 agents from the Registry (121B) ; allowing 10mins for agents to re-register  I0708 11:42:48.506022 352055296 replica.cpp:537] Replica received write request for position 2 from (3788)@127.0.0.1:50325  I0708 11:42:48.506479 352055296 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 320us  I0708 11:42:48.506513 352055296 replica.cpp:712] Persisted action at 2  I0708 11:42:48.506978 351518720 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0708 11:42:48.507155 351518720 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 169us  I0708 11:42:48.507237 351518720 leveldb.cpp:399] Deleting ~1 keys from leveldb took 37us  I0708 11:42:48.507264 351518720 replica.cpp:712] Persisted action at 2  I0708 11:42:48.507285 351518720 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0708 11:42:48.521363 1927435008 cluster.cpp:432] Creating default 'local' authorizer  I0708 11:42:48.522498 350982144 slave.cpp:205] Agent started on 119)@127.0.0.1:50325  I0708 11:42:48.522538 350982144 slave.cpp:206] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/zhitao/Uber/sync/zhitao-mesos1.dev.uber.com/home/uber/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX""  W0708 11:42:48.522903 350982144 slave.cpp:209]   **************************************************  Agent bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address.  **************************************************  I0708 11:42:48.522922 350982144 credentials.hpp:86] Loading credential for authentication from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/credential'  W0708 11:42:48.522965 1927435008 scheduler.cpp:157]   **************************************************  Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.  **************************************************  I0708 11:42:48.522992 1927435008 scheduler.cpp:172] Version: 1.0.0  I0708 11:42:48.523066 350982144 slave.cpp:343] Agent using credential for: test-principal  I0708 11:42:48.523092 350982144 credentials.hpp:37] Loading credentials for authentication from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/http_credentials'  I0708 11:42:48.523334 350982144 slave.cpp:395] Using default 'basic' HTTP authenticator  I0708 11:42:48.523973 352055296 scheduler.cpp:461] New master detected at master@127.0.0.1:50325  I0708 11:42:48.524050 350982144 slave.cpp:594] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0708 11:42:48.524196 350982144 slave.cpp:602] Agent attributes: [  ]  I0708 11:42:48.524224 350982144 slave.cpp:607] Agent hostname: localhost  I0708 11:42:48.525522 350445568 state.cpp:57] Recovering state from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/meta'  I0708 11:42:48.525853 350445568 status_update_manager.cpp:200] Recovering status update manager  I0708 11:42:48.526165 350445568 slave.cpp:4856] Finished recovery  I0708 11:42:48.527223 349372416 status_update_manager.cpp:174] Pausing sending status updates  I0708 11:42:48.527231 352055296 slave.cpp:969] New master detected at master@127.0.0.1:50325  I0708 11:42:48.527276 352055296 slave.cpp:1028] Authenticating with master master@127.0.0.1:50325  I0708 11:42:48.527328 352055296 slave.cpp:1039] Using default CRAM-MD5 authenticatee  I0708 11:42:48.527561 352055296 slave.cpp:1001] Detecting new master  I0708 11:42:48.527582 348299264 authenticatee.cpp:121] Creating new client SASL connection  I0708 11:42:48.528666 349908992 master.cpp:6006] Authenticating slave(119)@127.0.0.1:50325  I0708 11:42:48.528880 352055296 authenticator.cpp:98] Creating new server SASL connection  I0708 11:42:48.529089 350445568 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 127.0.0.1:50918  I0708 11:42:48.529233 350445568 master.cpp:2272] Received subscription request for HTTP framework 'default'  I0708 11:42:48.529261 350445568 master.cpp:2012] Authorizing framework principal 'test-principal' to receive offers for role '*'  I0708 11:42:48.529323 352055296 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5  I0708 11:42:48.529357 352055296 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'  I0708 11:42:48.529417 352055296 authenticator.cpp:204] Received SASL authentication start  I0708 11:42:48.529503 352055296 authenticator.cpp:326] Authentication requires more steps  I0708 11:42:48.529561 352055296 master.cpp:2370] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]  I0708 11:42:48.529721 349908992 authenticatee.cpp:259] Received SASL authentication step  I0708 11:42:48.530005 348835840 authenticator.cpp:232] Received SASL authentication step  I0708 11:42:48.530241 348835840 authenticator.cpp:318] Authentication success  I0708 11:42:48.530254 350445568 hierarchical.cpp:271] Added framework e055d60c-05ff-487e-82da-d0a43e52605c-0000  I0708 11:42:48.530900 349908992 authenticatee.cpp:299] Authentication success  I0708 11:42:48.531186 350982144 master.cpp:6036] Successfully authenticated principal 'test-principal' at slave(119)@127.0.0.1:50325  I0708 11:42:48.531657 348299264 slave.cpp:1123] Successfully authenticated with master master@127.0.0.1:50325  I0708 11:42:48.531935 349372416 master.cpp:4676] Registering agent at slave(119)@127.0.0.1:50325 (localhost) with id e055d60c-05ff-487e-82da-d0a43e52605c-S0  I0708 11:42:48.532304 349908992 registrar.cpp:464] Applied 1 operations in 55us; attempting to update the 'registry'  I0708 11:42:48.532908 348835840 log.cpp:577] Attempting to append 326 bytes to the log  I0708 11:42:48.533015 352055296 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I0708 11:42:48.533641 349372416 replica.cpp:537] Replica received write request for position 3 from (3798)@127.0.0.1:50325  I0708 11:42:48.533867 349372416 leveldb.cpp:341] Persisting action (345 bytes) to leveldb took 186us  I0708 11:42:48.533917 349372416 replica.cpp:712] Persisted action at 3  I0708 11:42:48.537066 349908992 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I0708 11:42:48.538169 349908992 leveldb.cpp:341] Persisting action (347 bytes) to leveldb took 914us  I0708 11:42:48.538226 349908992 replica.cpp:712] Persisted action at 3  I0708 11:42:48.538255 349908992 replica.cpp:697] Replica learned APPEND action at position 3  I0708 11:42:48.539247 352055296 registrar.cpp:509] Successfully updated the 'registry' in 6.895104ms  I0708 11:42:48.539302 348299264 log.cpp:596] Attempting to truncate the log to 3  I0708 11:42:48.539393 348299264 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4  I0708 11:42:48.539798 348835840 master.cpp:4745] Registered agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]  I0708 11:42:48.539881 348299264 hierarchical.cpp:478] Added agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )  I0708 11:42:48.539901 349908992 slave.cpp:1169] Registered with master master@127.0.0.1:50325; given agent ID e055d60c-05ff-487e-82da-d0a43e52605c-S0  I0708 11:42:48.540287 350445568 status_update_manager.cpp:181] Resuming sending status updates  I0708 11:42:48.540501 351518720 replica.cpp:537] Replica received write request for position 4 from (3799)@127.0.0.1:50325  I0708 11:42:48.540583 352055296 master.cpp:5835] Sending 1 offers to framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)  I0708 11:42:48.540798 351518720 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 247us  I0708 11:42:48.540868 351518720 replica.cpp:712] Persisted action at 4  I0708 11:42:48.540895 349908992 slave.cpp:1229] Forwarding total oversubscribed resources   I0708 11:42:48.541035 352055296 master.cpp:5128] Received update of agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) with total oversubscribed resources   I0708 11:42:48.541291 349908992 hierarchical.cpp:542] Agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])  I0708 11:42:48.541630 350982144 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0  I0708 11:42:48.541911 350982144 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 189us  I0708 11:42:48.541965 350982144 leveldb.cpp:399] Deleting ~2 keys from leveldb took 28us  I0708 11:42:48.541987 350982144 replica.cpp:712] Persisted action at 4  I0708 11:42:48.542006 350982144 replica.cpp:697] Replica learned TRUNCATE action at position 4  I0708 11:42:48.544836 352055296 http.cpp:381] HTTP POST for /master/api/v1 from 127.0.0.1:50920  I0708 11:42:48.544884 352055296 http.cpp:484] Processing call SUBSCRIBE  I0708 11:42:48.545382 352055296 master.cpp:7599] Added subscriber: a85e7341-ac15-4f18-9021-1a2efa326442 to the list of active subscribers  I0708 11:42:48.550048 348835840 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 127.0.0.1:50919  I0708 11:42:48.550339 348835840 master.cpp:3468] Processing ACCEPT call for offers: [ e055d60c-05ff-487e-82da-d0a43e52605c-O0 ] on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)  I0708 11:42:48.550390 348835840 mas...",3,1.7970793
MESOS-5822,Add a build script for the Windows CI,"The ASF CI for Mesos runs a script that lives inside the Mesos codebase:  https://github.com/apache/mesos/blob/1cbfdc3c1e4b8498a67f8531ab264003c8c19fb1/support/docker_build.sh    ASF Infrastructure have set up a machine that we can use for building Mesos on Windows.  Considering the environment, we will need a separate script to build here.",3,1.9562573
MESOS-5824,Include disk source information in stringification,"Some frameworks (like kafka_mesos) ignore the Source field when trying to reserve an offered mount or path persistent volume; the resulting error message is bewildering:    {code:none}  Task uses more resources  cpus(*):4; mem(*):4096;     ports(*):[31000-31000]; disk(kafka, kafka)[kafka_0:data]:960679  than available  cpus(*):32; mem(*):256819;  ports(*):[31000-32000]; disk(kafka, kafka)[kafka_0:data]:960679;   disk(*):240169;  {code}    The stringification of disk resources should include source information.  ",3,1.9492793
MESOS-5825,Support mounting image volume in mesos containerizer.,"Mesos containerizer should be able to support mounting image volume type. Specifically, both image rootfs and default manifest should be reachable inside container's mount namespace.",5,3.2639434
MESOS-5828,Modularize Network in replicated_log,Currently replicated_log relies on Zookeeper for coordinator election. This is done through network abstraction _ZookeeperNetwork_. We need to modularize this part in order to enable replicated_log when using Master contender/detector modules.,8,1.8705125
MESOS-5841,Clean up `FlagsBase::add`,"In the definition for {{FlagsBase}}, we currently have 20 overloads for the {{FlagsBase::add}} function. This makes both the {{FlagsBase}} class definition and the {{flags.cpp}} files in Mesos difficult to read. We should clean up {{FlagsBase::add}} so that it does not require so many overloads.",3,0.6106087
MESOS-5844,PersistentVolumeEndpointsTest.OfferCreateThenEndpointRemove test is flaky,"Observed on ASF CI: https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=gcc,CONFIGURATION=--verbose,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu%3A14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2497/changes    {code}  [ RUN      ] PersistentVolumeEndpointsTest.OfferCreateThenEndpointRemove  I0713 18:43:55.968503 28220 cluster.cpp:155] Creating default 'local' authorizer  I0713 18:43:56.082345 28220 leveldb.cpp:174] Opened db in 113.403661ms  I0713 18:43:56.131445 28220 leveldb.cpp:181] Compacted db in 49.034774ms  I0713 18:43:56.131533 28220 leveldb.cpp:196] Created db iterator in 28012ns  I0713 18:43:56.131552 28220 leveldb.cpp:202] Seeked to beginning of db in 3046ns  I0713 18:43:56.131564 28220 leveldb.cpp:271] Iterated through 0 keys in the db in 255ns  I0713 18:43:56.131614 28220 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I0713 18:43:56.134064 28246 recover.cpp:451] Starting replica recovery  I0713 18:43:56.134627 28246 recover.cpp:477] Replica is in EMPTY status  I0713 18:43:56.136396 28252 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (9553)@172.17.0.8:35418  I0713 18:43:56.136759 28252 recover.cpp:197] Received a recover response from a replica in EMPTY status  I0713 18:43:56.137676 28246 recover.cpp:568] Updating replica status to STARTING  I0713 18:43:56.148720 28242 master.cpp:382] Master 2258d072-b0c9-4c40-874c-6cf933ee345a (500c3e866abe) started on 172.17.0.8:35418  I0713 18:43:56.148759 28242 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""50ms"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/LrwRl4/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.1.0/_inst/share/mesos/webui"" --work_dir=""/tmp/LrwRl4/master"" --zk_session_timeout=""10secs""  I0713 18:43:56.149247 28242 master.cpp:434] Master only allowing authenticated frameworks to register  I0713 18:43:56.149265 28242 master.cpp:448] Master only allowing authenticated agents to register  I0713 18:43:56.149273 28242 master.cpp:461] Master only allowing authenticated HTTP frameworks to register  I0713 18:43:56.149283 28242 credentials.hpp:37] Loading credentials for authentication from '/tmp/LrwRl4/credentials'  I0713 18:43:56.149780 28242 master.cpp:506] Using default 'crammd5' authenticator  I0713 18:43:56.149940 28242 master.cpp:578] Using default 'basic' HTTP authenticator  I0713 18:43:56.150091 28242 master.cpp:658] Using default 'basic' HTTP framework authenticator  I0713 18:43:56.150209 28242 master.cpp:705] Authorization enabled  W0713 18:43:56.150233 28242 master.cpp:768] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information  I0713 18:43:56.150760 28240 hierarchical.cpp:151] Initialized hierarchical allocator process  I0713 18:43:56.151018 28249 whitelist_watcher.cpp:77] No whitelist given  I0713 18:43:56.155668 28242 master.cpp:1973] The newly elected leader is master@172.17.0.8:35418 with id 2258d072-b0c9-4c40-874c-6cf933ee345a  I0713 18:43:56.155781 28242 master.cpp:1986] Elected as the leading master!  I0713 18:43:56.155848 28242 master.cpp:1673] Recovering from registrar  I0713 18:43:56.156065 28254 registrar.cpp:332] Recovering registrar  I0713 18:43:56.201568 28245 hierarchical.cpp:1537] No allocations performed  I0713 18:43:56.201666 28245 hierarchical.cpp:1172] Performed allocation for 0 agents in 167962ns  I0713 18:43:56.218626 28246 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 80.746657ms  I0713 18:43:56.218705 28246 replica.cpp:320] Persisted replica status to STARTING  I0713 18:43:56.219219 28246 recover.cpp:477] Replica is in STARTING status  I0713 18:43:56.221391 28246 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (9556)@172.17.0.8:35418  I0713 18:43:56.221869 28253 recover.cpp:197] Received a recover response from a replica in STARTING status  I0713 18:43:56.222760 28249 recover.cpp:568] Updating replica status to VOTING  I0713 18:43:56.252303 28254 hierarchical.cpp:1537] No allocations performed  I0713 18:43:56.252404 28254 hierarchical.cpp:1172] Performed allocation for 0 agents in 167038ns  I0713 18:43:56.270256 28249 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 47.316392ms  I0713 18:43:56.270387 28249 replica.cpp:320] Persisted replica status to VOTING  I0713 18:43:56.270700 28250 recover.cpp:582] Successfully joined the Paxos group  I0713 18:43:56.271121 28250 recover.cpp:466] Recover process terminated  I0713 18:43:56.271503 28248 log.cpp:553] Attempting to start the writer  I0713 18:43:56.273140 28240 replica.cpp:493] Replica received implicit promise request from (9557)@172.17.0.8:35418 with proposal 1  I0713 18:43:56.303086 28254 hierarchical.cpp:1537] No allocations performed  I0713 18:43:56.303175 28254 hierarchical.cpp:1172] Performed allocation for 0 agents in 155905ns  I0713 18:43:56.312978 28240 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 39.718643ms  I0713 18:43:56.313405 28240 replica.cpp:342] Persisted promised to 1  I0713 18:43:56.314775 28245 coordinator.cpp:238] Coordinator attempting to fill missing positions  I0713 18:43:56.316547 28250 replica.cpp:388] Replica received explicit promise request from (9558)@172.17.0.8:35418 for position 0 with proposal 2  I0713 18:43:56.354794 28239 hierarchical.cpp:1537] No allocations performed  I0713 18:43:56.354898 28239 hierarchical.cpp:1172] Performed allocation for 0 agents in 178033ns  I0713 18:43:56.363484 28250 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 46.846904ms  I0713 18:43:56.363585 28250 replica.cpp:712] Persisted action at 0  I0713 18:43:56.365622 28250 replica.cpp:537] Replica received write request for position 0 from (9559)@172.17.0.8:35418  I0713 18:43:56.365727 28250 leveldb.cpp:436] Reading position from leveldb took 45172ns  I0713 18:43:56.406314 28252 hierarchical.cpp:1537] No allocations performed  I0713 18:43:56.406421 28252 hierarchical.cpp:1172] Performed allocation for 0 agents in 177001ns  I0713 18:43:56.421867 28250 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 56.06514ms  I0713 18:43:56.421968 28250 replica.cpp:712] Persisted action at 0  I0713 18:43:56.423286 28254 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I0713 18:43:56.458665 28250 hierarchical.cpp:1537] No allocations performed  I0713 18:43:56.458799 28250 hierarchical.cpp:1172] Performed allocation for 0 agents in 250863ns  I0713 18:43:56.470486 28254 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 47.13918ms  I0713 18:43:56.470552 28254 replica.cpp:712] Persisted action at 0  I0713 18:43:56.470584 28254 replica.cpp:697] Replica learned NOP action at position 0  I0713 18:43:56.471782 28247 log.cpp:569] Writer started with ending position 0  I0713 18:43:56.475908 28253 leveldb.cpp:436] Reading position from leveldb took 79764ns  I0713 18:43:56.479058 28247 registrar.cpp:365] Successfully fetched the registry (0B) in 322.939904ms  I0713 18:43:56.479388 28247 registrar.cpp:464] Applied 1 operations in 50643ns; attempting to update the 'registry'  I0713 18:43:56.483093 28247 log.cpp:577] Attempting to append 168 bytes to the log  I0713 18:43:56.483269 28249 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I0713 18:43:56.484673 28245 replica.cpp:537] Replica received write request for position 1 from (9560)@172.17.0.8:35418  I0713 18:43:56.509866 28239 hierarchical.cpp:1537] No allocations performed  I0713 18:43:56.509959 28239 hierarchical.cpp:1172] Performed allocation for 0 agents in 157789ns  I0713 18:43:56.512147 28245 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 27.358967ms  I0713 18:43:56.512193 28245 replica.cpp:712] Persisted action at 1  I0713 18:43:56.513278 28250 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I0713 18:43:56.537894 28250 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 24.568093ms  I0713 18:43:56.537973 28250 replica.cpp:712] Persisted action at 1  I0713 18:43:56.538008 28250 replica.cpp:697] Replica learned APPEND action at position 1  I0713 18:43:56.539737 28252 registrar.cpp:509] Successfully updated the 'registry' in 60.26496ms  I0713 18:43:56.539949 28252 registrar.cpp:395] Successfully recovered registrar  I0713 18:43:56.540544 28252 master.cpp:1781] Recovered 0 agents from the Registry (129B) ; allowing 10mins for agents to re-register  I0713 18:43:56.540832 28250 hierarchical.cpp:178] Skipping recovery of hierarchical allocator: nothing to recover  I0713 18:43:56.541285 28251 log.cpp:596] Attempting to truncate the log to 1  I0713 18:43:56.541637 28248 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I0713 18:43:56.542763 28240 replica.cpp:537] Replica received write request for position 2 from (9561)@172.17.0.8:35418  I0713 18:43:56.571691 28240 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 28.798341ms  I0713 18:43:56.571889 28240 replica.cpp:712] Persisted action at 2  I0713 18:43:56.573218 28240 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I0713 18:43:56.620200 28240 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 46.927607ms  I0713 18:43:56.620338 28240 leveldb.cpp:399] Deleting ~1 keys from leveldb took 59898ns  I0713 18:43:56.620512 28240 replica.cpp:712] Persisted action at 2  I0713 18:43:56.620630 28240 replica.cpp:697] Replica learned TRUNCATE action at position 2  I0713 18:43:56.624091 28249 hierarchical.cpp:1537] No allocations performed  I0713 18:43:56.624169 28249 hierarchical.cpp:1172] Performed allocation for 0 agents in 140818ns  I0713 18:43:56.628180 28220 containerizer.cpp:196] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni  W0713 18:43:56.629341 28220 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos  W0713 18:43:56.629616 28220 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges  I0713 18:43:56.631988 28220 cluster.cpp:432] Creating default 'local' authorizer  I0713 18:43:56.635001 28243 slave.cpp:205] Agent started on 251)@172.17.0.8:35418  I0713 18:43:56.635308 28220 resources.cpp:572] Parsing resources as JSON failed: disk:512  Trying semicolon-delimited string format instead  I0713 18:43:56.635026 28243 slave.cpp:206] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-1.1.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""disk(*):1024"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ""  I0713 18:43:56.635709 28243 credentials.hpp:86] Loading credential for authentication from '/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/credential'  I0713 18:43:56.635892 28243 slave.cpp:343] Agent using credential for: test-principal  I0713 18:43:56.635924 28243 credentials.hpp:37] Loading credentials for authentication from '/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/http_credentials'  I0713 18:43:56.636272 28243 slave.cpp:395] Using default 'basic' HTTP authenticator  I0713 18:43:56.636615 28243 resources.cpp:572] Parsing resources as JSON failed: disk(*):1024  Trying semicolon-delimited string format instead  I0713 18:43:56.636878 28243 resources.cpp:572] Parsing resources as JSON failed: disk(*):1024  Trying semicolon-delimited string format instead  I0713 18:43:56.637318 28243 slave.cpp:594] Agent resources: disk(*):1024; cpus(*):16; mem(*):47270; ports(*):[31000-32000]  I0713 18:43:56.637859 28243 slave.cpp:602] Agent attributes: [  ]  I0713 18:43:56.638073 28220 sched.cpp:226] Version: 1.1.0  I0713 18:43:56.638074 28243 slave.cpp:607] Agent hostname: 500c3e866abe  I0713 18:43:56.640148 28253 sched.cpp:330] New master detected at master@172.17.0.8:35418  I0713 18:43:56.640650 28253 sched.cpp:396] Authenticating with master master@172.17.0.8:35418  I0713 18:43:56.640738 28253 sched.cpp:403] Using default CRAM-MD5 authenticatee  I0713 18:43:56.640801 28239 state.cpp:57] Recovering state from '/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/meta'  I0713 18:43:56.640976 28243 authenticatee.cpp:121] Creating new client SASL connection  I0713 18:43:56.641319 28253 status_update_manager.cpp:200] Recovering status update manager  I0713 18:43:56.641477 28243 master.cpp:6006] Authenticating scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418  I0713 18:43:56.641636 28239 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(554)@172.17.0.8:35418  I0713 18:43:56.641542 28240 containerizer.cpp:522] Recovering containerizer  I0713 18:43:56.642201 28239 authenticator.cpp:98] Creating new server SASL connection  I0713 18:43:56.642602 28252 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5  I0713 18:43:56.642634 28252 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'  I0713 18:43:56.642714 28239 authenticator.cpp:204] Received SASL authentication start  I0713 18:43:56.642792 28239 authenticator.cpp:326] Authentication requires more steps  I0713 18:43:56.642882 28239 authenticatee.cpp:259] Received SASL authentication step  I0713 18:43:56.642978 28239 authenticator.cpp:232] Received SASL authentication step  I0713 18:43:56.643009 28239 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '500c3e866abe' server FQDN: '500c3e866abe' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false   I0713 18:43:56.643026 28239 auxprop.cpp:181] Looking up auxiliary property '*userPassword'  I0713 18:43:56.643064 28239 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'  I0713 18:43:56.643091 28239 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '500c3e866abe' server FQDN: '500c3e866abe' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true   I0713 18:43:56.643107 28239 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true  I0713 18:43:56.643117 28239 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true  I0713 18:43:56.643136 28239 authenticator.cpp:318] Authentication success  I0713 18:43:56.643290 28239 authenticatee.cpp:299] Authentication success  I0713 18:43:56.643379 28239 master.cpp:6036] Successfully authenticated principal 'test-principal' at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418  I0713 18:43:56.643501 28244 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(554)@172.17.0.8:35418  I0713 18:43:56.643987 28244 sched.cpp:502] Successfully authenticated with master master@172.17.0.8:35418  I0713 18:43:56.644011 28244 sched.cpp:820] Sending SUBSCRIBE call to master@172.17.0.8:35418  I0713 18:43:56.644103 28244 sched.cpp:853] Will retry registration in 809.142674ms if necessary  I0713 18:43:56.644287 28244 master.cpp:2550] Received SUBSCRIBE call for framework 'default' at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418  I0713 18:43:56.644346 28244 master.cpp:2012] Authorizing framework principal 'test-principal' to receive offers for role 'role1'  I0713 18:43:56.644675 28249 provisioner.cpp:253] Provisioner recovery complete  I0713 18:43:56.645089 28245 master.cpp:2626] Subscribing framework default with checkpointing disabled and capabilities [  ]  I0713 18:43:56.645783 28249 hierarchical.cpp:271] Added framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000  I0713 18:43:56.645916 28249 hierarchical.cpp:1537] No allocations performed  I0713 18:43:56.646000 28249 hierarchical.cpp:1632] No inverse offers to send out!  I0713 18:43:56.646083 28248 sched.cpp:743] Framework registered with 2258d072-b0c9-4c40-874c-6cf933ee345a-0000  I0713 18:43:56.646116 28249 hierarchical.cpp:1172] Performed allocation for 0 agents in 249850ns  I0713 18:43:56.646163 28248 sched.cpp:757] Scheduler::registered took 21831ns  I0713 18:43:56.646317 28246 slave.cpp:4856] Finished recovery  I0713 18:43:56.663516 28246 slave.cpp:5028] Querying resource estimator for oversubscribable resources  I0713 18:43:56.664029 28254 status_update_manager.cpp:174] Pausing sending status updates  I0713 18:43:56.664043 28246 slave.cpp:969] New master detected at master@172.17.0.8:35418  I0713 18:43:56.664567 28246 slave.cpp:1028] Authenticating with master master@172.17.0.8:35418  I0713 18:43:56.665148 28246 slave.cpp:1039] Using default CRAM-MD5 authenticatee  I0713 18:43:56.665555 28246 slave.cpp:1001] Detecting new master  I0713 18:43:56.665590 28244 authenticatee.cpp:121] Creating new client SASL connection  I0713 18:43:56.665889 28246 slave.cpp:5042] Received oversubscribable resources  from the resource estimator  I0713 18:43:56.666071 28253 master.cpp:6006] Authenticating slave(251)@172.17.0.8:35418  I0713 18:43:56.666316 28244 authenticator.cpp:414] Starting authentication session for crammd5_authenticat...",1,1.8642652
MESOS-5845,The fetcher can access any local file as root,"The Mesos fetcher currently runs as root and does a blind cp+chown of any file:// URI into the task's sandbox, to be owned by the task user. Even if frameworks are restricted from running tasks as root, it seems they can still access root-protected files in this way. We should secure the fetcher so that it has the filesystem permissions of the user its associated task is being run as. One option would be to run the fetcher as the same user that the task will run as.",3,1.433097
MESOS-5848,Docker health checks are malformed.,"When wrapping the health check command into {{docker exec}}, docker executor erroneously forms the health check command itself. Here is an excerpt from an executor log:  {noformat}  Launching health check process: docker exec mesos-2070f452-2120-45ad-a8d2-a339d234da41-S0.c27d1b78-d4aa-424b-91fa-1e91576db9b5 sh -c "" true "" /opt/mesosphere/packages/mesos--59d45b30116143cb8d9995ca26f9dec5e93dc710/libexec/mesos/mesos-health-check --executor=(1)@10.0.1.41:40651 --health_check_json={""command"":{""shell"":true,""value"":""docker exec mesos-2070f452-2120-45ad-a8d2-a339d234da41-S0.c27d1b78-d4aa-424b-91fa-1e91576db9b5 sh -c \"" true \""""},""consecutive_failures"":1,""delay_seconds"":0.0,""grace_period_seconds"":10.0,""interval_seconds"":5.0,""timeout_seconds"":10.0} --task_id=testhc.db69c60b-4a75-11e6-b9b0-c254ada9b06d  {noformat}",1,2.8395174
MESOS-5850,Add a test that runs the 'mesos-local' binary,"The balloon framework test runs the Mesos master and agent binaries, but we don't seem to have any tests which run the {{mesos-local}} binary at the moment. Such a test should be added, or one of the existing example framework tests could be modified to accomplish this.",2,1.9494723
MESOS-5852,CMake build needs to generate protobufs before building libmesos,"The existing CMake lists place protobufs at the same level as other Mesos sources:  https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/CMakeLists.txt#L415    This is incorrect, as protobuf changes need to be regenerated before we can build against them.    Note: in the autotools build, this is done by compiling protobufs into {{libmesos}}, which then builds {{libmesos_no_3rdparty}}:  https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/Makefile.am#L1304-L1305",2,3.146237
MESOS-5855,Create a 'Disk (not) full' example framework,"We need example frameworks for verifying the correct behavior of posix/disk isolator when the disk quota enforcement is in place. One framework for verifying that disk quota enforcement is working and that container gets terminated when it goes beyond disk quota, and another one for verifying that container does not get killed if it stays within its disk quota bounds.   ",3,2.3939867
MESOS-5856,Logrotate ContainerLogger module does not rotate logs when run as root with --switch_user,"The logrotate ContainerLogger module runs as the agent's user.  In most cases, this is {{root}}.    When {{logrotate}} is run as root, there is an additional check the configuration files must pass (because a root {{logrotate}} needs to be secured against non-root modifications to the configuration):  https://github.com/logrotate/logrotate/blob/fe80cb51a2571ca35b1a7c8ba0695db5a68feaba/config.c#L807-L815    Log rotation will fail under the following scenario:  1) The agent is run with {{--switch_user}} (default: true)  2) A task is launched with a non-root user specified  3) The logrotate module spawns a few companion processes (as root) and this creates the {{stdout}}, {{stderr}}, {{stdout.logrotate.conf}}, and {{stderr.logrotate.conf}} files (as root).  This step races with the next step.  4) The Mesos containerizer and Fetcher will {{chown}} the task's sandbox to the non-root user.  Including the files just created.  5) When {{logrotate}} is run, it will skip any non-root configuration files.  This means the files are not rotated.    ----    Fix: The logrotate module's companion processes should call {{setuid}} and {{setgid}}.",1,1.6865305
MESOS-5863,Enabling SSL causes fetcher fail to fetch from HTTPS sites.,"This is because curl (which fetcher relies on) also relies on some of the environment variables used by libprocess SSL support. For instance, `SSL_CERT_FILE`. If the operator sets `SSL_CERT_FILE` env var for Mesos agent, the fetcher will inherit this env var and cause curl to fail:    {noformat}  [centos@ip-10-10-0-205 ~]$ SSL_CERT_FILE=/run/dcos/pki/tls/certs/mesos-slave.crt curl https://registry-1.docker.io:443/v2/library/alpine/manifests/latest  curl: (60) SSL certificate problem: unable to get local issuer certificate  More details here: https://curl.haxx.se/docs/sslcerts.html    curl performs SSL certificate verification by default, using a ""bundle""   of Certificate Authority (CA) public keys (CA certs). If the default   bundle file isn't adequate, you can specify an alternate file   using the --cacert option.  If this HTTPS server uses a certificate signed by a CA represented in   the bundle, the certificate verification probably failed due to a   problem with the certificate (it might be expired, or the name might   not match the domain name in the URL).  If you'd like to turn off curl's verification of the certificate, use   the -k (or --insecure) option.    [centos@ip-10-10-0-205 ~]$ curl https://registry-1.docker.io:443/v2/library/alpine/manifests/latest  {""errors"":[{""code"":""UNAUTHORIZED"",""message"":""authentication required"",""detail"":[{""Type"":""repository"",""Name"":""library/alpine"",""Action"":""pull""}]}]}  {noformat}    To solve this problem, we deprecated the existing `SSL_` env variables and used `LIBPROCESS_SSL_` instead. To be backward compatible, we still accept `SSL_` env variables for the time being.",3,2.9912484
MESOS-5864,Document MESOS_SANDBOX executor env variable.,And we should document the difference with MESOS_DIRECTORY.,2,2.0122046
MESOS-5874,Only send ShutdownFrameworkMessage to agents associated with framework.,slave.cpp:2079] Asked to shut down framework ${framework} by master@${master}  slave.cpp:2094] Cannot shut down unknown framework ${framework}     For high framework/churn clusters this saturates agent logs with these messages. When a framework terminates a ShutdownFrameworkMessage is sent to every registered slave in a for loop. This patch proposes sending this message to agents with executors associated with the framework.     Also proposed is moving the logline to VLOG(1). ,1,2.8908134
MESOS-5878,Strict/RegistrarTest.UpdateQuota/0 is flaky,"Observed on ASF CI (https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu:14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2539/consoleFull). Log file is attached. Note that this might have been uncovered due to the recent removal of {{os::sleep}} from {{Clock::settle}}.",3,2.082859
MESOS-5879,cgroups/net_cls isolator causing agent recovery issues,"We run with 'cgroups/net_cls' in our isolator list, and when we restart any agent process in a cluster running an experimental custom isolator as well, the agents are unable to recover from checkpoint, because net_cls reports that unknown orphan containers have duplicate net_cls handles.    While this is a problem that needs to be solved (probably by fixing our custom isolator), it's also a problem that the net_cls isolator fails recovery just for duplicate handles in cgroups that it is literally about to unconditionally destroy during recovery. Can this be fixed?",1,1.9903427
MESOS-5886,FUTURE_DISPATCH may react on irrelevant dispatch.,"[{{FUTURE_DISPATCH}}|https://github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/include/process/gmock.hpp#L50] uses [{{DispatchMatcher}}|https://github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/include/process/gmock.hpp#L350] to figure out whether a processed {{DispatchEvent}} is the same the user is waiting for. However, comparing {{std::type_info}} of function pointers is not enough: different class methods with same signatures will be matched. Here is the test that proves this:  {noformat}  class DispatchProcess : public Process<DispatchProcess>  {  public:    MOCK_METHOD0(func0, void());    MOCK_METHOD1(func1, bool(bool));    MOCK_METHOD1(func1_same_but_different, bool(bool));    MOCK_METHOD1(func2, Future<bool>(bool));    MOCK_METHOD1(func3, int(int));    MOCK_METHOD2(func4, Future<bool>(bool, int));  };  {noformat}  {noformat}  TEST(ProcessTest, DispatchMatch)  {    DispatchProcess process;      PID<DispatchProcess> pid = spawn(&process);      Future<Nothing> future = FUTURE_DISPATCH(        pid,        &DispatchProcess::func1_same_but_different);      EXPECT_CALL(process, func1(_))      .WillOnce(ReturnArg<0>());      dispatch(pid, &DispatchProcess::func1, true);      AWAIT_READY(future);      terminate(pid);    wait(pid);  }  {noformat}  The test passes:  {noformat}  [ RUN      ] ProcessTest.DispatchMatch  [       OK ] ProcessTest.DispatchMatch (1 ms)  {noformat}    This change was introduced in https://reviews.apache.org/r/28052/.",5,2.3899982
MESOS-5887,Enhance DispatchEvent to include demangled method name.,"Currently, [{{DispatchEvent}}|https://github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/include/process/event.hpp#L148] does not include any user-friendly information about the actual method being dispatched. This can be helpful in order to simplify triaging and debugging, e.g., using {{\_\_processes\_\_}} endpoint. Now we print the [event type only|https://github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/src/process.cpp#L3198-L3203].",5,2.5588949
MESOS-5891,/help endpoint does not set Content-Type to HTML,"This change added a default {{Content-Type}} to all responses:  https://github.com/apache/mesos/commit/b2c5d91addbae609af3791f128c53fb3a26c7d53    Unfortunately, this changed the {{/help}} endpoint from no {{Content-Type}} to {{text/plain}}.  For a browser to render this page correctly, we need an HTML content type.",1,2.380981
MESOS-5901,Make the command executor unversioned,"Currently, the command executor in {{src/launcher/executor.cpp}} is in the {{v1}} namespace. As referenced in the versioning design doc, we had agreed to keep the mesos internal code in the unversioned namespace and use {{evolve/devolve}} helpers for requests/responses.     Following this pattern, we should bring the command executor in the {{mesos::internal}} namespace.",2,2.5295162
MESOS-5907,ExamplesTest.DiskFullFramework fails on Arch,"This test fails consistently on recent Arch linux, running in a VM.",1,1.7287347
MESOS-5909,"Stout ""OsTest.User"" test can fail on some systems","Libc call {{getgrouplist}} doesn't return the {{gid}} list in a sorted manner (in my case, it's returning ""471 100"") ... whereas {{id -G}} return a sorted list (""100 471"" in my case) causing the validation inside the loop to fail.    We should sort both lists before comparing the values.",2,1.81404
MESOS-5923,"Ubuntu 14.04 LTS GPU Isolator ""/run"" directory is noexec","In Ubuntu 14.04 LTS the mount for /run directory is noexec.  It affect the {{/var/run/mesos/isolators/gpu/nvidia_352.63/bin}} directory which mesos GPU isolators depended on.    {{bill@billz:/var/run$ mount | grep noexec  proc on /proc type proc (rw,noexec,nosuid,nodev)  sysfs on /sys type sysfs (rw,noexec,nosuid,nodev)  devpts on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=0620)  tmpfs on /run type tmpfs (rw,noexec,nosuid,size=10%,mode=0755)}}    The /var/run is link to /run:  {{bill@billz:/var$ ll  total 52  drwxr-xr-x 13 root root     4096 May  5 20:00 ./  drwxr-xr-x 27 root root     4096 Jul 14 17:29 ../  lrwxrwxrwx  1 root root        9 May  5 19:50 lock -> /run/lock/  drwxrwxr-x 19 root syslog   4096 Jul 28 08:00 log/  drwxr-xr-x  2 root root     4096 Aug  4  2015 opt/  lrwxrwxrwx  1 root root        4 May  5 19:50 run -> /run/}}    Current the work around is mount without noexec:  {{sudo mount -o remount,exec /run}}",3,1.5442152
MESOS-5924,Fetcher may print logging error when run as unprivileged user,"Now that the fetcher performs its fetching as  the framework's/task's user when one is specified, it prints an error message when its user does not have permissions to create the default glog logging file:  {code}  I0728 17:29:39.337363 25464 logging.cpp:194] INFO level logging started!  I0728 17:29:39.337628 25464 fetcher.cpp:498] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/57c21e0d-487d-4668-8da6-005f13401598-S0\/centos"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""cache"":false,""executable"":false,""extract"":true,""value"":""file:\/\/\/nonrootdir\/nonroottest""}}],""sandbox_directory"":""\/var\/lib\/mesos\/slave\/slaves\/57c21e0d-487d-4668-8da6-005f13401598-S0\/frameworks\/57c21e0d-487d-4668-8da6-005f13401598-0001\/executors\/non-root-success.dc9e820d-54e8-11e6-b082-70b3d5120001\/runs\/3cae229f-8c6d-439e-8116-78bf06ac3731"",""user"":""centos""}  I0728 17:29:39.339738 25464 fetcher.cpp:409] Fetching URI 'file:///nonrootdir/nonroottest'  I0728 17:29:39.339758 25464 fetcher.cpp:250] Fetching directly into the sandbox directory  I0728 17:29:39.339777 25464 fetcher.cpp:187] Fetching URI 'file:///nonrootdir/nonroottest'  I0728 17:29:39.339792 25464 fetcher.cpp:167] Copying resource with command:cp '/nonrootdir/nonroottest' '/var/lib/mesos/slave/slaves/57c21e0d-487d-4668-8da6-005f13401598-S0/frameworks/57c21e0d-487d-4668-8da6-005f13401598-0001/executors/non-root-success.dc9e820d-54e8-11e6-b082-70b3d5120001/runs/3cae229f-8c6d-439e-8116-78bf06ac3731/nonroottest'  Could not create logging file: Permission denied  COULD NOT CREATE A LOGGINGFILE 20160728-172939.25464!W0728 17:29:39.342435 25464 fetcher.cpp:289] Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: file:///nonrootdir/nonroottest  I0728 17:29:39.342511 25464 fetcher.cpp:547] Fetched 'file:///nonrootdir/nonroottest' to '/var/lib/mesos/slave/slaves/57c21e0d-487d-4668-8da6-005f13401598-S0/frameworks/57c21e0d-487d-4668-8da6-005f13401598-0001/executors/non-root-success.dc9e820d-54e8-11e6-b082-70b3d5120001/runs/3cae229f-8c6d-439e-8116-78bf06ac3731/nonroottest'  + /opt/mesosphere/packages/mesos--6c64154890d6c22595d7d047193773cda8de6a7c/libexec/mesos/mesos-containerizer mount --help=false --operation=make-rslave --path=/  I0728 17:29:39.603394 25433 exec.cpp:161] Version: 1.0.0  I0728 17:29:39.699053 25490 exec.cpp:236] Executor registered on agent 57c21e0d-487d-4668-8da6-005f13401598-S0  {code}    It seems that the fetcher binary is unable to create the default logging file due to a permissions issue. However, when I set the relevant {{GLOG_logtostderr=true}} flag, which should prevent the creation of this default file, it had no effect.    Note that the fetcher's logging output was piped to stdout/stderr as expected, and the task ran and completed successfully, so these errors do not seem to affect the execution of the task.",2,2.608832
MESOS-5928,Agent's '--version' flag doesn't work,"With the removal of the agent's default {{work_dir}}, the {{--version}} flag no longer works. Instead, the agent complains about the lack of a {{work_dir}} and prints the usage instructions.",1,1.1266873
MESOS-5934,Enable the upgrade test script to run multiple masters/agents,"The script designed to test upgrades between different Mesos versions, {{support/test-upgrade.py}}, should be improved to test upgrades with multiple masters and agents.",3,2.6131237
MESOS-5935,Add upgrade testing to the ASF CI,"We should add execution of the {{support/test-upgrade.py}} script to the ASF CI runs. This will require having a build of a previous Mesos version to run against latest master; perhaps we could cache builds of the last stable release somewhere, which could be fetched and executed against CI builds.",5,1.70798
MESOS-5943,Incremental http parsing of URLs leads to decoder error,"When requests arrive to the decoder in pieces (e.g. {{mes}} followed by a separate chunk of {{os.apache.org}}) the http parser is not able to handle this case if the split is within the URL component.    This causes the decoder to error out, and can lead to connection invalidation.    The scheduler driver is susceptible to this.",3,1.9328413
MESOS-5944,Remove `O_SYNC` from StatusUpdateManager logs,Currently the {{StatusUpdateManager}} uses {{O_SYNC}} to flush status updates to disk.     We don't need to use {{O_SYNC}} because we only read this file if the host did not crash. {{os::write}} success implies the kernel will have flushed our data to the page cache. This is sufficient for the recovery scenarios we use this data for.,1,1.0758145
MESOS-5945,NvidiaVolume::create() should check for root before creating volume,"Without root, we cannot create the nvidia volume in {{/var/run/mesos}} or mount a {{tmpfs}} in cases where we need to override the {{noexec}} on the current file system.",2,2.845738
MESOS-5954,Docker executor does not use HealthChecker library.,"https://github.com/apache/mesos/commit/1556d9a3a02de4e8a90b5b64d268754f95b12d77 refactored health checks into a library. Command executor uses the library instead of the ""mesos-health-check"" binary, docker executor should do the same for consistency.",3,2.736068
MESOS-5955,"The ""mesos-health-check"" binary is not used anymore.","MESOS-5727 and MESOS-5954 refactored the health check code into the {{HealthChecker}} library, hence the ""mesos-health-check"" binary became unused.    While the command and docker executors could just use the library to avoid the subprocess complexity, we may want to consider keeping a binary version that ships with the installation, because the intention of the binary was to allow other executors to re-use our implementation. On the other side, this binary is ill suited to this since it uses libprocess message passing, so if we do not have code that requires the binary it seems ok to remove it for now. Custom executors may use the {{HealthChecker}} library directly, it is not much more complex than using the binary.",3,1.8671026
MESOS-5959,All non-root tests fail on GPU machine,A recent addition to ensure that {{NvidiaVolume::create()}} ran as root broke all non-root tests on GPU machines. The reason is that we unconditionally create this volume so long as we detect {{nvml.isAvailable()}} which will fail now that we are only allowed to create this volume if we have root permissions.    We should fix this by adding the proper conditions to determine when / if we should create this volume based on some combination of {{\-\-containerizer}} and {{\-\-isolation}} flags.,2,2.7335434

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Model Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from GPT2SP import GPT2ForSequenceClassification as GPT2SP\n",
    "from transformers import GPT2ForSequenceClassification as LinearGPT2\n",
    "from transformers import GPT2Config\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE_RATIO = 0.3 # within proj: 0.3 / cross proj: 0.4\n",
    "SEQUENCE_LEN = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "TOKENIZER = 'gpt2' # available: gpt2, wordlevel, sentencepiece, wordpiece \n",
    "MODEL_NAME = 'gpt2sp' # available: gpt2sp, gpt2\n",
    "\n",
    "# define device\n",
    "global DEVICE\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# define files to be used\n",
    "global DATA_PATH \n",
    "DATA_PATH = './sp_dataset/marked_training_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Methods and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT = ''\n",
    "MODEL = None\n",
    "DYNAMIC_BATCH = True\n",
    "BATCH_SIZE = None\n",
    "WITHIN_PROJECT = None\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "\n",
    "def data_processing(file_pair):\n",
    "    global BATCH_SIZE, BATCH_SIZE_RATIO, DATA_PATH, WITHIN_PROJECT, DYNAMIC_BATCH\n",
    "\n",
    "    train_data = pd.DataFrame(columns=['text', 'label'])\n",
    "    for train_file_name in file_pair['train']:\n",
    "        fname = DATA_PATH + train_file_name + '.csv'\n",
    "        df = prepare_dataframe(fname)\n",
    "        train_data = train_data.append(df)\n",
    "        \n",
    "    # data split\n",
    "    if WITHIN_PROJECT:\n",
    "        train_text, train_labels, val_text, val_labels, test_text, test_labels = within_project_split(train_data)\n",
    "    else:\n",
    "        train_text, train_labels, val_text, val_labels = train_val_split(train_data, 0.6)\n",
    "    # define batch size dynamically based on training length\n",
    "    if DYNAMIC_BATCH:\n",
    "        BATCH_SIZE = int(len(train_text) * BATCH_SIZE_RATIO)\n",
    "    # tokenization\n",
    "    tokens_train = tokenization(train_text.tolist())\n",
    "    tokens_val = tokenization(val_text.tolist())\n",
    "    print(tokens_train['input_ids'][:5])\n",
    " \n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
    "    train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
    "    val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "    \n",
    "    # prepare testing datasets\n",
    "    all_test_dataloader = []\n",
    "    test_file_names = []\n",
    "    if WITHIN_PROJECT:\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(file_pair['test'][0])\n",
    "        return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "    for test_file_name in file_pair['test']:\n",
    "        fname = DATA_PATH + test_file_name + '.csv'\n",
    "        test_data = prepare_dataframe(fname)\n",
    "\n",
    "        test_text = test_data['text']\n",
    "        test_labels = test_data['label']\n",
    "\n",
    "        # tokenization\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(test_file_name)\n",
    "    print('cross project data processing!')\n",
    "    return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "\n",
    "def train_val_split(data, split_ratio):\n",
    "    print('cross project split!')\n",
    "    split_point = int(len(data) * split_ratio)\n",
    "    train_text = data['text'][:split_point]\n",
    "    train_labels = data['label'][:split_point]\n",
    "    val_text = data['text'][split_point:]\n",
    "    val_labels = data['label'][split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels\n",
    "\n",
    "\n",
    "def tokenization(text_list):\n",
    "    global TOKENIZER, SEQUENCE_LEN, MODEL\n",
    "    # tokenization\n",
    "    if TOKENIZER == 'wordpiece':\n",
    "        print('using wordpiece tokenizer!')\n",
    "        tokenizer = BertTokenizer('all_tokenizers/word_piece/vocab.txt')\n",
    "    elif TOKENIZER == 'sentencepiece':\n",
    "        print('using sentencepiece tokenizer!')\n",
    "        tokenizer = XLNetTokenizer('all_tokenizers/sentence_piece/spm_tokenizer.model', padding_side='right')\n",
    "    elif TOKENIZER == 'wordlevel':\n",
    "        print('using wordlevel tokenizer!')\n",
    "        tokenizer = Tokenizer.from_file('all_tokenizers/word_level/wordlevel.json')\n",
    "        encoded_sentences = {'input_ids':[]}\n",
    "        for sentence in text_list:\n",
    "            encoded = tokenizer.encode(sentence)\n",
    "            encoded = encoded.ids\n",
    "            if len(encoded) > SEQUENCE_LEN:\n",
    "                encoded = encoded[:SEQUENCE_LEN]\n",
    "            elif len(encoded) < SEQUENCE_LEN:\n",
    "                padding = SEQUENCE_LEN - len(encoded)\n",
    "                for _ in range(padding):\n",
    "                    encoded.append(3)\n",
    "            encoded_sentences['input_ids'].append(encoded)\n",
    "        return encoded_sentences\n",
    "    elif TOKENIZER == 'gpt2':\n",
    "        print('using pretrained gpt-2 tokenizer')\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER)\n",
    "        tokenizer.pad_token = '[PAD]'\n",
    "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
    "\n",
    "\n",
    "def prepare_dataframe(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    # some rows have no description, fill blank to avoid Null\n",
    "    data = data.fillna(' ')\n",
    "    d = {'text': (data['title']).tolist(), 'label': data['storypoint']}\n",
    "    return pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    global BATCH_SIZE\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def within_project_split(data):\n",
    "    print('within project split!')\n",
    "    train_val_split_point = int(len(data) * 0.6)\n",
    "    val_test_split_point = int(len(data) * 0.8)\n",
    "    train_text = data['text'][:train_val_split_point]\n",
    "    train_labels = data['label'][:train_val_split_point]\n",
    "    val_text = data['text'][train_val_split_point:val_test_split_point]\n",
    "    val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "    test_text = data['text'][val_test_split_point:]\n",
    "    test_labels = data['label'][val_test_split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels, test_text, test_labels   \n",
    "\n",
    "\n",
    "def train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names):\n",
    "    global LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE\n",
    "    optimizer = AdamW(MODEL.parameters(), lr=LEARNING_RATE)    \n",
    "    # Total number of training steps is [number of batches] x [number of epochs]\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    # Create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    print(\"Start training for \", file_pair, \".....\")\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer_path = 'tb/' + str(file_pair['train'][0]) + '_' + str(file_pair['test'][0])\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    \n",
    "    # vars for model selection\n",
    "    min_eval_loss_epoch = [10000, 0]\n",
    "    \n",
    "    time_records = []\n",
    "    MAE_RECORDS = []\n",
    "    MDAE_RECORDS = []\n",
    "    start_time = time.time()\n",
    "    loss_fct = nn.L1Loss()\n",
    "    for e in range(EPOCHS):\n",
    "        # ---TRAINING---\n",
    "        # clean GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\">>> epoch \", e)\n",
    "        # set model into train mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):            \n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_train_loss += loss.item()  \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # clean memory\n",
    "            del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "        writer.add_scalar('loss/train', avg_train_loss, e)\n",
    "        # clean memory\n",
    "        del avg_train_loss, total_train_loss\n",
    "        \n",
    "        time_records.append(time.time() - start_time)\n",
    "        \n",
    "        # ---EVAL---\n",
    "        print(\"-\")\n",
    "        # set model into eval mode\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for batch in val_dataloader:            \n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_eval_loss += loss.item()  \n",
    "            # clean memory\n",
    "            del b_input_ids, b_labels, batch, result, loss, logits\n",
    "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "        \n",
    "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "            min_eval_loss_epoch[0] = avg_eval_loss\n",
    "            min_eval_loss_epoch[1] = e\n",
    "        \n",
    "        writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
    "        # clean memory\n",
    "        del avg_eval_loss, total_eval_loss\n",
    "        # save model state to dict\n",
    "        torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
    "        \n",
    "        print(\"===============================\")\n",
    "        \n",
    "        # testing on holdout data\n",
    "        index = 0\n",
    "        for test_dataloader in all_test_dataloader:\n",
    "            test_file_name = test_file_names[index]\n",
    "            index += 1\n",
    "            testing_start_time = time.time()\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for batch in test_dataloader:\n",
    "                batch = tuple(t.to(DEVICE) for t in batch)\n",
    "                b_input_ids, b_labels = batch\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids)\n",
    "                logits = logits['logits'].detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(label_ids)\n",
    "            # calculate errors\n",
    "            total_distance = 0\n",
    "            distance_records = []\n",
    "            for i in range(len(predictions)):\n",
    "                for j in range(len(predictions[i])):\n",
    "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                    total_distance += distance\n",
    "                    distance_records.append(distance)\n",
    "\n",
    "            total_data_point = len(predictions) * len(predictions[0])\n",
    "\n",
    "            MAE = total_distance / total_data_point\n",
    "            MdAE = np.median(np.array(distance_records)) \n",
    "\n",
    "            MAE_RECORDS.append(MAE)\n",
    "            MDAE_RECORDS.append(MdAE)\n",
    "            \n",
    "            global OUTPUT\n",
    "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
    "            print('MAE: ', MAE)\n",
    "            print('MdAE: ', MdAE)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # select model\n",
    "    os.rename('models/epo_' + str(min_eval_loss_epoch[1]), \n",
    "              'models/' + str(file_pair['train'][0]) + '_' \n",
    "              + str(file_pair['test'][0]) + '_epo_' + str(min_eval_loss_epoch[1]))\n",
    "    \n",
    "    # del unwanted models\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            os.remove(\"models/epo_\" + str(i))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
    "                + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "    global BATCH_SIZE\n",
    "    OUTPUT += 'batch size: ' + str(BATCH_SIZE)\n",
    "    print('all done for one project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within Project Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = True\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['bamboo'], 'test': ['bamboo']},\n",
    "                        {'train': ['clover'], 'test': ['clover']},\n",
    "                        {'train': ['datamanagement'], 'test': ['datamanagement']},\n",
    "                        {'train': ['duracloud'], 'test': ['duracloud']},\n",
    "                        {'train': ['jirasoftware'], 'test': ['jirasoftware']},\n",
    "                        {'train': ['mesos'], 'test': ['mesos']},\n",
    "                        {'train': ['moodle'], 'test': ['moodle']},\n",
    "                        {'train': ['mule'], 'test': ['mule']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['springxd'], 'test': ['springxd']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['talenddataquality']},\n",
    "                        {'train': ['talendesb'], 'test': ['talendesb']},\n",
    "                        {'train': ['titanium'], 'test': ['titanium']},\n",
    "                        {'train': ['usergrid'], 'test': ['usergrid']},\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'bbpe':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
    "        elif TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)           \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Within Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = False\n",
    "\n",
    "# within repo\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['mesos'], 'test': ['usergrid']},\n",
    "                        {'train': ['usergrid'], 'test': ['mesos']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['titanium']},\n",
    "                        {'train': ['titanium'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['titanium']},\n",
    "                        {'train': ['mule'], 'test': ['mulestudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'bbpe':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
    "        elif TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)           \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Cross Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = False\n",
    "\n",
    "# cross repo\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['clover'], 'test': ['usergrid']},\n",
    "                        {'train': ['talendesb'], 'test': ['mesos']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['mule'], 'test': ['titanium']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['titanium']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)           \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aeb9b208553ed88b6b603c645b036ca8f07699d154de6af90b2d9b928ffaf5e4"
  },
  "kernelspec": {
   "display_name": "Python (hf_xfmr)",
   "language": "python",
   "name": "hf_xfmr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
